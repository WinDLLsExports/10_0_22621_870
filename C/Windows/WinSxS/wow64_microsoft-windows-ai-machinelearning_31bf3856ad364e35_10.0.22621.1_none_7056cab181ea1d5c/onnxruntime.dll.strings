      
        
            
              
                    [
                    [0.0, 0.0, 1.0, 1.2],
                    [0.0, 0.0, 2.3, 3.4],
                    [0.0, 0.0, 4.5, 5.7],
                    [1.0, 1.2],
                    [2.3, 3.4],
                    [4.5, 5.7],
                    ]
                    ],
                cond_out = Identity (cond)
                current = Add (prev, delta)
                CX = Mul (C, X)
                ERFCX = Erf (CX)
                ERFCXPlus1 = Add (ERFCX, One)
                input_x = [2, 1, 1, 3, 4, 3]
                output_counts = [1, 2, 2, 1]
                output_idx = [0, 1, 1, 2, 3, 2]
                output_uniques = [2, 1, 3, 4]
                PhiX = Mul (ERFCXPlus1, Half)
                range = Identity (prev)
                T1 = Mul (X_bias, X_bias)
                T2 = Mul (c, T1)
                T3 = Add (b, T2)
                T4 = Mul (X_bias, T3)
                T5 = Tanh (T4)
                T6 = Add (one, T5)
                T7 = Mul (X_bias, T6)
                Y = Mul (a, T7)
                Y = Mul (X, PhiX)
              }>
              <body = loop_body_attribute (int64 i, bool cond, prev) => (cond_out, current, range) {
              Example:
              Finds all the unique values (deduped list) present in the given input tensor.
              of each value of the input in 'uniques'.
              sorted in the same order that they occur in the input.
              The first output tensor 'uniques' contains all of the unique elements of the input,
              The second output tensor 'idx' is the same size as the input and it contains the index
              The third output tensor 'counts' contains the count of each element of 'uniques' in the input.
              This operator returns 3 outputs.
            C = Or (O1, O2)
            ceil_result = Ceil (div_result)
            ceil_result_relu = Relu (ceil_result)
            ceil_result_relu_bool = Cast <to = 9> (ceil_result_relu)
            ceil_result_relu_int = Cast <to = 7> (ceil_result_relu)
            data = [
            delta_casted = Cast <to = 1> (delta)
            div_result = Div (sub_result_casted, delta_casted)
            Example:
            Given `data` tensor, pads, mode, and value.
            HS_X = HardSigmoid<alpha = 0.16666667163372, beta = 0.5>(X) 
            Insert 0 pads to the beginning of the second dimension.
            O1 = Less (A, B)
            O2 = Equal (A, B)
            output = [
            pads = [0, 2, 0, 0]
            sub_result = Sub (limit, start)
            sub_result_casted = Cast <to = 1> (sub_result)
            variadic_output, output = Loop (ceil_result_relu_int, ceil_result_relu_bool, start)
            Y = Mul (X, HS_X)
           Clipped_ZeroPoint_FP = Clip (Initial_ZeroPoint_FP, Q_Min, Q_Max)
           Initial_ZeroPoint_FP = Sub (Q_Min, Min_Scaled)
           Min_Scaled = Div (X_Min_Adjusted, Scale)
           Q_Max = Constant<value = float {255.0}>()
           Q_Min = Constant<value = float {0.0}>()
           Rounded_ZeroPoint_FP = Round (Clipped_ZeroPoint_FP)
           Scale = Div (X_Range, Q_Max)
           X_Max = ReduceMax <keepdims = 0> (x)
           X_Max_Adjusted = Max (X_Max, Q_Min)
           X_Min = ReduceMin <keepdims = 0> (x)
           X_Min_Adjusted = Min (X_Min, Q_Min)
           X_Range = Sub (X_Max_Adjusted, X_Min_Adjusted)
           y = QuantizeLinear (x, Scale, Zeropoint)
           y_scale = Identity (Scale)
           y_zero_point = Identity (Zeropoint)
           Zeropoint = Cast <to = 2> (Rounded_ZeroPoint_FP)
          {
          }
          E_Xsquared = ReduceMean <axes : ints = @axes> (X_squared)
          Epsilon = Constant <value = float {1e-9}>()
          EX_squared = Pow (X_RM, Exponent)
          Exponent = Constant <value = float {2.0}>()
          Processed_STD = Add (STD, Epsilon)
          STD = Sqrt (Variance)
          Variance = Sub (E_Xsquared, EX_squared)
          X_RM = ReduceMean <axes : ints = @axes> (X)
          X_squared = Pow (X, Exponent)
          X_variance = Sub (X, X_RM)
          Y = Div (X_variance, Processed_STD)
        (possibly with aspect ratio change) to a common output size specified by crop_height and crop_width.
        {
        }
        <requestedExecutionLevel level='asInvoker' uiAccess='false' />
        a fixed size = [crop_height, crop_width]. The result is a 4-D tensor [num_boxes, crop_height, crop_width, depth].
        Extracts crops from the input image tensor and resizes them using bilinear sampling or nearest neighbor sampling
        Returns a tensor with crops from the input image at positions defined at the bounding box locations in boxes.
        The cropped boxes are all resized (with bilinear or nearest neighbor interpolation) to
        The resizing is corner aligned.
       for a dictionary of fixed size.
      [1, 2, 3, 4],
      [2, 3, 4],
      [5, 6, 7, 8],
      [5, 6, 7],
      </requestedPrivileges>
      <requestedPrivileges>
      All other elements in the matrix are set to zero.
      Based on Torch operator Embedding, creates a lookup table of embedding vectors of fixed size,
      Currently, only spatial (4-D) inputs are supported. For `input` with shape (N, C, H, W) and `grid` with shape (N, H_out, W_out, 2),
      For each output location `output[n, :, h, w]`, the size-2 vector `grid[n, h, w]` specifies `input` pixel locations `x` and `y`,
      Given an `input` and a flow-field `grid`, computes the `output` using `input` values and pixel locations from `grid`.
      If k = 0, the triangular part on and above/below the main diagonal is retained.
      If upper is set to false, a positive k retains the lower triangular matrix including k diagonals above
      If upper is set to true, a positive k retains the upper triangular matrix excluding k diagonals above
      of the elements on and above the given diagonal (k). The lower triangular part consists of elements on and below the diagonal.
      Returns the upper or lower triangular part of a 2-D matrix, or batches of 2-D matrices. If the attribute "upper" is set to true,
      See also in [torch.nn.functional.grid_sample](https://pytorch.org/docs/master/generated/torch.nn.functional.grid_sample.html#torch-nn-functional-grid-sample).
      the `output` will have shape (N, C, H_out, W_out).
      The GridSample operator is often used in doing grid generator and sampler in the [Spatial Transformer Networks](https://arxiv.org/abs/1506.02025).
      the main diagonal. A negative k value excludes as many diagonals below the main diagonal.
      the main diagonal. A negative k value includes as many diagonals below the main diagonal.
      the upper triangular matrix is retained. Lower triangular matrix is retained otherwise. Default value for upper is true.
      Trilu takes one input tensor of shape [*, N, M], where * is zero or more batch dimensions. The upper triangular part consists
      which are used to interpolate the output value `output[n, :, h, w]`.
    </security>
    <security>
   ' 0 8 ; > A C G Q S S U ^ 
  - Ct = ft (.) Ct-1 + it (.) ct
  - ct = g(Xt*(Wc^T) + Ht-1*(Rc^T) + Wbc + Rbc)
  - ft = f(Xt*(Wf^T) + Ht-1*(Rf^T) + Pf (.) Ct-1 + Wbf + Rbf)
  - Ht = (1 - zt) (.) ht + zt (.) Ht-1
  - Ht = f(Xt*(Wi^T) + Ht-1*(Ri^T) + Wbi + Rbi)
  - ht = g(Xt*(Wh^T) + (rt (.) (Ht-1*(Rh^T) + Rbh)) + Wbh) # when linear_before_reset != 0
  - ht = g(Xt*(Wh^T) + (rt (.) Ht-1)*(Rh^T) + Rbh + Wbh) # default, when linear_before_reset = 0
  - Ht = ot (.) h(Ct)
  - it = f(Xt*(Wi^T) + Ht-1*(Ri^T) + Pi (.) Ct-1 + Wbi + Rbi)
  - ot = f(Xt*(Wo^T) + Ht-1*(Ro^T) + Po (.) Ct + Wbo + Rbo)
  - rt = f(Xt*(Wr^T) + Ht-1*(Rr^T) + Wbr + Rbr)
  - zt = f(Xt*(Wz^T) + Ht-1*(Rz^T) + Wbz + Rbz)
  "separators" is a list of strings which are regular expressions. "tokenexp" is a single regular expression.
  (NOTE: Below are optional)
  * 2-D inputs or
  * 3-D inputs ('Bilinear', 'Trilinear') or
  * 4-D inputs with the corresponding outermost 2 scale values being 1 or the corresponding outermost and innermost scale values being 1 or
  * 5-D inputs with the corresponding outermost 2 scale values being 1in the 
  ["Hello World", "I love computer science !"] whose shape is [2],
  </trustInfo>
  <trustInfo xmlns="urn:schemas-microsoft-com:asm.v3">
 ' 0 C E Q S ^ } ~ 
  09AFafAZ
  09az
  Affine(x)              - alpha*x + beta
  axes = [0, 1]
  AZaz
  data    = [[[0,1],[2,3]],[[4,5],[6,7]]]
  data    = [[0,1],[2,3]]
  data = [
  Elu(x)                 - x if x >= 0 else alpha*(e^x - 1)
  ends = [-1, 1000]
  ends = [2, 3]
  HardSigmoid(x)         - min(max(alpha*x + beta, 0), 1)
  If input is
  If input is ["Hello", "World"],
  If the maximum number of tokens found per input string is D, the output shape would be [N, C, D] when input shape is [N, C].
  indices = [[[0,1]],[[1,0]]]
  indices = [[0,0],[1,1]]
  indices = [[0,1],[1,0]]
  indices = [[1],[0]]
  LeakyRelu(x)           - x if x >= 0 else alpha * x
  Let's assume "separators" is [" "] and consider an example.
  Let's consider another example to illustrate the effect of setting "mark" to true.
  output  = [[[2,3]],[[4,5]]]
  output  = [[2,3],[0,1]]
  output  = [[2,3],[4,5]]
  output  = [0,3]
  Relu(x)                - max(0, x)
  result = [
  ScaledTanh(x)          - alpha*Tanh(beta*x)
  shape(A) = (2, 3, 4, 5), shape(B) = (,), i.e. B is a scalar tensor
  shape(A) = (2, 3, 4, 5), shape(B) = (1, 1), i.e. B is an 1-element tensor
  shape(A) = (2, 3, 4, 5), shape(B) = (2), with axis=0
  shape(A) = (2, 3, 4, 5), shape(B) = (3, 4), with axis=1
  shape(A) = (2, 3, 4, 5), shape(B) = (4, 5)
  shape(A) = (2, 3, 4, 5), shape(B) = (5,)
  Sigmoid(x)             - 1/(1 + e^{-x})
  Similarly, if input shape is [C] then the output should be [C, D]. Tokenizer has two different operation modes.
  Softplus(x)            - log(1 + e^x)
  Softsign(x)            - x/(1 + |x|)
  starts = [0, 1]
  starts = [1, 0]
  Tanh(x)                - (1 - e^{-2x})/(1 + e^{-2x})
  The first mode is selected when "tokenexp" is not set and "separators" is set. If "tokenexp" is set and "separators" is not set,
  the second mode will be used. The first mode breaks each input string into tokens by matching and removing separators.
  then the corresponding output would be [0x02, "Hello", "World", 0x03].
  then the output would be
  This implies that if mark is true, [C]/[N, C] - input's output shape becomes [C, D+2]/[N, C, D+2].
  ThresholdedRelu(x)     - x if x >= alpha else 0
  Tokenizer divides each string in X into a vector of strings along the last axis. Allowed input shapes are [C] and [N, C].
 !!"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%&&&&&&&&&&&&'&&()))*
 != mat_h:
 ( ) / / _ _ 
 (actual) rounded_bytes:
 (domain: 
 (falsenode).
 (node 
 (node_version: 
 (requested) num_bytes: 
 (truenode).
 (weights).
 ) is different from what is supplied (
 * . ` d f o 
 *!+!2!2!N!N!`!
 *0-0
 , or optional typed entities
 , or sparse tensors
 . Got: 
 / / _ _ 
 : : 
 ;VyT
 ["I", "love", "computer", "science", "!"]]
 [*!_/
 [["Hello", "World", padvalue, padvalue, padvalue],
 [seqno=
 [truncated]
 _^[]
 |,},o-o-/./.
 = Constant()
 > dense_size: 
 0(0,00080<0@0D0h0l0t0x0|0
 0)0-0105090=0A0E0I0V0
 0+060A0
 9 9 
 Actual:
 already exist.
 already exists.
 and 
 and Output 
 and the number of 
 and then launches another search starting from the first remained character after the first matched token.
 and type (
' appeared multiple times.
 appears in graph inputs and will not be treated as constant value/weight. 
 are '0' and '1'. 
 are '0' and '1'. The environment variable contained the value: 
 arena_extend_strategy: 
 as it still has output edges.
 at line 
 at pos=
 attribtues in LabelEncoder 
' attribute.
 Attribute:
 Axis is 
 axis value 
 Axis=
 BackUp() can only be called after Next().
 because the CPU execution path is deemed faster than overhead involved with execution on other EPs 
 BFC Arena shrunk by 
 bins of max chunk size 
 broadcasting: (
 but 
 but different TensorProto.
 but expected 
 but has 
 but has rank 
 but input '
 but is of type: 
 but ngram_indexes size: 
 but subgraphs produce 
 but the actually size is: 
 but the node in the model has the following type (
 but usage of initializer in graph expects 
 bytes for 
 bytes were able to be read.
 bytes.
 bytes. 
 can not be writen into Tensor type 
 Can't back up over more bytes than were returned by the last call to Next().
 capable of executing this node
 Char embedding size: 
 char_embedding_size attribute: 
 column: 
 combination in the memory arena shrink list: 
 combination is not an arena based allocator: 
 conv filter size: 
 conv kernal size 1: 
 Conv kernal size 2 : 
 conv_window_size attribute: 
 d f p t ~ 
 data_type: 
 DeviceId:
 did not match batch size of 
 did not return correct number of compiled functions
 did not.
' dimension 
 dimension != 
 Dimension=
 dimensions or more but input had shape of 
 dimensions.
 does not align with rank of input data: 
 does not contain a graph.
 does not match actual shape of 
 does not match existing output type of 
 does not match input batch size 
 does not match rank 
 does not match the actual size
 does not match the equation indices.
 does not match type of output: 
 does not match. 
 does not specify a valid type.
 does not.
 doesn't have an implementation that can cache computed pre-packed weights
 doesn't have an implementation that can consume provided pre-packed weights
' doesn't support memcpy 
 dst_size: 
 E E } } 
 elements.
 else=
 embedding_size attribute: 
 Encountered following errors: (
 entries which doesn't match the number of fetches the frame was initialized with of 
 error message: 
 exceeded maximum protobuf size of 2GB: 
 Expected 
 Expected DENSE or SPARSE
 expected size 
 Expected std::map<int64_t, float> or std::map<int64_t, std::string>
 expected to be of type: 
 expected to have optional type
 expected to have rank 
 expected to have rank >
 expected to have sequence type
 expected to have tensor or sparse tensor type
 expected to have tensor or sparse tensor type. Got: 
 expected to have tensor or sparse tensor type: 
 expected to have tensor or sparse type
 expected to have tensor type
 expected to have type but instead is null
 expected to have: 
 Expected TO_FLOAT, TO_STRING or TO_INT64
 Expected:
 Expected: 
 ExplicitInputs:
 fail, errcode = 
 fail: unexpected end
 failed
 failed.
 failed. Error:
 failed. File doesn't exist
 failed. Only 
 failed:
 failed: 
 filter_number: 
 for attribute 
 For each input string, the second mode searches matches of "tokenexp" and each match will be a token in Y.
 for input shape 
 for operator 
 for output 
 for SizeFromDimension. Tensor has 
 for the following indices
 found!
 Found:
 Got:
 got: 
 Got: 
 Graph may not conform to the ONNX spec and contain initializers that are not graph inputs.
 group: 
 has already been loaded.
 has already been registered.
 has batch size of 
' has been deprecated since version 
' has been used as graph input names multiple times.
' has been used as output names multiple times.
 has Compile error: 
' has element type 
 has inconsistent type 
 has length of 
 has mismatched dimensions of 
 has unknown expected type
 has unsupported type 
 However the types are incompatible.
 hpEE
 http://www.microsoft.com/windows0
 id: 
 If "separators" contains a single empty string, the Tokenizer will enter into character tokenezation mode. This means all strings
 If no match found, this operator will remove the first character from the remained string and do another search.
 Implicit input name 
 ImplicitInputs:
 in AddToThreadq
 in 'Constant' node '
' in custom op '
 in function opset imports.
 in initializer but not in graph input
 in KernelRegistryManager
 in node 
 in node (
 in one of the subgraphs.
 in step
 in the same dimension
 in the supported version range
 in tree 
 Index:
 index: 
 inferred output shape:
 inferred=
 initial_growth_chunk_size_bytes: 
 initializer name is not unique
 Input dim value: 
 input dimensions instead
 Input shape=
 input with name 
 Input=
 inputs and requires 
 inputs but 
 inputs but Scan was only given 
 inputs but subgraph has 
 inputs. Either provide all subgraph inputs, or just the required inputs.
 inputs. Found:
' instead of '
 into softmax(input + bias)
 is already there.
 is defined.
 is deprecated in domain_version of 
' is expected to have field 'floats'
' is expected to have field 'g'
' is expected to have field 'graphs'
' is expected to have field 'ints'
' is expected to have field 'sparse_tensor'
' is expected to have field 'strings'
' is expected to have field 't'
' is expected to have field 'tensors'
' is expected to have field 'type_proto'
' is expected to have field 'type_protos'
 is expected to have type: 
 is greater than input dim=
 is incompatible in the dimension 
 is invalid for a tensor of rank 
 is invalid.
 is marked single but has an empty string in the graph
 is missing type info.
' is missing.
 is missing. Invalid ORT format model.
 is NaN
' is not a graph input, initializer, or output of a previous node.
 is not a registered function/op
 is not a valid date
 is not a valid day
 is not a valid year
 is not compatible with 
 is not currently registered or supported
 is not expected to be of type sparse tensor.
 is not expected to be of type tensor sequence.
 is not expected to be of type tensor.
 is not implemented
 is not in (0, 
 is not in valid range [-
 is not output of any previous nodes.
 is not present.
 is not supported
 is not supported currently
 is not supported yet
 is not supported.
 is not the same as this node's index:
 is not used by any node.
 is null
 is null. Invalid ORT format model.
 is null. Type info is expected.
 is out of bounds of lhs_right: 
 is out of bounds of out_left: 
 is out of bounds.
 is outside range.
 is repeated.
 is required but missing.
 is required to be non-empty.
 is smaller than requested bytes of 
 is till opset 
 is undefined so it cannot be parsed.
 is under development and support for this is limited. The operator schemas and or other functionality could possibly change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain 
 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain 
 is unrecognized, acceptable values are TF,IDF,TFIDF
 is used by node 
 kernel channels: 
 kernel is not supported in 
 kernel start version: 
 kernel_end_version: 
 kernel_shape: 
 known by the checker.
 Left shape override: 
 line 
 Max:
 max_dead_bytes_per_chunk: 
 memory limit: 
 MemoryType:
 message of type "
 Microsoft Corporation. All rights reserved.
 Microsoft Operations Puerto Rico1&0$
' Model is invalid.
 model may run depending upon legacy support of some older opset version operators.
 model uses the deprecated attribute
 models with experimental operators: 
 MR^o
 must be 1 instead of 
 must be either specified in graph inputs or graph initializers.
 must be equal to or twice the values size: 
 must be less than total buffer size: 
 must be of equal size
 must be within the inclusive range [
 must have shape {
 node '
 node. Name:'
 node_version: 
' not found
 not found.
 not in allowed input sizes.
 not in allowed output sizes.
 not in range [min=
 not specified
 Note that the input at most can have two axes, so 3-D and higher dimension are not supported.
 Num entries in 'split' (must equal number of outputs) was 
 num_input_channels: 
 NumOutputs=
' of 
' of input parameter (
 of node 
' of node: 
 Operating System
 optype 
' optype 
' OpType:
 OpType: 
 or UNDEFINED. Got: 
 OrtAllocatorType:
 OrtMemType:
 out of bounds for shape 
 Output dim value: 
 outputs but Scan expects 
 outputs so the subgraph requires 
 outputs which doesn't match the subgraph's 
 outputs.
 outputs. Expected 
 P!_!
 Parameter to BackUp() can't be negative.
 Please fix either the inputs or the model.
 Provider: [
 r1~k`ay
 referenced by function body node 
 Requested shape:
 returned nullptr
 Right shape override: 
 Right shape: 
 row[
 rows: 
 rows[
 should be of integer type and specify a type.
' should be stored in field '
 should specify a shape
 size: 
 size=
' source:
 source=
 sparse initializer name is not unique across initializers and sparse_initializers
 specified. It should be either avg or max
 specified. It should be either bilinear or nearest
' Status Message: 
 Sum of sizes in 'split' (must equal size of selected axis) was 
 sum of split values=
 t!;;r
 t";7r
 t$;;r
 target:
 Target=
 target=
 Tensor=
 The matching of "tokenexp" is conducted greedily (i.e., a match should be as long as possible).
' the model will use the latest encountered initializer
 The production MUST never overflow. The accumulation may overflow if and only if in 32 bits.
 the same as values size: 
 The total allocated bytes is now 
 then=
 This op has been implemented only for the following types (
 This operator searches for the first match starting from the beginning of the considered string,
 This procedure will be repeated until reaching the end of the considered string.
 to be equal to values blocks: 
 to device type: 
 to have different number of elements
 type: 
 typestr: 
 unknown
 used in the node: 
 Value=
 values, but NNZ is 
 Version mismatch.
 version: 
 vs inner_B: 
 vs. 
 was 
' was 
 was false.
 was not
 was not a tensor.
 was not found. Defaulting to a rank 1 shape of {0}.
 were provided
 were provided.
 which is of op type: 
 whose shape is [2, 5] because you can find at most 5 tokens per input string.
 will be broken part into individual characters.
 Windows
 with domain_version of 
 with following configs: initial_chunk_size_bytes: 
!#!%!%!'!'!)!)!.!.!:!;!@!D!J!M!O!O!
!#!%!%!'!'!)!)!.!.!:!;!J!J!L!M!O!O!
!#%'**,,./:;?@\\
!#%*,/:;?@[]__{{}}
!$!$!&!&!(!(!*!-!/!9!<!?!E!I!N!N!
!$!$!&!&!(!(!*!-!0!3!>!?!E!E!
!%!'!)!,!1!3!M!O!_!
!&$@$J$`$
!(isinf_only && isnan_only)
!(it.GetName().empty())
!/!/!4!4!9!9!<!=!F!I!N!N!
!@!D!K!K!
!]_0t
!~09AZ__az ~09AZ__az09AZaz!/:@[`{~
!>0>Z>
!>j$Y
!0,^,a,a,e,f,h,h,j,j,l,l,q,q,s,t,v,{,
!0<0F0_0i082B2[2
!010=0I0h0
!0V0t0
!allow_zero
!c->in_use() && (c->bin_num != kInvalidBinNum)
!c->in_use() && (c->bin_num == kInvalidBinNum)
!c1->in_use() && !c2->in_use()
!char_tokenezation_ || mincharnum_ < 2
!chunk->in_use()
!coefficients_.empty()
!current_parallel_section
!D$ SV3
!found
!graph.GetInitializedTensor(new_initializer.name(), existing)
!H !H$!H(!H,
!has_axes || attr_axes_.size() == attr_starts_.size()
!helper.HaveTwoTensorInputs()
!input_tensor.IsDataType<std::string>()
!is_concrete_shape_
!is_train_ || ((!saved_mean && !saved_inv_std) || (saved_mean && saved_inv_std))
!IsNonTensor(*node_output)
!mask || mask->Shape() == X_shape
!node_consumers.empty()
!normalize_
!op_type.empty()
!points_.empty()
!pool_strings.empty()
!scale_.empty()
!separators.empty()
!sum_input_moved
!sw.empty()
!This program cannot be run in DOS mode.
!TkjE
!tokenexp.empty()
!using_counters_
!utils::HasExternalData(t_proto)
!ZsN}
" #!#|#|#
" : "
" because it is missing required fields: 
" not supported, expect bilinear, nearest or bicubic
" not supported, expect zeros, border or reflection
" when trying to load "
"&*USMa
", "block_size": [
"0&0*0.02060:0>0S0
"0)0A0l0
"0G0T0k0q0
"0N0b0n0
"0o`SE
"4>\HFN3W
"args" : {
"core": 
"dur" :
"Microsoft Window
"name" :"
"nO@-g
"num_run": 
"ph" : "X",
"pid" :
"thread_id": "
"thread_pool_name": "
"tid" :
"ts" :
"wa co
"XkU-
#"#(#+#{#}#
#&$@$J$
#(#+#&$@$J$
#)#)#h'h'j'j'l'l'n'n'p'p'r'r't't'
#)#*#h'u'
#*#*#i'i'k'k'm'm'o'o'q'q's's'u'u'
#0:0X0t0
#0`0l0x0
#0A0M0Y0e0
#0G0N0`0}0
#0H0`0
#0I0T0s0
$$++<>^^``||~~
$_^[]
$0;0S0
$0<0C0`0j0s0}0
$Microsoft Ireland Operations Limited1
$QQVV
$RVQPRVQP
$RVWS
$VRWjpZ
$VWjoY
$VWQjpZ
-%-'-'-----
%.0Lf
%[n:|
%~3a*
-%-'-'-----0-g-o-o-
-%-'-'-----A
%b %d %H : %M : %S %Y
%d / %m / %y
%H : %M
%H : %M : %S
%hs!%p: 
%hs(%d) tid(%x) %08X %ws
%hs(%u)\%hs!%p: 
%I : %M : %S %p
%m / %d / %y
%Microsoft Windows Production PCA 2011
%Microsoft Windows Production PCA 20110
%o&o&
%WSVR
%Y-%m-%d_%H-%M-%S
&!&!e
&030U0
&BpaGneS
&h FE
&hxiH
&kMtB
&l9IBi
&n&p&g'
&S|9a
&x]c[
(([[{{
(){}[]*+?|.^$\
(. xN
(?HaveMatch:%d)
(?-m:$)
(?-m:^)
(0.0A0G0Z0`0x0
(00080@0H0T0\0
(040V0u0
(070h0
(0f0s0
(0K0x0
(1B1n1
(caller: %p) 
(cannot determine missing fields for lite message)
(d$ f
(D$pf
(default) or 
(float, default 0.5) the ratio of random dropout
(fmod == 0) || (fmod == 1)
(htqE
(inputs_.size() - 1) == i
(int, default 0) if nonzero, run dropout in test mode where the output is simply Y = X.
(items % ngram_size == 0)
(line: 
(local_source >= source) && (local_source < source + num_blocks * blocksize)
(local_source >= source) && (local_source < source + num_blocks * num_elts_in_block)
(local_source >= source) && (local_source < source + num_blocks)
(local_source >= source) && (local_source < source + sizeof(T) * num_blocks)
(name: 
(null)
(op_type:
(Optional) A scalar or rank 1 tensor containing a single value to be filled if the mode chosen is `constant` (by default it is 0.0).
(Optional) Axis along which one-hot representation in added. Default: axis=-1. axis=-1 means that the additional dimension will be inserted as the innermost/last dimension in the output tensor.
(Optional) Axis along which one-hot representation in added. Default: axis=-1. axis=-1 means that the additional dimension will be inserted as the innermost/last dimension in the output tensor. Negative value means counting dimensions from the back. Accepted range is [-r-1, r] where r = rank(indices).
(Optional) Axis along which to take slices. If not specified, input is flattened before elements being selected.
(Optional) Axis along which to take slices. If not specified, input is flattened before elements being selected. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
(Optional) By default, when any value in the 'shape' input is equal to zero the corresponding dimension value is copied from the input tensor dynamically. allowzero=1 indicates that if any value in the 'shape' input is set to zero, the zero value is honored, similar to NumPy.
(Optional) Ending axis for slicing the shape. Negative value means counting dimensions from the back. If omitted, sizes of all axes upto (including) the last one will be included.
(Optional) Index of the diagonal to be populated with ones. Default is 0. If T2 is the output, this op sets T2[i, i+k] = 1. k = 0 populates the main diagonal, k > 0 populates an upper diagonal,  and k < 0 populates a lower diagonal.
(Optional) Seed to the random generator, if not specified we will auto generate one.
(Optional) Specify which axis is batch axis. Must be one of 1 (default), or 0.
(Optional) Specify which axis is time axis. Must be one of 0 (default), or 1.
(Optional) Starting axis for slicing the shape. Default value is 0.Negative value means counting dimensions from the back.
(Optional) The axis of the dequantizing dimension of the input tensor. Ignored for per-tensor quantization. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
(Optional) The axis of the quantization dimension of the input tensor. Ignored for per-tensor quantization. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
(Optional) The data type for the elements of the output tensor, if not specified, we will use int32.
(Optional) The data type for the elements of the output tensor, if not specified, we will use the data type of the input tensor.
(Optional) The data type for the elements of the output tensor. If not specified,the data type of the input tensor T1 is used. If input tensor T1 is also notspecified, then type defaults to 'float'.
(Optional) The data type of the tensors in the output sequence. The default type is 'float'.
(Optional) The dimension to apply unique. If not specified, the unique elements of the flattened input are returned. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
(Optional) The value of the output elements.Should be a one-element tensor. If not specified, it defaults to a tensor of value 0 and datatype float32
(Optional) Whether map negative infinity to true. Default to 1 so that negative infinity induces true. Set this attribute to 0 if negative infinity should be mapped to false.
(Optional) Whether map positive infinity to true. Default to 1 so that positive infinity induces true. Set this attribute to 0 if positive infinity should be mapped to false.
(Optional) Whether to sort the unique elements in ascending order before returning as output. Must be one of 0, or 1 (default).
(outputs_.size() - 1) == i
(Tensor<float>) whose value is the input data tensor scaled element-wise.
(Tensor<T>) where the affine function, y = alpha * x + beta,
(Tensor<T>) where the softplus function, y = alpha * ln(exp(beta * x) + 1), is applied to
(unknown type)
) != (
) != new size (
) != split_dim_size (
) : (
) + (
) + bottom_border (
) + bottomBorder (
) + right_border (
) + rightBorder (
) + scale[0] (
) + scale[1] (
) + scale_[0] (
) + scale_[1] (
) -> (
) and node 
) and outputs (
) and the split dimension of the input (
) are not at boundary of span with size:
) attribute (
) because the ORT planned memory location device 
) bound to different types (
) dimensions are not positive.
) does not exist in the graph.
) does not have type information set by parent node.
) does not have type information.
) does not match expected type (
) does not match number of inputdimensions values (
) does not match the data size(
) does not match the number of channels (
) first dimension size does not equal NNZ.
) for attribute 'axis'
) for tensor of length:
) from file 
) has 
) has input size 
) has more inputs (
) has more outputs (
) has no index values.
) has output size 
) has zero input and zero output.
) in kernel registries for 
) in node (
) in op definition.
) in proto
) index value at position [
) input arg (
) is 0-element but contains data!
) is invalid.
) is not equal to number of scan inputs (
) is not equal to number of scan outputs (
) is not equal to the existing dim value (
) is not equal to the existing rank value (
) is required but not specified.
) is stored externally and should not have data field.
) is stored externally but doesn't have a location.
) must have a dense-rank > 0
) must have INT64 type.
) must have rank 1 or 2.
) must have rank 1.
) must have the same length. 
) needs to be greater than or equal to the left_border (
) needs to be greater than or equal to the leftBorder (
) needs to be greater than or equal to the top_border (
) needs to be greater than or equal to the topBorder (
) node with name '
) of node (
) of operator (
) of Optype (
) of output arg (
) Op (
) or 1
) output arg (
) second dimension size does not match rank of tensor.
) should be stored in 
) should contain one and only one value field.
) should not be stored in raw_data field
) should not contain more than one value field.
) should refer to attribute in parent node.
) specified for sequence of size (
) than declared (
) to UNDEFINED is not allowed
) type inference failed
) vs (
)".".$.$.&.&.(.(.B.B.
)#.#.%.%.'.'.).).
)) , expected: (
))]]}}
), but the current device does not support 16-bit float.
), input tensor data type (
)080R0^0c0u0
)8_^[]
)D$`;T$
)D$`v
)D$p;|$
)Microsoft Root Certificate Authority 20100
)'s input 
)'s output 
)s+v+
*>{69
-*0/0
*0+D+G+L+)
-*0-0
*0H0Y0h0
*out_size >= 0
*TB_Xd
*wmUrK "
, am_attn_size}, Got:
, aw_attn_size}. Got:
, block in memory pattern size is: 
, but it doesn't exist or is not accessible.
, but it is already registered from file 
, but it its domain is not
, but it its version is higher
, but its domain is not
, but its version is not 
, column 
, data shape: 
, Error 
, error code: 
, expect 2
, external_data.length: 
, fall back to default allocation behavior
, got 
, Got 
, has unsupported type: 
, indices shape: 
, max supported IR version: 
, max=
, node name: 
, requested shape:
, type: 
,.,`,`,b,d,g,g,i,i,k,k,m,p,r,r,u,u,~,
,.,0,^,`
,.,0,^,`,
,;L;T;\;h;
,+_1K
,>0>4>8><>@>D>H>L>P>T>X>\>`>
,0@0S0`0
,ASApw7gBYXBcDWocFzTlsOwO1K9CneYG12fQ0xzcO3c=0Z
,P.Q.
,p-p-
,SVWj
. . .
'. 0 == forward. 1 == reverse.
. batch_size=
. bin_num:
. Can't constant fold 
. Dimension 0 is 
. Do you have duplicated calls to SessionState::AddInitializedTensor function?
. Error message 
'. Error message 
. Execution Provider must generate unique names across the entire model.
. Execution will fail if ORT does not have a specialized kernel for this op
. Expected:
. Falling back to lenient merge.
. Ignoring allocator from 
. Index:
. Input rank=
. Input tensor rank was 
. Invalid ORT format model.
. It can only be 
'. It is no longer used by any node.
'. It is not used by any node and should be removed from the model.
. Must be 0 or 1
'. Must be one of 'forward', 'reverse', or 'bidirectional'.
. No opset import for domain
. No schema registered for this operator.
. Num args is 
. Output tensor rank was 
. Please, fix your model.
. Shape:
. shape=
. The shape is: 
'. Valid values are 'LEFT' or 'RIGHT'.
. Validate usage of dim_value (values should be > 0) and dim_param (all values with the same string should equate to the same size) in shapes in the model.
. Value must be in range [0,
.!.!.0
.*...0.9.<.?.A.A.C.O.R.R.
...0.O.R.R.
.:.;.@.@.
.?AU?$Abs@_J@functors@onnxruntime@@
.?AU?$Abs@_K@functors@onnxruntime@@
.?AU?$Abs@C@functors@onnxruntime@@
.?AU?$Abs@E@functors@onnxruntime@@
.?AU?$Abs@F@functors@onnxruntime@@
.?AU?$Abs@G@functors@onnxruntime@@
.?AU?$Abs@H@functors@onnxruntime@@
.?AU?$Abs@I@functors@onnxruntime@@
.?AU?$Abs@M@functors@onnxruntime@@
.?AU?$Abs@N@functors@onnxruntime@@
.?AU?$Ceil@M@functors@onnxruntime@@
.?AU?$Celu@M@functors@onnxruntime@@
.?AU?$default_delete@VBFCArena@onnxruntime@@@std@@
.?AU?$default_delete@VCPUExecutionProvider@onnxruntime@@@std@@
.?AU?$default_delete@VIAllocator@onnxruntime@@@std@@
.?AU?$default_delete@VIExecutionProvider@onnxruntime@@@std@@
.?AU?$default_delete@VModel@onnxruntime@@@std@@
.?AU?$Elu@M@functors@onnxruntime@@
.?AU?$Exp@M@functors@onnxruntime@@
.?AU?$Exp@N@functors@onnxruntime@@
.?AU?$Floor@M@functors@onnxruntime@@
.?AU?$HardSigmoid@M@functors@onnxruntime@@
.?AU?$LeakyRelu@M@functors@onnxruntime@@
.?AU?$Log@M@functors@onnxruntime@@
.?AU?$Log@N@functors@onnxruntime@@
.?AU?$MaxPool1DTask@C@onnxruntime@@
.?AU?$MaxPool1DTask@E@onnxruntime@@
.?AU?$MaxPool1DTask@M@onnxruntime@@
.?AU?$MaxPool1DTask@N@onnxruntime@@
.?AU?$MaxPool2DTask@C@onnxruntime@@
.?AU?$MaxPool2DTask@E@onnxruntime@@
.?AU?$MaxPool2DTask@M@onnxruntime@@
.?AU?$MaxPool2DTask@N@onnxruntime@@
.?AU?$MaxPool3DTask@C@onnxruntime@@
.?AU?$MaxPool3DTask@E@onnxruntime@@
.?AU?$MaxPool3DTask@M@onnxruntime@@
.?AU?$MaxPool3DTask@N@onnxruntime@@
.?AU?$MaxpoolWithMask1DTask@M@contrib@onnxruntime@@
.?AU?$MaxpoolWithMask2DTask@M@contrib@onnxruntime@@
.?AU?$MaxpoolWithMask3DTask@M@contrib@onnxruntime@@
.?AU?$Neg@_J@functors@onnxruntime@@
.?AU?$Neg@C@functors@onnxruntime@@
.?AU?$Neg@H@functors@onnxruntime@@
.?AU?$Neg@M@functors@onnxruntime@@
.?AU?$Neg@N@functors@onnxruntime@@
.?AU?$ParametricSoftplus@M@functors@onnxruntime@@
.?AU?$Pool1DTask@MVLpPool@onnxruntime@@@onnxruntime@@
.?AU?$Pool2DTask@MVLpPool@onnxruntime@@@onnxruntime@@
.?AU?$Pool3DTask@MVLpPool@onnxruntime@@@onnxruntime@@
.?AU?$Powx@M@functors@onnxruntime@@
.?AU?$QLinearPool1DTask@EVAveragePool@onnxruntime@@@contrib@onnxruntime@@
.?AU?$QLinearPool2DTask@EVAveragePool@onnxruntime@@@contrib@onnxruntime@@
.?AU?$QLinearPool3DTask@EVAveragePool@onnxruntime@@@contrib@onnxruntime@@
.?AU?$QLinearPoolNhwc1DTask@EVAveragePool@onnxruntime@@@contrib@onnxruntime@@
.?AU?$QLinearPoolNhwc2DTask@EVAveragePool@onnxruntime@@@contrib@onnxruntime@@
.?AU?$QLinearPoolNhwc3DTask@EVAveragePool@onnxruntime@@@contrib@onnxruntime@@
.?AU?$Reciprocal@M@functors@onnxruntime@@
.?AU?$Reciprocal@N@functors@onnxruntime@@
.?AU?$Relu@C@functors@onnxruntime@@
.?AU?$Relu@H@functors@onnxruntime@@
.?AU?$Relu@M@functors@onnxruntime@@
.?AU?$Relu@N@functors@onnxruntime@@
.?AU?$ScaledTanh@M@functors@onnxruntime@@
.?AU?$Selu@M@functors@onnxruntime@@
.?AU?$Sigmoid@M@functors@onnxruntime@@
.?AU?$Sigmoid@N@functors@onnxruntime@@
.?AU?$Softplus@M@functors@onnxruntime@@
.?AU?$Softsign@M@functors@onnxruntime@@
.?AU?$Sqrt@M@functors@onnxruntime@@
.?AU?$Sqrt@N@functors@onnxruntime@@
.?AU?$Tanh@M@functors@onnxruntime@@
.?AU?$Tanh@N@functors@onnxruntime@@
.?AU?$ThresholdedRelu@M@functors@onnxruntime@@
.?AU_Crt_new_delete@std@@
.?AUctype_base@std@@
.?AUException@Ort@@
.?AUhresult_access_denied@winrt@@
.?AUhresult_canceled@winrt@@
.?AUhresult_changed_state@winrt@@
.?AUhresult_class_not_available@winrt@@
.?AUhresult_class_not_registered@winrt@@
.?AUhresult_error@winrt@@
.?AUhresult_illegal_delegate_assignment@winrt@@
.?AUhresult_illegal_method_call@winrt@@
.?AUhresult_illegal_state_change@winrt@@
.?AUhresult_invalid_argument@winrt@@
.?AUhresult_no_interface@winrt@@
.?AUhresult_not_implemented@winrt@@
.?AUhresult_out_of_bounds@winrt@@
.?AUhresult_wrong_thread@winrt@@
.?AUmessages_base@std@@
.?AUmoney_base@std@@
.?AUNodeCompare@onnxruntime@@
.?AUPriorityNodeCompare@onnxruntime@@
.?AUtime_base@std@@
.?AV?$_Iosb@H@std@@
.?AV?$_Mpunct@_W@std@@
.?AV?$_Mpunct@D@std@@
.?AV?$_Mpunct@G@std@@
.?AV?$basic_filebuf@DU?$char_traits@D@std@@@std@@
.?AV?$basic_ios@DU?$char_traits@D@std@@@std@@
.?AV?$basic_ostream@DU?$char_traits@D@std@@@std@@
.?AV?$basic_streambuf@DU?$char_traits@D@std@@@std@@
.?AV?$codecvt@_WDU_Mbstatet@@@std@@
.?AV?$codecvt@DDU_Mbstatet@@@std@@
.?AV?$codecvt@GDU_Mbstatet@@@std@@
.?AV?$collate@_W@std@@
.?AV?$collate@D@std@@
.?AV?$collate@G@std@@
.?AV?$ctype@_W@std@@
.?AV?$ctype@D@std@@
.?AV?$ctype@G@std@@
.?AV?$messages@_W@std@@
.?AV?$messages@D@std@@
.?AV?$messages@G@std@@
.?AV?$money_get@_WV?$istreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$money_get@DV?$istreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$money_get@GV?$istreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$money_put@_WV?$ostreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$money_put@DV?$ostreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$money_put@GV?$ostreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$moneypunct@_W$00@std@@
.?AV?$moneypunct@_W$0A@@std@@
.?AV?$moneypunct@D$00@std@@
.?AV?$moneypunct@D$0A@@std@@
.?AV?$moneypunct@G$00@std@@
.?AV?$moneypunct@G$0A@@std@@
.?AV?$num_get@_WV?$istreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$num_get@DV?$istreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$num_get@GV?$istreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$num_put@_WV?$ostreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$num_put@DV?$ostreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$num_put@GV?$ostreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$numpunct@_W@std@@
.?AV?$numpunct@D@std@@
.?AV?$numpunct@G@std@@
.?AV?$time_get@_WV?$istreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$time_get@DV?$istreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$time_get@GV?$istreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV?$time_put@_WV?$ostreambuf_iterator@_WU?$char_traits@_W@std@@@std@@@std@@
.?AV?$time_put@DV?$ostreambuf_iterator@DU?$char_traits@D@std@@@std@@@std@@
.?AV?$time_put@GV?$ostreambuf_iterator@GU?$char_traits@G@std@@@std@@@std@@
.?AV_Facet_base@std@@
.?AV_Generic_error_category@std@@
.?AV_Iostream_error_category2@std@@
.?AV_Locimp@locale@std@@
.?AV_System_error@std@@
.?AV<lambda_001ec6ab06d68d1c890e5bf445b4d487>@@
.?AV<lambda_0031e1631721c75a1f7a0e71cb748ed7>@@
.?AV<lambda_006f042491572090b778a6887556c541>@@
.?AV<lambda_007ddac2123b71f70c18b64d41e9be57>@@
.?AV<lambda_009e82d406f51c12618b944e70e2ad71>@@
.?AV<lambda_00b59445cb71ad9abff9006cdec65ab0>@@
.?AV<lambda_0118a8ab65a6f61a54e801a4c2f6a842>@@
.?AV<lambda_01a9f753d9bd01cb2bfad855ca7f10c7>@@
.?AV<lambda_02dbf81fbe0e79782741ce39a831bb2f>@@
.?AV<lambda_02f0fede11d4b9d7439baa125e06788b>@@
.?AV<lambda_031e8dfd8d6a123502d9ec2194443e60>@@
.?AV<lambda_034a872241814873b16b150e98786486>@@
.?AV<lambda_0367bcb44eefe2d25755a8cf3194c733>@@
.?AV<lambda_03c53d5c540d06d25e58298bd3e2675f>@@
.?AV<lambda_04111ef4993b4b7660410cb114680c48>@@
.?AV<lambda_043c78e85491bd0db92f03f63f8c56ae>@@
.?AV<lambda_048b899a3305cb2c0c070dd18e5b4d17>@@
.?AV<lambda_057831c89172b3e745c34953b854d875>@@
.?AV<lambda_0579b41fc30dc7704e4037c901365f86>@@
.?AV<lambda_05b97968a36f0f68189b0f8fe2c5855e>@@
.?AV<lambda_061ce113b6886684397fa36d116f6701>@@
.?AV<lambda_06e5eb766e97cbbd1e9836fc044820b5>@@
.?AV<lambda_07364fb607ea89235f416789a046b92d>@@
.?AV<lambda_076f399d54601fd1059236c6a3813828>@@
.?AV<lambda_077886625d25c359a62527fbaa92e49b>@@
.?AV<lambda_077c87efaa9c726a84085e4b8a5f0dfc>@@
.?AV<lambda_07c226046fe685e37979217c354766b5>@@
.?AV<lambda_08b89006bb2cd45177aec966a5a64a25>@@
.?AV<lambda_09860caeb18fe5c6553c7cece043a6bf>@@
.?AV<lambda_0a0326aaa0c17e1dc10459d8b6c3398c>@@
.?AV<lambda_0a77da8549581038807d4f031ad4715d>@@
.?AV<lambda_0a786afdc494302a03a8347211af4f5e>@@
.?AV<lambda_0a9bedc68e3bb7f405a0cb52fae05ca5>@@
.?AV<lambda_0aa1cbb10b7e27c8eaa9e1c4d936a012>@@
.?AV<lambda_0adf0d24f2c35fa4a66c3d2e349117c6>@@
.?AV<lambda_0af4c846dd5af6632beb77ffbd6469c9>@@
.?AV<lambda_0b7b88c72b3c6e2d1499af326149f2ac>@@
.?AV<lambda_0b84cd883df2cf8f5da7751da99be58c>@@
.?AV<lambda_0ba48c1f3299a5f4228b96a5876e6f39>@@
.?AV<lambda_0ce942d577225b307b9474951f72244f>@@
.?AV<lambda_0d1cea9b21275bcc722a263f1867aab0>@@
.?AV<lambda_0d228c1c046c6169ab9bdaf46172d4b8>@@
.?AV<lambda_0d37934dc4686e9f8cdc4b98c71d039f>@@
.?AV<lambda_0d76ad475b70d10280a0f03bb12202c0>@@
.?AV<lambda_0d82943818680ad159348f56b6d25693>@@
.?AV<lambda_0ec02d5276c6a16fd616d10b796f4d2c>@@
.?AV<lambda_0f569c3741dec4cbd25547f5cfa47a1f>@@
.?AV<lambda_0f8bfbc63e2fcc8494bd65f666c7791b>@@
.?AV<lambda_0f9a7ecc0cde6f755170ea655764ab2c>@@
.?AV<lambda_0fb4e93f4014e9a9ed52287648f86c91>@@
.?AV<lambda_0fe8093ffea6514602a81f131e8a213b>@@
.?AV<lambda_104e5d4f3c346a5807a7db11389af990>@@
.?AV<lambda_10a89fcba82ae9af0fd4de45f400c57a>@@
.?AV<lambda_10f3982d8c74497eebbba2b59c0e4116>@@
.?AV<lambda_1147aa6d38b42c0a8559813b3004180f>@@
.?AV<lambda_12065ee63fda2ae25d17bc16a002c579>@@
.?AV<lambda_125b77507bd9658139842e91e09bb467>@@
.?AV<lambda_12dd4c4dcfd6c2d8effe6071c823bd43>@@
.?AV<lambda_132f150dbe2a6661cedd4acd928f9ad6>@@
.?AV<lambda_13bf14abed62c4ddb7d24aa834c66cd3>@@
.?AV<lambda_140ab31565a9f257f202ece6453c4bfd>@@
.?AV<lambda_140d9b670661cea14ac8b7667467087d>@@
.?AV<lambda_1433794861c06b7d98f325651aa885c9>@@
.?AV<lambda_1433cb9cd8f8490322ad74b6fa812120>@@
.?AV<lambda_14407bcf8cfc66db9a85fd2745a31d94>@@
.?AV<lambda_1476157c6a284e4f379191854345eea5>@@
.?AV<lambda_14ab4d68c965e23bff80a9edfde3b16e>@@
.?AV<lambda_14cf9ddcdab5183a36b1158dec720c25>@@
.?AV<lambda_159e0921f0d4eb52aba9cdc091e10630>@@
.?AV<lambda_15ec4af4e71787ba502de1205d84eea4>@@
.?AV<lambda_1642adc2d95a594ec2d1ca1c1679605d>@@
.?AV<lambda_16aeacf7ad4b53ba550defbc9c03abde>@@
.?AV<lambda_16bdac8a01801cd7a5fcb1f21cc7a9f0>@@
.?AV<lambda_16be1195897e7eadc7afc6c2ac4f5c83>@@
.?AV<lambda_175914cea4037e183fc735b60abe96c9>@@
.?AV<lambda_18dbcf0a6112b471cd3435637c32f0f8>@@
.?AV<lambda_18e0b70d8d5a276d72055cc8661094dd>@@
.?AV<lambda_19294b02f6e15956b08baa928b37992a>@@
.?AV<lambda_192a5a4ddd61389a8d1be3b6ca8c764b>@@
.?AV<lambda_199eec5d9457d35704c78b6cd8a09f57>@@
.?AV<lambda_19a25a0e0bced01a01388502a5fb897a>@@
.?AV<lambda_19ac80b5fbb34feb84f95acd76dbb931>@@
.?AV<lambda_1abd5511a80f7bfc6909f2fad522d987>@@
.?AV<lambda_1c17b461588ff1b22a6f9497bff5b28b>@@
.?AV<lambda_1cd6b5a2cebd273f7225f466512c43d5>@@
.?AV<lambda_1cdda74d195f4dc7bbdbaed3ee174bf7>@@
.?AV<lambda_1d18bde1e76bce053ccc17b3ed08cad8>@@
.?AV<lambda_1da5929b711c2681e91623a20d50a98f>@@
.?AV<lambda_1dab84a38765585bcd86328d60f37519>@@
.?AV<lambda_1e1df0082e8fe003ce7e63ccb14a6aa2>@@
.?AV<lambda_1e62541677783c96e5b936f0f682ca92>@@
.?AV<lambda_1e6c58f2c47fbfdcb340ca8aa595e354>@@
.?AV<lambda_1eb06dcafd75d6cff15c185443e559ed>@@
.?AV<lambda_1f154a6fecf26a7ac5bcd91fb4939855>@@
.?AV<lambda_1f77acb4e34034b00a5a2150384c9d77>@@
.?AV<lambda_1f810ffbf9d1e55e1001a4c64ceea3d9>@@
.?AV<lambda_2047474ce4cc4fb3591bd2cb304e60de>@@
.?AV<lambda_20ce3835ea17537cf13d089eb1a443c1>@@
.?AV<lambda_2122617e64cb8af81fe74e4dbd5b36b4>@@
.?AV<lambda_217211e0b9216fbae93bbaf0026e78ee>@@
.?AV<lambda_217727e3bc662f496961b1bb8f9e821c>@@
.?AV<lambda_221457049dd6e599b1e592be3898266a>@@
.?AV<lambda_221cfc4539809ca9dda24743972ab18a>@@
.?AV<lambda_22485ec429c0920bef998442ed4b0141>@@
.?AV<lambda_227060e20de8b811822001a4d5e06e66>@@
.?AV<lambda_22a56cf2842ab7e44f2594d178dc2c1e>@@
.?AV<lambda_22dae262e3744994c648a6a28f79603c>@@
.?AV<lambda_23fc752a2fe808aaecaafb193b1a6fe4>@@
.?AV<lambda_245134a5955e4a8621bc7afd20ddbe70>@@
.?AV<lambda_2529e063101eea511edf1238da0f8253>@@
.?AV<lambda_253a3836eec6c1ddbcb7d28211e21bd6>@@
.?AV<lambda_2615da15ba53b69982b830ee817c81ee>@@
.?AV<lambda_26de9b5e95466f00708e3ae83a84c608>@@
.?AV<lambda_26ee2a828034f17afc207c8ad30c609a>@@
.?AV<lambda_273270d63d2edeeb9cb50e9432a42c00>@@
.?AV<lambda_27534cc625d5ab04dc11bfb2c47d616f>@@
.?AV<lambda_27a631d2450c357a52927c1dfbd2efda>@@
.?AV<lambda_28d76a4078a4a8dbd4ec44ed08d36b25>@@
.?AV<lambda_29460ba72c9a9f61937f695b212c2385>@@
.?AV<lambda_295cf861cf0e14c55f05d3a6f16e5476>@@
.?AV<lambda_2997ea06f466f3cb7af7580139e0e9b1>@@
.?AV<lambda_2a6ee45563922231f230ab0594ec8ab5>@@
.?AV<lambda_2a976ac88d60c988f6c82762c8669484>@@
.?AV<lambda_2b360603aec7588daf58b55d3e48bc2d>@@
.?AV<lambda_2b438ab8008a3839c2ba42ed80d39f62>@@
.?AV<lambda_2b4b545989d1d9ef9003a30b5e0ca43b>@@
.?AV<lambda_2bb569fe2197271e2b1302ced7b75d4a>@@
.?AV<lambda_2bc5f319d61b1e5d5273f7fc52f04186>@@
.?AV<lambda_2bed918e0092d38de095a3dfcc39bb6a>@@
.?AV<lambda_2cdbc5f873c2ce32c36481fab53d6870>@@
.?AV<lambda_2ce98da2a528969c49f46cb09a7b6eee>@@
.?AV<lambda_2d30aee68f6d7fc70a227ffb90481678>@@
.?AV<lambda_2d779c3a3c8b726adf3efb1bfd3cbb40>@@
.?AV<lambda_2dc16cf30de15202e0434934c8ec0571>@@
.?AV<lambda_2e2760691203ec6d385de10317d43eb8>@@
.?AV<lambda_2e43ccf94d692d90fc25b9f3f15b1f3a>@@
.?AV<lambda_2e79487384136cbeb73bf5cd55164ae5>@@
.?AV<lambda_2ed99b1b7f84cedcec6ea06ce6c01dc5>@@
.?AV<lambda_2f25b8f6b95e7cf36d88a2facd4ae1ab>@@
.?AV<lambda_2f4103e1c4a1773101a6c1f055d56df4>@@
.?AV<lambda_2f6217725d36a8fee3233a5a3a8acbc4>@@
.?AV<lambda_2f8e437cb9f95dd76b3982067268238d>@@
.?AV<lambda_2fd33bbe7c4a2b258d4d665b8341f671>@@
.?AV<lambda_301279dcdc0bb6da9000a75437e1d615>@@
.?AV<lambda_30bb158bcf23a957ab7bf46d0a465fb6>@@
.?AV<lambda_31433bd7aba335c4dcfc2d21c7b0556b>@@
.?AV<lambda_3199312d6fe856623dcfbc1d675d418f>@@
.?AV<lambda_31da21da588bb3698a015fc212bb17cf>@@
.?AV<lambda_331d7506767c1ed93515cbb23d39b0d9>@@
.?AV<lambda_3354f944db7877754471d985be3fe29b>@@
.?AV<lambda_33763db33c7430e0074fd25b65b4479f>@@
.?AV<lambda_339aa899e5780231e41cee84a9ac8a33>@@
.?AV<lambda_33c11271a908f4f215dd361ca0be165e>@@
.?AV<lambda_34078c7540b6602389f40bae3a86d150>@@
.?AV<lambda_348952d4e8cb4d6ca40b91f67d9fe4cc>@@
.?AV<lambda_348a4c4053fb0e4fb238401e94ac3418>@@
.?AV<lambda_34f127269096bdecdd81b55565792ef7>@@
.?AV<lambda_34f93c0ee84ed95033247b350e3e9c13>@@
.?AV<lambda_3535579646aa11381d1f86e8f1dad915>@@
.?AV<lambda_35452b6fa29b4a84e3a3d83c879b06cd>@@
.?AV<lambda_359fe019cdce504d42df9f98cd05ca23>@@
.?AV<lambda_35b794944dfe770731cae452127f793f>@@
.?AV<lambda_35d59ef9b84b9c29d8badb1b46db4e70>@@
.?AV<lambda_36945aef9b69b8bc246158e60ded74ff>@@
.?AV<lambda_36c84409ed98093c11b3d0d02c089264>@@
.?AV<lambda_36dfc92b17b40b8d41cb63bf3c56bb44>@@
.?AV<lambda_375dfab7231b0fed5cbc2d28f45f1de4>@@
.?AV<lambda_37fc46271b5a9d577e78557058b76819>@@
.?AV<lambda_38e1bdb2070443b647c40452b6c90c15>@@
.?AV<lambda_39d5079814e87d3ff2cd3537d46812cc>@@
.?AV<lambda_3a5e9806f5a74aa166ab1dd78ca1a933>@@
.?AV<lambda_3a80fca790cdb0fce14ddaedefa20351>@@
.?AV<lambda_3ad3f00f380cbc79acdede7031c2b35c>@@
.?AV<lambda_3bcb9b903ba1cf4e300afd0f291b2ee4>@@
.?AV<lambda_3bf9d7b5239a137326d2c5fa821fa737>@@
.?AV<lambda_3c57c4f415fca3790b9eb6cedd38e28f>@@
.?AV<lambda_3c5ccc04791f73730d9ca40d92c258b4>@@
.?AV<lambda_3cd6c3d57d7e8805d712028bde485ef8>@@
.?AV<lambda_3ce4407e5879111e6c1a6435d36077e4>@@
.?AV<lambda_3d55206f062b0cb6d4082a57d197185d>@@
.?AV<lambda_3deb1638445ba1a8c860cf9e9ece0b32>@@
.?AV<lambda_3e2860b55958cf532cc6672e843fd5e5>@@
.?AV<lambda_3e31c198cbd0b452ff5ea25fd603eaa6>@@
.?AV<lambda_3e4d0130fa8e6aaa56df4817dcf6c190>@@
.?AV<lambda_3e5e22c83296c8f56ae7effebfec7009>@@
.?AV<lambda_3e8fe92fc2a983eaaae8e23df9000d98>@@
.?AV<lambda_3e9031054352b07bad13c7fb5c2ce7bb>@@
.?AV<lambda_3ec6b12ad4cd55e04d715e409cf94caa>@@
.?AV<lambda_3f231ead7bd5577794abe2ca66a64f06>@@
.?AV<lambda_3f2f36db15d672a75a701714494919da>@@
.?AV<lambda_3f849c6f51e31293c619565ac636bda7>@@
.?AV<lambda_404d0fe71cc8d7867c9f1bc290b28bbd>@@
.?AV<lambda_404f128c43627cca26dd30079f566078>@@
.?AV<lambda_4098378d7ad3a9be3cb392d1987f5a82>@@
.?AV<lambda_40a7ea59743a73ab67f43e4e9682b9a3>@@
.?AV<lambda_40d7d549b7296d37bd8a375a6a32735f>@@
.?AV<lambda_40f74427ec28f20a5d59b70a3f7ba162>@@
.?AV<lambda_414a43a198ea3ff0dfdcf4ed89cfe63a>@@
.?AV<lambda_4177af3ea58d3ef77d743843af9cc9e4>@@
.?AV<lambda_41ab0f59dafb4a0526d00edffc434f5b>@@
.?AV<lambda_41ba827606a0dc44b626761483b4bac9>@@
.?AV<lambda_423c7f4514cb0f580d17c1969d94ab4d>@@
.?AV<lambda_4276ac6945fda74461cbbc3f55ea202e>@@
.?AV<lambda_42f0399bf2e271e6e951294a5aec23bb>@@
.?AV<lambda_42fc8c33eb035f61f8d39bab0f514aa0>@@
.?AV<lambda_43719f9d8c83073f26885f37cf523fe1>@@
.?AV<lambda_43a24ff760813c6ddd87379f0ec23ef7>@@
.?AV<lambda_43d76a454d7e446551a32b163679964a>@@
.?AV<lambda_43ec4bbe506690d5dbfb40f909da196a>@@
.?AV<lambda_43f02a942b5a7f30668fcdafd5ea2fac>@@
.?AV<lambda_446c3abfba59357df64608aa0ee4e290>@@
.?AV<lambda_457d048bb1398944d91bd06e08be9004>@@
.?AV<lambda_458163864faccd230ccf1ce10d3e187d>@@
.?AV<lambda_45afddba11e0bff90c21310e9f91b09f>@@
.?AV<lambda_45c4687729fcbb9b32a11320d77112ec>@@
.?AV<lambda_45e08ad47c9bff0006768a2c526864a3>@@
.?AV<lambda_45e1f6e47762148b282b3bb16964d9a7>@@
.?AV<lambda_45f451203469e52634465d9e0804de20>@@
.?AV<lambda_466bfce1c88393bfc4930b50054f11e7>@@
.?AV<lambda_4682f43d0273e3d01e9224f8a5dcf803>@@
.?AV<lambda_47109259520c4e793b98906acbd4073b>@@
.?AV<lambda_4775b089fde8e28dff85f398113525c6>@@
.?AV<lambda_4782a3788b4d37949c863bd3b1d2db4e>@@
.?AV<lambda_47f0845d7b66643be4c58515f34249ea>@@
.?AV<lambda_486023130535c54dc87259934f24df6e>@@
.?AV<lambda_4895a69c6e2d4d3665d84240b52946d2>@@
.?AV<lambda_491a5ff9baf0c87e80ac72a0beb259ee>@@
.?AV<lambda_4a58e5f6034c78ef31ef5d51eb58949a>@@
.?AV<lambda_4a61ff195999a8a104bf36ba131bf403>@@
.?AV<lambda_4a6aeb4175d7888941bfb44f87a822c0>@@
.?AV<lambda_4a8ce7446699411c4438ae92f5cf818a>@@
.?AV<lambda_4ad3f36a5fcdf1d188458be0a989ed3e>@@
.?AV<lambda_4ae79009ffed4baaf3b13205e614d6ec>@@
.?AV<lambda_4b1533207937d10b38e9bd484b05643c>@@
.?AV<lambda_4b40e91ae8e8c37fb63980fee637f885>@@
.?AV<lambda_4c0d5472c0e9893aea307d804bd55070>@@
.?AV<lambda_4c798c4f860875abb86117d8e4ba8ff1>@@
.?AV<lambda_4c81a7b179f9e26e2d05ebefbb83c381>@@
.?AV<lambda_4d3103e3f4278a6d0b0a41cd1fe918ee>@@
.?AV<lambda_4d8164c41daa6332527fd7a024ba3da7>@@
.?AV<lambda_4e6fa5fab57276f6c38287bd072060d0>@@
.?AV<lambda_4e978ba75a8a0f3f4d2d1e75d74ac4de>@@
.?AV<lambda_4ed1fe6920001affda0cd1722766e0e3>@@
.?AV<lambda_4ee11195e6ac34d8802828c9de8931d7>@@
.?AV<lambda_4f86db608486d9d830b4d71843ace9e1>@@
.?AV<lambda_4ff3da6778c55d2ab99a53821a349232>@@
.?AV<lambda_5016737842528dd11679bfef0d0ab738>@@
.?AV<lambda_50bf1563c4f2a658d5a1f8997cf7d841>@@
.?AV<lambda_510b48fcc953c4af1dbcb4b8cf19756a>@@
.?AV<lambda_5151cdb5e2d0e967b0ff9b516f4dde5e>@@
.?AV<lambda_5169332acacd2cda6014329563a49097>@@
.?AV<lambda_519fe9cae569b1ad1c3f73288fa352d4>@@
.?AV<lambda_51fc723ea39c4518ba8a40d8ce3fda32>@@
.?AV<lambda_51ff266d4a509f82cc61194622d6aafc>@@
.?AV<lambda_522944c70ac483ca59ad2373ac030c86>@@
.?AV<lambda_5241072fb302daf3d92fe8af4902fcf1>@@
.?AV<lambda_52a86257ba105ca00a347edc7c75852c>@@
.?AV<lambda_53687b87e8fc26669694bb154ed06213>@@
.?AV<lambda_53958a524045125d538038a834c170f9>@@
.?AV<lambda_54b4204c0ce4d49d0ae3ff6e8b9800a4>@@
.?AV<lambda_556d5dc9dce2234c43f6705a2f6c7701>@@
.?AV<lambda_55a8c7f82471609cdeead8ac3d02d601>@@
.?AV<lambda_55e8e059d201c280196491784367b3c1>@@
.?AV<lambda_56054d16adda54f2046f2f8778fd36d1>@@
.?AV<lambda_566e56833ba647bded2d1d52059227e2>@@
.?AV<lambda_56708f1224b2fa079c579dbcc99f5723>@@
.?AV<lambda_56724cedf7b3d45672d6aaa9b61c2b1a>@@
.?AV<lambda_56c3ac63d8a5c9a2d5ed84f082ef3cf5>@@
.?AV<lambda_577697c65f7f30ab5c890fb5e643c3c6>@@
.?AV<lambda_57db475e5df80ef36653d188ca9951ac>@@
.?AV<lambda_58a57dc152175dcefed537af0e4224e8>@@
.?AV<lambda_58ba1217675fabc528cd9f4eff671e5e>@@
.?AV<lambda_58e45bcf81f2eb4f6ae6b3f124defb13>@@
.?AV<lambda_590e59531275857afafc7e97ec0dad13>@@
.?AV<lambda_5949c53d8ff4275e91f18552fb52fd7c>@@
.?AV<lambda_5977a43abcce90501d97902d58330cfd>@@
.?AV<lambda_597c15d719a814d12541cbd2bbff60a5>@@
.?AV<lambda_598c6d249dbd14226af8e69fe738f910>@@
.?AV<lambda_5996e886d07d4d6c6b33ab2939ab88dc>@@
.?AV<lambda_59c51bd67b590a247c9762a18455397f>@@
.?AV<lambda_59d9acea5faf21b910fd8b35ea3c81b3>@@
.?AV<lambda_59ed2dcfa7c0d744c558df76076fb0aa>@@
.?AV<lambda_5b19bcf90126dbac847fa4a3a03b8aab>@@
.?AV<lambda_5bd441e42294bfab33849ec1b22ce125>@@
.?AV<lambda_5c5ac1f6c71d812ad45802a5c8ea5757>@@
.?AV<lambda_5c90896dede9afea4c2bfc57d475c2a5>@@
.?AV<lambda_5ca3b2fb2261ad467c7e40fda304e71e>@@
.?AV<lambda_5cc43c49af3b89fd43e2aa8a8afc2e1b>@@
.?AV<lambda_5d86fa40c6f141e2f5308ab25a37373d>@@
.?AV<lambda_5da63fe37738ec572cb4587955372cb4>@@
.?AV<lambda_5dddc316c01bb02791c37b03fa523bb0>@@
.?AV<lambda_5de0fe5ec3ad834b64888d20361865ce>@@
.?AV<lambda_5de94ebf8ac34b72c76fcea7a63f223e>@@
.?AV<lambda_5e0adb550519bb305a282a49bc052ff5>@@
.?AV<lambda_5e0f6565dc7aa7f6ebc79fe2fd37d8ea>@@
.?AV<lambda_5e6f4957377cd9027386cbe3c8840003>@@
.?AV<lambda_5e8795de6046262c724adaee055c9399>@@
.?AV<lambda_5ead457e3eeae74d2baaa54a34ab876e>@@
.?AV<lambda_5ef99cfa5845f4c4d490717f4ebe278d>@@
.?AV<lambda_604acd6b059bd301f8025e011790c72a>@@
.?AV<lambda_609ecd802714b672c2c81632a57c73f4>@@
.?AV<lambda_6105a132d5626ddb9b2ed4958c88e1e2>@@
.?AV<lambda_6107f589d79fa5fffbc0daf95ca9b592>@@
.?AV<lambda_62479ee4cba0a1701384406c477dee66>@@
.?AV<lambda_62526b7eac31a4fea5091c2f348a5e23>@@
.?AV<lambda_62f1ce9bc208e37c7e5739c8f36388ed>@@
.?AV<lambda_6367df6aa050372a33bd7465b9bffebf>@@
.?AV<lambda_637049d8de28c0182bf4a2696729caaa>@@
.?AV<lambda_637d27d6a01beef351cd6df162f9426e>@@
.?AV<lambda_6423e25c1122e8ea98faabe27ee57b24>@@
.?AV<lambda_64826d400df5e2683a863a4dbb954602>@@
.?AV<lambda_64e20fef6bc734aa72d77b8028e3b077>@@
.?AV<lambda_651b32f5fa8d57f69a57602106f1e1ba>@@
.?AV<lambda_65ea036252223d30a37862dfd4f7bc69>@@
.?AV<lambda_665f26c4f805d51279dd7cc3218f1cd8>@@
.?AV<lambda_6665c408830d0dff611752f43635ad2f>@@
.?AV<lambda_66b6761e17b1698f92d4664857f34eef>@@
.?AV<lambda_66b8e949c8c097783546f36f94506804>@@
.?AV<lambda_672376956558218714cb97c30f1f09fd>@@
.?AV<lambda_673e4ce19ce5833538c9ec8e56f2275c>@@
.?AV<lambda_67cab59cf9e4d6fff4a228b6f50a31f4>@@
.?AV<lambda_67d57227949a925edd39b6db70853db7>@@
.?AV<lambda_68094969648d2e0eeda4bdcc6834405e>@@
.?AV<lambda_6829271c72aa0e0367fc0ccf7a9e3edb>@@
.?AV<lambda_687b6564a98631c4ae886f974f6a6448>@@
.?AV<lambda_68c5430cbb13436022345da27dc88632>@@
.?AV<lambda_68f4cdf20c40d247dba9fe0156c932f5>@@
.?AV<lambda_691026265f1e73c9b49221bbcaaf7e8d>@@
.?AV<lambda_6922b57be4edd39a89a7cea9e2ebb658>@@
.?AV<lambda_69fb29d342558f2407dd09c009c85823>@@
.?AV<lambda_6a264f227367afe63ba4921d698f9b11>@@
.?AV<lambda_6a32611970906250c6b47d8292ad00e7>@@
.?AV<lambda_6aeba50936d3f6c7b11f3c24b58165e0>@@
.?AV<lambda_6b8e74af1b2b937274f6d872f7931a4b>@@
.?AV<lambda_6bf7af48e7a4158e20b4f833c15de130>@@
.?AV<lambda_6d0547d7d9e564311a780ca9dc7db655>@@
.?AV<lambda_6d1a31765a629c9bc43dcea20434203a>@@
.?AV<lambda_6d1c642e24095d4ebbe2c39c89123bfc>@@
.?AV<lambda_6d6f3d3230139a3d222e52865731387e>@@
.?AV<lambda_6dd6cf995614bc7c2e490d548728e197>@@
.?AV<lambda_6e847de2b8df09456577c83c1abac749>@@
.?AV<lambda_6f010deb824d03b6370449d6f782b9da>@@
.?AV<lambda_6f05a8b4f6851ad7c85a3a3bcb9b9104>@@
.?AV<lambda_6f9292e3cd6b9f1b15fc272b7af94e1a>@@
.?AV<lambda_7029d41d4a95ff2575c8227d099f78bc>@@
.?AV<lambda_70a4e4f9104009b11570775cde48242b>@@
.?AV<lambda_712b3a1726a5ab1ce39d1b3e7f50d979>@@
.?AV<lambda_7134bef299e596690440ce2c3fc8b019>@@
.?AV<lambda_715d031581e8d9abada945eeadbab576>@@
.?AV<lambda_716d92b2455b6dca0a3e195a50699167>@@
.?AV<lambda_71ef653055b00e102f19330e0f78a252>@@
.?AV<lambda_7263948f9424947480bb5df404fee515>@@
.?AV<lambda_7291a02d034b9a7f0a27fdb5705bfe30>@@
.?AV<lambda_730e393831d92a8b1623c0ee3e0e1a73>@@
.?AV<lambda_73119717ae86d28cb548ac4543eb73ed>@@
.?AV<lambda_7361ba811d5ff92bf50e102c73312480>@@
.?AV<lambda_73cc642ed8afaa2889c4f09584fca345>@@
.?AV<lambda_7413de4275d15476d9100c3e64569703>@@
.?AV<lambda_747d01ad8121cb89860d325b0c86de9f>@@
.?AV<lambda_747f5a5054cea5042105b3988bb8e54a>@@
.?AV<lambda_74ee45c15b018f2c2709e09284f9b432>@@
.?AV<lambda_7523eb0c2ea9937bd2da9cb954f010c9>@@
.?AV<lambda_754c2a248a4974a3df70d7c3cf8fcac0>@@
.?AV<lambda_766375b35334ae7ebdda42fcdeada652>@@
.?AV<lambda_76eec3e346e3038343d242b83e7ef7cf>@@
.?AV<lambda_76f850960fe6ac327acd4613d41835f7>@@
.?AV<lambda_771974d945874fd2518ef4fcc7999ce0>@@
.?AV<lambda_772b291d0a2826994f823d005fbf5dd0>@@
.?AV<lambda_77bbe4874254ed84a312b5fa826a83d3>@@
.?AV<lambda_7825462dd39e55a2adc835dd480c65dc>@@
.?AV<lambda_786ab6644ee6b69320106188cb9d35b2>@@
.?AV<lambda_788dd9ac40e758b51e6e5062cd2232df>@@
.?AV<lambda_7965ceabe5f45db89655ad0a7fd04cd6>@@
.?AV<lambda_7a5892277686c5fc70de8922cd945035>@@
.?AV<lambda_7a6442793a842f584bc31790203846c7>@@
.?AV<lambda_7a970010ad38e3a1508f7194da0afc42>@@
.?AV<lambda_7abe23a5198e897647d783cb391d2ac6>@@
.?AV<lambda_7b0f2fd49a85118506333bcc9c82c163>@@
.?AV<lambda_7be8916f64e405639dec1d893b64c678>@@
.?AV<lambda_7c393e0ced2515f63b0674de1e1229b1>@@
.?AV<lambda_7c5eb1f068179d900c888fe3d5890426>@@
.?AV<lambda_7ca26f875a5bd8018609c044fb5616a1>@@
.?AV<lambda_7d629299b17d3241cda0057fa57cac3b>@@
.?AV<lambda_7d7889780bcdcdf31ae68e0e6e2b6b04>@@
.?AV<lambda_7d7d52dd8f31ce0fc48036fb1081b283>@@
.?AV<lambda_7dbc984020fde247f30ce0431c9445e7>@@
.?AV<lambda_7e64dc56260f5aa848b67008058a8451>@@
.?AV<lambda_7e705c133fb64792e116b9bec62de367>@@
.?AV<lambda_7ec65dfaaf4dc75c3f7f2cbd42331128>@@
.?AV<lambda_7f77cbd32daf2cf04fdea2edf875f503>@@
.?AV<lambda_7f850842e5126f5ee8e78b4d81ce1b94>@@
.?AV<lambda_7fdfed5ff1e8b275291a3713701dc711>@@
.?AV<lambda_80600ba38f1578c0943cc3b4cc8d084c>@@
.?AV<lambda_8063db1f151bd88e3ec79c09eea46da4>@@
.?AV<lambda_810201540ffb706396f9dcc1175a3533>@@
.?AV<lambda_811c61ef0fce38996d92306d6a5fd2ba>@@
.?AV<lambda_814de60c7177af03b3ddc9e5c36d561a>@@
.?AV<lambda_81de0b469ab0f1e7bc6d7cfef2665991>@@
.?AV<lambda_8226f92ae14eb377c8291a622693e68d>@@
.?AV<lambda_82b38e81208f4cf45ccff9cab1f1ab56>@@
.?AV<lambda_845fb0d365668e61a65e09bae6280c09>@@
.?AV<lambda_84e8a40ae9ede95d9e10e30d574bf4f6>@@
.?AV<lambda_850028fba9642b015a2c6ff81bf07107>@@
.?AV<lambda_8516815e67f3594718fcfcc07116860d>@@
.?AV<lambda_85740bb65c1a7256d972833e138b25dd>@@
.?AV<lambda_85b8c9439e9295f048404909acdf29d3>@@
.?AV<lambda_860b5cff89ff5f292872db2aaa19c52a>@@
.?AV<lambda_86574d63891f8965457453a282e51ac5>@@
.?AV<lambda_8687679c83f6820abda420b1312e974a>@@
.?AV<lambda_86895a13d56222c09289c706b79a6852>@@
.?AV<lambda_8704e7031b6ab9a2ec7b63d5aee19048>@@
.?AV<lambda_87082f934acdbb058c5a96c46c9520a5>@@
.?AV<lambda_874d535e153c7e2cc7bc8cf3f159ba49>@@
.?AV<lambda_87a4a0298034d28fa5baea8b9a5f4bfa>@@
.?AV<lambda_87df0aeb452bc224372e0c71d6a5bb70>@@
.?AV<lambda_87e032e72109af3068317fff82549670>@@
.?AV<lambda_8854c23c2fa70d516c73af69bc8b08fb>@@
.?AV<lambda_888c8332848e83c225ef5f5587c3b38e>@@
.?AV<lambda_88b1496ecc213c07fd24b30bb5c856e7>@@
.?AV<lambda_893ec3becb9cd41d9efbc7eea98d41c9>@@
.?AV<lambda_8a5db151b7b360081c3c199e43485789>@@
.?AV<lambda_8a900b8b41180a41f1571bb2c4cf6f8e>@@
.?AV<lambda_8ac5cfcccd347e512ac9419fd63346cb>@@
.?AV<lambda_8bbb4bb940b6248f0079a061cca5209e>@@
.?AV<lambda_8c32b6976971eb48d48ba6d5dd1a9ae2>@@
.?AV<lambda_8d52f2d53a04afb64fff4765a2f91d21>@@
.?AV<lambda_8da27afb1f9e2f39230ce8504a3b22f1>@@
.?AV<lambda_8ec99f105e4604958a8ba328f7e77b1f>@@
.?AV<lambda_8f9d3b0d9ea77fd034ca21fb6a734c46>@@
.?AV<lambda_8fef7dbb6b3e81cda489ebe45ab449bb>@@
.?AV<lambda_90e67d7cf9c8fca59bac4a84fc304da2>@@
.?AV<lambda_913ebb992bc3e09bdf2737529cebbf5a>@@
.?AV<lambda_91fdf8e96bd0363c4307fa79933bf9ee>@@
.?AV<lambda_9257e4c042146540822b621335e71112>@@
.?AV<lambda_9283763b81fbf1c4441399956127b6eb>@@
.?AV<lambda_92ea355126c2f4fbe475cb5acb7184bb>@@
.?AV<lambda_932674f6b1528de9101a6cf1da2f3619>@@
.?AV<lambda_9330135269578d8e01a33adf741ca665>@@
.?AV<lambda_939f4702fdcc80c80258913a08838422>@@
.?AV<lambda_946ede80a21d7425c1a0e51bfb4c4cf1>@@
.?AV<lambda_947e49898d3c63f075f4c68c9b06caa0>@@
.?AV<lambda_9531d8dd38f435ce1e9840c5a24c7961>@@
.?AV<lambda_95777b3f28fcd1bc04104997acb745bb>@@
.?AV<lambda_95a0740ad14942e833a8456dd992b6d9>@@
.?AV<lambda_95d4c31e5917c076b85b93f005fcb675>@@
.?AV<lambda_96332e6dc90d0b557fee4b084f8c74f6>@@
.?AV<lambda_965a3b769c39b0bedf189a3ea0545ea9>@@
.?AV<lambda_9671e9cce0def787c04a0ca1f741034c>@@
.?AV<lambda_96fe9f03daad80a6076c498e634c5f49>@@
.?AV<lambda_975b91ffee199f0bfc6f725443e3d8f8>@@
.?AV<lambda_976990ca77e2d50b70bfeaa03811f4a4>@@
.?AV<lambda_976a37e7ee737ef6672be8fc72541275>@@
.?AV<lambda_97b64e3cab478b2173a15dcda1845df3>@@
.?AV<lambda_9821a1e0b606be014cc2fca34ba53b1a>@@
.?AV<lambda_9838567c14a727b6e962c38ba47aee11>@@
.?AV<lambda_98b53a1372ec257c871e8ef28d59be07>@@
.?AV<lambda_98f67966096a245ba60cb535f4355420>@@
.?AV<lambda_999155f75226cbdc950d71bcbf55a961>@@
.?AV<lambda_99c04f24fb37c840416c3031e266a935>@@
.?AV<lambda_9a2786289a4d18fa33a8facc5fc736fa>@@
.?AV<lambda_9a36e00d72a5d737ec77f3dedc2d9072>@@
.?AV<lambda_9a4c460641eeb4213d39e91aec522d90>@@
.?AV<lambda_9b848ee398be1c2a596086e8c2c0124c>@@
.?AV<lambda_9b8516a2c0a6d92f94f78080c5bc3b53>@@
.?AV<lambda_9c03d71b3e2a72420d0112f6f3840dd4>@@
.?AV<lambda_9c23f807f5b88acf4c74334fd51de3b9>@@
.?AV<lambda_9c73eb49aba39bc780e6a604a9602ad6>@@
.?AV<lambda_9c9dbe7f2291bf7ad9d3c12ac8113947>@@
.?AV<lambda_9cbf79b94b2fe6cbcf57182cdc276d42>@@
.?AV<lambda_9cc2379234b917d77614797746e75684>@@
.?AV<lambda_9d370f39b593a9c0408175efba97f3a7>@@
.?AV<lambda_9d8771d050eb3774da949f76ff7b2f7a>@@
.?AV<lambda_9e468aee1cda7bcda7bd21224cf0d16f>@@
.?AV<lambda_9e6211c87401ff34ec951ed3f9d92212>@@
.?AV<lambda_9e70336b15c8be3003ecde8d0dd2702d>@@
.?AV<lambda_9ec8ee75f7d0151bdad20dc55223ff7f>@@
.?AV<lambda_9f005c5385db773ebfdb522ad8aa0be6>@@
.?AV<lambda_9f09f893fd25c5f3e434b7471f6ab3a6>@@
.?AV<lambda_9f22cccb787c8b0be66f7b40706128ae>@@
.?AV<lambda_9f3bde71d5b761f626949ad2b1feb96c>@@
.?AV<lambda_a0193b154f39aae0502793b45f34eecb>@@
.?AV<lambda_a0fb753c0036461099c9ffd5a5b954e2>@@
.?AV<lambda_a1e22134fbe96254ce6d7a539c26c203>@@
.?AV<lambda_a2eeee047031406dbe388d0066fc2709>@@
.?AV<lambda_a2fde83c21b5ed7bbde18c0d05ee467a>@@
.?AV<lambda_a4015e490e3e9078f9108a810d677815>@@
.?AV<lambda_a41c606239635631821142ba5046850a>@@
.?AV<lambda_a45f274125d5aab3ada44bd6d91ed7f6>@@
.?AV<lambda_a4eb294818c1d4196ae37e2aec2c57f3>@@
.?AV<lambda_a4f3d40f4d5a0b2abf807cb8d9827633>@@
.?AV<lambda_a4fb1c1b5f905e04c31cbbe6c165878a>@@
.?AV<lambda_a4fe3bd39dcd0107f90455cb45c21429>@@
.?AV<lambda_a582beb35c004ba9f7a0a06ee52a248a>@@
.?AV<lambda_a5f2de4a08d99e6253357934807ae733>@@
.?AV<lambda_a65213a64b99d0456231cd4b3dfd703b>@@
.?AV<lambda_a6d62ed7a3aadce21a3a08f0b111f7e9>@@
.?AV<lambda_a6ed0d25733f7001ea91db6552d588cd>@@
.?AV<lambda_a7688683053999985ad4b5ee1c5e7d55>@@
.?AV<lambda_a777aefbfa0ed449706f4a933e2595f0>@@
.?AV<lambda_a7b5600ffd3fa627825fdc42ab8e2c48>@@
.?AV<lambda_a7b64ccd3ccc5eb737481b0a45bc9e55>@@
.?AV<lambda_a7ed49975b7832e11720c7c8bc70dba8>@@
.?AV<lambda_a81fb453624cd459713d464cbd39f40b>@@
.?AV<lambda_a875e26a77cffbb176a8ef549b2a14b9>@@
.?AV<lambda_a8e944132f86054019a0dd06ad885a3f>@@
.?AV<lambda_a8ee5a68ff8f1273772c7d2c6e9affbe>@@
.?AV<lambda_a949a47209eded0ee39e3c26cfeb0e88>@@
.?AV<lambda_a978e2da606089f22a9b81ea0013257b>@@
.?AV<lambda_a9804f77c3a977ad53df45464de9292b>@@
.?AV<lambda_a99d8d5e4f7ccd722937dd9eb05ffd73>@@
.?AV<lambda_a9ad92a14c7791713a13c8f77948b581>@@
.?AV<lambda_aa3d1936b4520347874ab4b1ff4df1a8>@@
.?AV<lambda_aa572ddb1cb313eeb2f9a7fe6780d39d>@@
.?AV<lambda_aa831217bcc44892538d39aae8c330f6>@@
.?AV<lambda_ab7cc530eb49a3a15082dbc27eb613da>@@
.?AV<lambda_ab99d16aa8824a5d2e1b2dc9090914c7>@@
.?AV<lambda_ab9a9c5c0feb1fa6529cfd9c464ffa82>@@
.?AV<lambda_abeea81c8db6fc4afe459674d5836b9e>@@
.?AV<lambda_ac9e5c5dae7b58201a89b0e1484eabd5>@@
.?AV<lambda_acdb2664af6229da1e4c43d330c314d8>@@
.?AV<lambda_ad704c2931acf5ee8c141f89b8d0ba76>@@
.?AV<lambda_aec81523952d967a50c07470b5814993>@@
.?AV<lambda_af2b72ce93dfb4b4ab2e91dda701576d>@@
.?AV<lambda_af2c39467b4484bc2a2f611cf533daa7>@@
.?AV<lambda_af999ce2e03a2039dbf31c1ff13e0675>@@
.?AV<lambda_afc7894d5145143e12a5c0e655a064d4>@@
.?AV<lambda_afcaa8d78bccbe99ca495a99e4eeaf71>@@
.?AV<lambda_aff3a47ffdeb0107e2635883ea459138>@@
.?AV<lambda_afff46216fde1007fd3787aa1c6f64ac>@@
.?AV<lambda_b090b097d440a4b2445143675bf63aa8>@@
.?AV<lambda_b0d291db2b118b82971a2961eee9c859>@@
.?AV<lambda_b0ebe73dd875315ff8ad13e2b0e77e37>@@
.?AV<lambda_b0f9e3c706f87be7af57692823441e06>@@
.?AV<lambda_b13b6f35685da4b80165c8f42fe46541>@@
.?AV<lambda_b1c0cff63caf505f6536baee30943a62>@@
.?AV<lambda_b1dfe0352ec14503f6bc27c9ab7381cc>@@
.?AV<lambda_b24067cfb776b28c2465a46fad4c39cd>@@
.?AV<lambda_b29f4ae43268c809ebc31a8ad9d86159>@@
.?AV<lambda_b35f6304ab9b91e067de16d773b507fb>@@
.?AV<lambda_b3cbfd0276cf0ad6f3b58532d60f239f>@@
.?AV<lambda_b3ffb2b19f45444139b13fc0568f9537>@@
.?AV<lambda_b4032de8aabc62f3d8e14c9557b8e4c2>@@
.?AV<lambda_b437eb3a1b8ce8112ef34101e8422db3>@@
.?AV<lambda_b56eca9e5f17216a19fa025aad3fbc12>@@
.?AV<lambda_b588eebdb691c2e62a30a6a9e67054ef>@@
.?AV<lambda_b58abd66dfb794261458d2caeeba2a75>@@
.?AV<lambda_b629d0fd638b8e3cc3ff0e2e1fa99a79>@@
.?AV<lambda_b6b6722f1e0f042c63804b4645002c54>@@
.?AV<lambda_b6dee4d13517d8cf66ef1e240c3ec6a4>@@
.?AV<lambda_b7019bfd7941f33388d4d95b9daa50e8>@@
.?AV<lambda_b7be74f8e6a45391be9d480e0a0a34ff>@@
.?AV<lambda_b86aa8ef1de0593a5547f08d792c1565>@@
.?AV<lambda_b881ee6a3336741e3e3697374c374936>@@
.?AV<lambda_b8c456fe40447d4b39feffecfde76884>@@
.?AV<lambda_b9005de86b74a327b831744e992c382d>@@
.?AV<lambda_b904ddb979f839dff33d10b1e7d0bdbf>@@
.?AV<lambda_b9678ecfd54fea19e98c665d6b7111d8>@@
.?AV<lambda_ba0868398d1cd25f6a2d11110301255b>@@
.?AV<lambda_ba8ab8ea5e43c631481b914bba906376>@@
.?AV<lambda_bb011bad1a611a9fc5e45821394e55ea>@@
.?AV<lambda_bb1f7ff97f0a93bd0d62291fdf3e87e2>@@
.?AV<lambda_bb245b6c588ba3a259962707ee90e1f2>@@
.?AV<lambda_bb3d425e1c6940fb9e0c2d83749b4004>@@
.?AV<lambda_bb618da91ef45dc5293c089a74c35488>@@
.?AV<lambda_bc55cb0d4e1fc3c67aaad8faf5025983>@@
.?AV<lambda_bc6c2ea8bb268799c4c254106b4aba77>@@
.?AV<lambda_bc73456ccef784a5af3834b4cd8faaf9>@@
.?AV<lambda_bcb80e95aca4245f759eaa0035e910aa>@@
.?AV<lambda_bd14764690ecfef9fa94612b0f5ef502>@@
.?AV<lambda_be82f0d3082d1770e6fe9b6560ab180a>@@
.?AV<lambda_bf5d26f70475f0fe685bc8ad946c36b3>@@
.?AV<lambda_c01406ec4a694ccc7628eb9a89481150>@@
.?AV<lambda_c09531cdd6b30fb34e9285a274065481>@@
.?AV<lambda_c193fdd8c0f883adaf2de3acc3f64e63>@@
.?AV<lambda_c20626d04f0b55b1c8f4e6d26e7dbf7f>@@
.?AV<lambda_c225c1d25bbd93cb1adc0485c77bcd32>@@
.?AV<lambda_c2634be0290355aa32c42903b822d942>@@
.?AV<lambda_c277816ed3e96f001ac0003fb11da190>@@
.?AV<lambda_c2e380cf1102ff4d7d9bd2e215fde5b4>@@
.?AV<lambda_c311c1ca35d5610174ed400a6355dd88>@@
.?AV<lambda_c332dd4138e096b0ec5e0eae392e23e1>@@
.?AV<lambda_c3ba679072e8df17b11fd4f7af75d138>@@
.?AV<lambda_c46cb457b677540589add8ea80c57d8e>@@
.?AV<lambda_c4a3869be068e7ba91c5160df08ada1c>@@
.?AV<lambda_c4dc4b2967ca7204e8ba49b0607e0250>@@
.?AV<lambda_c5677617c2b0074bdccd3f6596388b76>@@
.?AV<lambda_c580670d4132814d3bd00ecba71eaf16>@@
.?AV<lambda_c5dec7e5184735ef54bbb3fa15124e26>@@
.?AV<lambda_c6a4c758ec4a05da2df0a96bda1f195f>@@
.?AV<lambda_c6cc7a538d9c817426d2f4671032805a>@@
.?AV<lambda_c765ee4079d8132e5b64fd41ad2c9a7d>@@
.?AV<lambda_c7733fbf1e7357c5a2b00aac2ede4865>@@
.?AV<lambda_c7a086a1b0574ca822082c2b20b57abb>@@
.?AV<lambda_c7c1c280bcb590aff0e32b0d2bffa767>@@
.?AV<lambda_c7e777d5cfa84aa6006d2063a10ce1f4>@@
.?AV<lambda_c898490d638c72402474e8c5b41a9926>@@
.?AV<lambda_c8c33a5265022502f864eaf2ce2649c7>@@
.?AV<lambda_c8fa02c31283474933648634b584ab88>@@
.?AV<lambda_c9cc85b6189178098c01bbceb71ba127>@@
.?AV<lambda_c9d29aed4526d430f521ffb38e7c1059>@@
.?AV<lambda_c9e83474de578fe8631d32b8d8721959>@@
.?AV<lambda_ca1582b0d14abd4d03d9895812c8d107>@@
.?AV<lambda_ca50ed85a5b6a014480832a0283a2f4d>@@
.?AV<lambda_ca87e505bf8a7e1f26579083c3683d94>@@
.?AV<lambda_caa048ef690674a871bcb383934123ad>@@
.?AV<lambda_cb4a1e251f906453321bd121147ccb84>@@
.?AV<lambda_cb775efbeda17e0717f38321c23d0575>@@
.?AV<lambda_cb8e619381015eaa09861c60b8150125>@@
.?AV<lambda_cc475f2c9c4a5e3bde84a6796d60e832>@@
.?AV<lambda_cc4bc3c6927d9351454759e8077d0a48>@@
.?AV<lambda_cc810076352af54d8ab1334d58ef2ac8>@@
.?AV<lambda_cc9e076beb57119667e5b07bb3c48707>@@
.?AV<lambda_cd453f5abbb4020fb3775475830cd8d1>@@
.?AV<lambda_cd4f9515fd1d18f38ab6ba0038b1c96b>@@
.?AV<lambda_d028772904ecde58905f5707c0de32a5>@@
.?AV<lambda_d099cfc55508afaa7cb7103a60412ec4>@@
.?AV<lambda_d0fdf4211aa9516cbe57f435cdbb747f>@@
.?AV<lambda_d1104bdf26eb9a3a2f36a00e0287055b>@@
.?AV<lambda_d125101b1e77d289ab0937ad814f16d4>@@
.?AV<lambda_d13d78c406485322a9ca312b3346960c>@@
.?AV<lambda_d222db6c7292e4a7be3d95ab9d5f696d>@@
.?AV<lambda_d252a66a2c07e9c22634d060e74c7bae>@@
.?AV<lambda_d2a49f03e450ccce2f16f6b50c34ec41>@@
.?AV<lambda_d2aedfced5017e33ef2fd58df23b739b>@@
.?AV<lambda_d2d693a490e9887da5a024c703dc8e3e>@@
.?AV<lambda_d2d7cc7cd6283dbb6ec9d6d27a7f28ba>@@
.?AV<lambda_d303bd53dadec643ee9070953471f748>@@
.?AV<lambda_d30ba908d7a4df45178218a83c8b90aa>@@
.?AV<lambda_d318dbc50159c2f1256019c88eaff40b>@@
.?AV<lambda_d3bde8af1c4b2f7f5d32bc0631a76a47>@@
.?AV<lambda_d41a0a7dd6ff244599c8f5fcf6aa6a8e>@@
.?AV<lambda_d430b12f7c973c17dbb8b927fbda5d7d>@@
.?AV<lambda_d45414743c2d05f7edcf6eb84b1c5376>@@
.?AV<lambda_d492ab087615483c649f2fbeb7c7afb8>@@
.?AV<lambda_d49bbfb4942fd85adb6441f849e70bf8>@@
.?AV<lambda_d51c06990161dc274f89c0ff046b1644>@@
.?AV<lambda_d522df09ab9335db57f0cfc8c09d6630>@@
.?AV<lambda_d523df6827c8f76e5f91e59ed710694a>@@
.?AV<lambda_d5280148c0688c7d0bac28aeda9b34e8>@@
.?AV<lambda_d5f3ad82c6f06fa69e5f77344156b4ec>@@
.?AV<lambda_d60ebcb8fb80278504251155552d9189>@@
.?AV<lambda_d610c17b70b54be4f32b495a21be0c2e>@@
.?AV<lambda_d698068c9068e0c2606c104c4b724c40>@@
.?AV<lambda_d6ca9c94cfca15f9aa33f45058a1fcd2>@@
.?AV<lambda_d6d6a472b26fc7f192d382e93bba1d57>@@
.?AV<lambda_d6f808cdffda8618fc34f8b7c63dabf2>@@
.?AV<lambda_d73505fcfab084acb258f01cd57039ac>@@
.?AV<lambda_d73f309e1e17ba4988eef7d15e33e2b5>@@
.?AV<lambda_d7733c1584aa3badfa0c0498f12b13b3>@@
.?AV<lambda_d7bfa7cc168368c290c2d7a0788cd723>@@
.?AV<lambda_d7d65f11678621cb189879800cf53faf>@@
.?AV<lambda_d8059ce711ce14c36533947d64fa5a34>@@
.?AV<lambda_d86f3bcde641b4c18df519c60bc10d09>@@
.?AV<lambda_d8c2999bab01791ce81720c649701452>@@
.?AV<lambda_d8ccbf6f2719267adef088325c832169>@@
.?AV<lambda_d8d69ea69c6171a0cf8e50796541d588>@@
.?AV<lambda_d95ddce1a1fa4ff8b9f16e34e85c7615>@@
.?AV<lambda_d9adada7822ba67bc618dd6220c7e11e>@@
.?AV<lambda_da7c0e243075d823e83aad5424aee4f1>@@
.?AV<lambda_daa97a08fba16356a015278e0f3ca1ed>@@
.?AV<lambda_dada8d57643005b7e2297cabdd6baa97>@@
.?AV<lambda_dc38421579cc359d17e1b6586ccd53c4>@@
.?AV<lambda_dc5b9bd79fd7f93771785e3c76ee3967>@@
.?AV<lambda_dc786c9bfe42f2772e18a10475fce9cf>@@
.?AV<lambda_dc838e8d8ebeda8eb60e5b42b6de163e>@@
.?AV<lambda_dcbbb46dcb2ccb38f1b7e3e89a0ca172>@@
.?AV<lambda_dcbbb9aee7454d5a0feb975df66a7001>@@
.?AV<lambda_dcd647a35a2dbde7587f9217025bc168>@@
.?AV<lambda_dd543c4aa2344f0c721733cdff68d0b3>@@
.?AV<lambda_ddff9a77cb22aafff303a88b4705bee2>@@
.?AV<lambda_de00575298e9c7ead3b243890c596395>@@
.?AV<lambda_de588006d8cdb99d9cfbe9ec58914e7c>@@
.?AV<lambda_de8b881f6b63dfc498a2c66c13a87c6a>@@
.?AV<lambda_deb528c4a03e167325d02f55697f57f9>@@
.?AV<lambda_dec8978adb68f88cd37c14b215792fcb>@@
.?AV<lambda_dfa4f11b86420105767b17dba3f3e58c>@@
.?AV<lambda_dfc50b5c75707c0e32bd4de87875de8d>@@
.?AV<lambda_dfdd1161f2f278e15b5873916c2b0ca7>@@
.?AV<lambda_e0b99860c2bc4f26550c2e8d362d7aeb>@@
.?AV<lambda_e0cbc3c78c2ca2e1945708d5effc57eb>@@
.?AV<lambda_e12a5cc64ca47f52fabae1f759666c1b>@@
.?AV<lambda_e14efb45021ce055c5f707c6ecaf4d7e>@@
.?AV<lambda_e15d10c4385b9adab7c3b2967076a5a0>@@
.?AV<lambda_e1d13b2195468d926eb95765f5990099>@@
.?AV<lambda_e2fce933a994c63cb8e5ceee1e0963b6>@@
.?AV<lambda_e31b0a3938d8cf9c535bd4f318233ee3>@@
.?AV<lambda_e36f361e8289b79d05cd119226887515>@@
.?AV<lambda_e4ad6020d0e0535a8933e08ebcb686f6>@@
.?AV<lambda_e57b1a786ad480298008b53b2aee7436>@@
.?AV<lambda_e5e6dbc489066f0491275014df31d396>@@
.?AV<lambda_e64d6f02466f94741deccd76032ec2f7>@@
.?AV<lambda_e6687d53b2ff8a73e462a3df53da446b>@@
.?AV<lambda_e6ae48ddd96d9e98a83433a4d595515b>@@
.?AV<lambda_e6da43affed7c8fa1984fb0568cffe7d>@@
.?AV<lambda_e6ed4e30f502254d17f91a8198ab3dff>@@
.?AV<lambda_e742661535f51343e97c0bd1f594fd9e>@@
.?AV<lambda_e7465666a1cce2f1be8b885d5e56a9d2>@@
.?AV<lambda_e74921842af68f126d6a963647d9b662>@@
.?AV<lambda_e7f381725b47e4a5cf7649db595fbdf3>@@
.?AV<lambda_e87e8efc1f18e44e772bf580822fd15c>@@
.?AV<lambda_e8a1f60a9605811126ddd13004e0920a>@@
.?AV<lambda_e8a6e880ee4bae18375748e5a9775705>@@
.?AV<lambda_e8aeb42bf0aa89a79a408edee79cf184>@@
.?AV<lambda_e9a129094f8ce9402077e710005a387c>@@
.?AV<lambda_ea55a0337c6548d84265ecdc39ed5e30>@@
.?AV<lambda_eb3cc1469a265bbe9f2fbbb9b40f13a2>@@
.?AV<lambda_eb7a0c3b4f55a6896582386a718283be>@@
.?AV<lambda_eb7de4d30a85c7b44c19b4f8197380ee>@@
.?AV<lambda_ebc32470c7ba4b7c18e53e48be43fa56>@@
.?AV<lambda_ec14091c8d829910b5be28a833d6a3dd>@@
.?AV<lambda_ec1910e0e2f828f423168f93cf5ec05e>@@
.?AV<lambda_ecbde622a6c4fa53412bd71f973ccdba>@@
.?AV<lambda_edcbc83438e6dd39d1d927db086748d9>@@
.?AV<lambda_ee881615ba5030f55f651fb9e91a2637>@@
.?AV<lambda_ee988755aabf6d6b8cbe1162f42f9b60>@@
.?AV<lambda_ee98d36ba3b6f304603cfa013effa1d0>@@
.?AV<lambda_ef1b5ee9690bf60d6d8ba03dec70f0fc>@@
.?AV<lambda_f02097f693588c4875083cbaa36b78e4>@@
.?AV<lambda_f05d87f4b55c6f94a6e4ce358c6d246e>@@
.?AV<lambda_f08787ec2811763c406ae46c36fa9fd3>@@
.?AV<lambda_f0a5b0eb3f2f9f34c815ca05348f51fc>@@
.?AV<lambda_f0c2d149234417642d38bb42f3187940>@@
.?AV<lambda_f12636f558763d225cd192e9911d0ad0>@@
.?AV<lambda_f1490410ee527b42f3bd28e691dae2a7>@@
.?AV<lambda_f1cd88210e6db5811994272c07493962>@@
.?AV<lambda_f20e48e7127186879f643480a395548e>@@
.?AV<lambda_f22ff450a42615719b4e6b964a52c793>@@
.?AV<lambda_f331e8788a2dcbae240a86dcd0d7b3ca>@@
.?AV<lambda_f36a9f1162ce53394a7287b72f55835f>@@
.?AV<lambda_f388454f75e91de2e1f91bb03ba26f7a>@@
.?AV<lambda_f38e192139ba1cff9e8db37924678e9b>@@
.?AV<lambda_f3fcc869c6c1998b84de6d82e27f5b75>@@
.?AV<lambda_f43464c7a9a747b50c25ae66c092bf51>@@
.?AV<lambda_f491f0d15366e04e0edf3bf2753174a5>@@
.?AV<lambda_f4d85bd911255cc25b789888dd092506>@@
.?AV<lambda_f4ee709d6b6434a145be84c46551b589>@@
.?AV<lambda_f513723b143f04ceb7a6dc1caaf8d647>@@
.?AV<lambda_f5cf874a75941a4df47924a1ea9ea866>@@
.?AV<lambda_f65b8c2fd23e2320c311133a80f471c4>@@
.?AV<lambda_f65c61404cbe8a2665c2dfba4b8d6c08>@@
.?AV<lambda_f66636158d41c23d6c8741d8b8b758e4>@@
.?AV<lambda_f72360673da28b091dd40b649eb46e9c>@@
.?AV<lambda_f749f947d6e043250dfdbe549acd74ee>@@
.?AV<lambda_f78d8769dd40946ef61ad5956cbf10fe>@@
.?AV<lambda_f7a998b3515df6d592f0862620c5de18>@@
.?AV<lambda_f86ec1fac20fee0d4eb5798f81c1da0c>@@
.?AV<lambda_f95b3cba5a1c25b0ed905809853fb736>@@
.?AV<lambda_f981a1e78256ff9696dd76fdbfa999a8>@@
.?AV<lambda_f9b942c519c5abdf8c9e76770faf15a7>@@
.?AV<lambda_f9fcc66b14829c5756eb43c20bdf984b>@@
.?AV<lambda_fa21abf52748e92ccc4db121eff52d81>@@
.?AV<lambda_fabc2f3ff1a13bd31a439cf6cb082152>@@
.?AV<lambda_fada670c8c8fb6ae388f5a9e4132a8cd>@@
.?AV<lambda_fb29648c107445cc8f675923b21825bb>@@
.?AV<lambda_fbb10b34ab7c53fa4628c7de9c1d7ff5>@@
.?AV<lambda_fbd17297dd38c3cf3535c4dd5c8397f9>@@
.?AV<lambda_fc1efac575c2b28ad2e40dacdc9d02c9>@@
.?AV<lambda_fc59cfd6180e7962f9dbfcf6d01970a3>@@
.?AV<lambda_fd266a9166d0b10d35a0d8f14994daa2>@@
.?AV<lambda_fd3f60c5d0df63667d14dc08824a1b73>@@
.?AV<lambda_fdc34539e9b271a361a2a0c8195e907f>@@
.?AV<lambda_ff5e6787b4748302677d41e07ba5b8c0>@@
.?AV<lambda_ffd7c59dd1db3541219e1571e11a3c77>@@
.?AVbad_alloc@std@@
.?AVbad_array_new_length@std@@
.?AVbad_cast@std@@
.?AVbad_exception@std@@
.?AVbad_function_call@std@@
.?AVbad_optional_access@std@@
.?AVbad_variant_access@std@@
.?AVcodecvt_base@std@@
.?AVerror_category@std@@
.?AVexception@detail@nlohmann@@
.?AVexception@std@@
.?AVfacet@locale@std@@
.?AVfailure@ios_base@std@@
.?AVFatalException@protobuf@google@@
.?AVInferenceError@onnx@@
.?AVinvalid_argument@std@@
.?AVinvalid_iterator@detail@nlohmann@@
.?AVios_base@std@@
.?AVlength_error@std@@
.?AVlogic_error@std@@
.?AVNotImplementedException@onnxruntime@@
.?AVOnnxRuntimeException@onnxruntime@@
.?AVother_error@detail@nlohmann@@
.?AVout_of_range@detail@nlohmann@@
.?AVout_of_range@std@@
.?AVparse_error@detail@nlohmann@@
.?AVrange_error@std@@
.?AVResultException@wil@@
.?AVruntime_error@std@@
.?AVSchemaError@onnx@@
.?AVstl_critical_section_interface@details@Concurrency@@
.?AVstl_critical_section_win7@details@Concurrency@@
.?AVsystem_error@std@@
.?AVtype_error@detail@nlohmann@@
.?AVtype_info@@
.?AVValidationError@checker@onnx@@
.@$d%tr
.0/0#
.0/011
.00cfg
.CRT$XCA
.CRT$XCC
.CRT$XCL
.CRT$XCU
.CRT$XCZ
.CRT$XDA
.CRT$XDZ
.CRT$XIA
.CRT$XIC
.CRT$XIZ
.CRT$XLA
.CRT$XLC
.CRT$XLD
.CRT$XLZ
.CRT$XPA
.CRT$XPZ
.CRT$XTA
.CRT$XTZ
.data
.data$r
.data$rs
.didat
.didat$2
.didat$3
.didat$4
.didat$5
.didat$6
.didat$7
.edata
.gfids
.giats
.h8^F
.idata$2
.idata$3
.idata$4
.idata$5
.idata$6
.json
.P6A?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@ABV01@0@Z
.P6A?AV?$OrtValueTensorSlicer@$$CBUOrtValue@@@onnxruntime@@ABUOrtValue@@_J1@Z
.P6A?AV?$OrtValueTensorSlicer@UOrtValue@@@onnxruntime@@AAUOrtValue@@_J1@Z
.P6A?AV?$unique_ptr@VTensor@onnxruntime@@U?$default_delete@VTensor@onnxruntime@@@std@@@std@@ABVTensor@onnxruntime@@_J1V?$shared_ptr@VIAllocator@onnxruntime@@@1@PAX@Z
.P6A?AV?$unique_ptr@VTensor@onnxruntime@@U?$default_delete@VTensor@onnxruntime@@@std@@@std@@ABVTensor@onnxruntime@@V?$span@$$CB_J@gsl@@_NV?$shared_ptr@VIAllocator@onnxruntime@@@1@PBVTensorShape@3@PAVThreadPool@concurrency@3@PAX@Z
.P6A?AVStatus@common@onnxruntime@@ABV?$vector@IV?$allocator@I@std@@@std@@ABVTensor@2@AAV52@PBVTensorShape@2@PAX@Z
.P6A?AVStatus@common@onnxruntime@@ABVNode@2@AAVGraph@2@ABV?$vector@PBVTypeProto@onnx@@V?$allocator@PBVTypeProto@onnx@@@std@@@std@@AAV56@ABUResolveOptions@42@@Z
.P6A?AVStatus@common@onnxruntime@@ABVTensor@2@AAV32@PAX@Z
.P6A?AVStatus@common@onnxruntime@@PAXAAV?$vector@UOrtValue@@V?$allocator@UOrtValue@@@std@@@std@@0I@Z
.P6A?AVStatus@common@onnxruntime@@PB_J0PA_JIIIIIIIPAVThreadPool@concurrency@2@PAX@Z
.P6A?AVStatus@common@onnxruntime@@PBH0PAHIIIIIIIPAVThreadPool@concurrency@2@PAX@Z
.P6A?AVStatus@common@onnxruntime@@PBM0PAMIIIIIIIPAVThreadPool@concurrency@2@PAX@Z
.P6A?AVStatus@common@onnxruntime@@PBN0PANIIIIIIIPAVThreadPool@concurrency@2@PAX@Z
.P6A_NABUFunctionBodyBuildContext@onnx@@ABVOpSchema@1@AAVFunctionProto@1@@Z
.P6AMMMM@Z
.P6APAVOpKernel@onnxruntime@@ABVOpKernelInfo@1@@Z
.P6AXAAUDataPropagationContext@onnx@@@Z
.P6AXAAUInferenceContext@onnx@@@Z
.P6AXPAX@Z
.rdata
.rdata$r
.rdata$sxdata
.rdata$T
.rdata$zETW0
.rdata$zETW1
.rdata$zETW2
.rdata$zETW9
.rdata$zzzdbg
.rsrc
.rsrc$01
.rsrc$02
.rtc$IAA
.rtc$IZZ
.rtc$TAA
.rtc$TZZ
.text
.text$di
.text$mn
.text$x
.text$yd
.tls$
.tls$ZZZ
.WhxlJ
.xdata$x
-/./.
/0]0g0
/060K0`0
/0C0s0
: :$:(:,:0:4:8:<:@:D:H:L:P:T:X:\:`:d:h:l:p:t:x:|:
: :%:<:O:T:h:u:
: :(:0:<:\:d:p:
: :(:0:8:@:L:T:t:
: :(:4:T:\:d:l:t:|:
: :(:4:T:\:d:l:x:
: :*:G:^:d:v:
: :,:
: :,:K:
: :,:L:T:\:d:l:x:
: :,:L:T:\:d:p:
: :@:\:l:x:
: :@:L:l:t:|:
: :0:@:H:X:`:p:x:
: :0:4:8:<:@:T:d:t:
: :0:5:L:]:c:h:t:~:
: :0:B:
: :D:L:T:\:d:l:t:|:
: :G:
: ;*;?;T;q;
: ;3;
: ;3;N;
: ;F<
: ;I;n;
: ;r;
: Conflict with existing kernel def hash.
: Conflicting with a registered kernel with op versions.
: 'Cubic' mode only support 2-D inputs ('Bicubic') or 4-D inputs with the corresponding outermost 2 scale values being 1.
: failed validating the check: 
: 'Linear' mode only support 2-D inputs or 3-D inputs ('Bilinear', 'Trilinear') or 4-D inputs or 5-D inputs with the corresponding outermost 2 scale values being 1.
:!:&:6:;:O:T:Y:n:v:{:
:!:-:9:E:d:
:!:':r:
:!:;:A:T:q:
:!:@:_:~:
:!:+:6:K:P:s:
:!:2:s:
:!:R:a:
:!;1;=;I;h;
:!;5;[;o;
:!;Q;
:!;w;
:":(:B:H:b:h:
:";/;`;o;
:";k;
:"<S=q=}=
:#:;:L:R:W:c:m:w:
:#:6:J:V:g:
:#:7:K:[:k:{:
:#:8:U:
:#:A:M:Y:e:
:#:H:e:
:#:P:}:
:#:v:
:#;?;
:#;`;
:#;0;a;p;
:#;A;M;Y;e;
:#;I;T;s;
:#;n;
:#;u;
:$:*:>:D:Z:m:s:
:$:,:4:@:`:h:p:x:
:$:,:4:@:`:h:t:
:$:,:4:@:`:l:
:$:,:4:<:D:L:T:\:d:l:t:
:$:,:4:<:D:L:T:\:d:l:t:|:
:$:,:4:<:D:L:T:\:d:l:x:
:$:,:4:<:D:L:T:\:d:p:
:$:,:4:<:D:L:T:\:h:
:$:,:4:<:D:L:X:|:
:$:,:4:<:D:P:p:x:
:$:,:4:<:D:P:t:|:
:$:,:4:<:H:h:p:x:
:$:,:8:@:t:
:$:,:8:\:d:l:t:|:
:$:,:8:X:d:
:$:.:B:H:L:`:r:
:$:0:<:H:T:`:l:x:
:$:0:P:X:`:h:p:
:$:0:P:X:`:h:x:
:$:0:T:\:d:l:t:|:
:$:4:@:d:l:t:|:
:$:4:8:H:T:d:t:
:$:5:
:$:9:E:_:{:
:$:A:[:a:t:
:$:C:d:
:$:I:
:$:l:
:$:L:T:\:d:
:$:x:
:$:X:`:h:p:x:
:$;,;0;4;<;@;D;H;p;x;|;
:$;.;C;X;v;
:$;\;
:$;^;
:$;+;=;R;o;
:$;2;H;
:$;8;X;g;
:$;G;
:$;j;
:$;J;z;
:$;m;
:$;T;w;
:$<6<b=
:$<Q<
:%:1:=:e:
:%:1:B:
:%:1:Y:
:%:D:
:%;,;E;l;s;
:%;9;X;g;
:%;G;x;
:&:?:P:m:
:&:|:
:&:+:;:@:T:d:y:
:&:2:>:J:V:b:n:z:
:&:A:f:
:&;.;L;S;h;};
:&;:;F;W;
:&;};
:&;a;
:&;A;f;
:&;H;
:&;J;V;d;
:&;n;
:&;q;
:(:::T:^:
:(:-:=:
:(:@:V:\:p:v:
:(:0:4:8:@:D:H:L:t:|:
:(:8:H:l:t:|:
:(:8:H:X:h:x:
:(:D:T:`:h:
:(:e:q:
:(:H:P:\:|:
:(:H:P:X:`:h:x:
:(:H:P:X:d:
:(:H:T:t:
:(:l:
:(:L:T:\:d:l:t:
:(:L:T:\:d:l:t:|:
:(;/;G;r;
:(;\;
:(;`;
:(;4;@;L;k;
:(;7;
:(;9;
:(;C;
:(;f;
:(;n;
:):.:D:I:Z:d:o:
:):~:
:):5:]:
:):j:
:):J:\:c;q;
:);/;D;J;b;s;y;~;
:);?<
:);3;G;T;i;o;
:);3;M;T;j;p;
:);6;F;K;P;
:);C<
:);H;r;
:*:=:
:*:5:
:*:8:~:
:*:A:j:
:*:B:Q:]:z:
:*:G:R:h:t:y:
:*;g;
:*;k;u;
:*;M;
:*;u;
:*;X<
:,:?:
:,:<:D:T:`:p:
:,:<:L:\:l:t:
:,:=:_:`;{;
:,:3:@:L:]:
:,:3:L:]:c:p:z:
:,:4:@:H:|:
:,:4:<:D:L:T:\:d:l:t:|:
:,:4:A:Y:_:n:y:
:,:5:S:
:,:6:d:u:a;f;U>q>v>H?d?i?
:,:7:C:U:t:
:,:B:I:^:d:v:
:,:x:
:,;<;
:,;o;
:,<1<
:,u0B
:.:::P:e:
:.:;:Y:
:.:G:j:{:
:.:G:X:^:k:}:
:.:M:l:
:.:S:
:.:W:
:.;I;^;d;
:.;I;Z;`;
:/:@:
:/:}:
:/:7:G:O:[:g:
:/:F:a:
:/:K:b:};
:/:P:i:
:/;5;;;S;~;
:/;O;
:/;Y;t;
:::m:
:::M:Y:
:::t:
::;B;v;!<.<B<
:':^:
:':1:Q:V:`:l:}:
:-:2:W:p:
:-:3:
:-:9:J:
:-:B:_:
:-:f:p:
:-:i:
:-:P:
:;:^:
:;:E:b:
:';=;
:-;=;
:-;H;Y;
:';i;
:-;m<
:-;R;
:?:j:o:
:?:N:
:?:n:
:?;Q;f;
:@:H:L:P:X:\:`:d:
:@:T:\:d:l:t:|:
:@;(<G<Q<f<{<
:[:m:y:
:[>n>s>
:\<e<r=
:]:s:
:_;x;
:_;y<
:`:p:|:
:};D<
:~;Y=p=H>_>
:+:1:F:M:s:y:~:
:+:2:?:I:j:w:
:+:2:J:b:z:
:+:7:C:Q:n:
:+:9:G:S:Y:b:h:n:t:
:+:d:z:
:+:V:x:
:+:Y:
:+;J;Q;f;{;
:<:_:
:<:D:L:T:\:d:
:<:D:L:T:\:d:l:t:
:<:D:L:T:\:d:l:t:|:
:<:D:L:T:\:h:
:<:D:P:p:x:
:<:H:h:p:
:<:H:h:p:|:
:<:n:
:<:Q:f:n:v:
:<:t:
:<;^;
:<;D;L;X;x;
:<;H;
:<;J;N;R;V;Z;^;b;f;j;n;r;v;z;~;
:<;U;
:<<Q<c<k<s<
:=:d:
:=;h;{;
:=;j;
:=;J;v;
:>:^:
:>:`:
:>:~:9;R;
:>:q:!;6;=;Y;
:>:R:x:
:0:::
:0:<:\:d:l:x:
:0:<:D:\:d:
:0:=:Q:e:v:
:0:>=
:0:6:M:h:
:0:7:P:a:g:t:
:0:8:@:H:X:|:
:0:8:<:@:H:L:P:T:|:
:0:8:D:d:l:x:
:0:8:D:d:p:
:0:G:[:j:
:0:j:
:0:O:
:0:y:
:0;F;
:0;h;
:0;Y;
:0D0s0~0
:0Q0j0
:1:`:
:1:|:
:1:=:I:U:}:
:1:=:I:U:t:
:1:=:I:X:x:
:1:J:a:z:
:1:L:z:
:1:q:
:1:t:
:1:Z:q:
:1;];c;x;~;
:1;^;v;
:1;A;Q;a;q;
:1;j;
:1;Q;
:2:<:
:2:=:]:
:2:D:
:2:K:r:
:2:M:
:2:y:
:2;|;
:2;7;
:2;9;R;`;
:2;9;R;v;};
:2;P;W;
:2;r;
:2;s;
:3:?:P:
:3:|:
:3:9:=:G:n:~:
:3:H:
:3:I;v;
:3:J:a:x:
:3;g;9<W<]<a<i<z<~<
:3;K;R;g;|;
:3;m;
:3;Q;
:3;Q;e;q;};
:3;s;
:4:<:D:L:T:\:h:
:4:<:D:L:T:`:
:4:<:D:P:p:x:
:4:<:H:p:x:
:4:A:
:4:d:
:4:D:P:X:x:
:4:h:}:
:4;f;
:4;l;
:5:M:R:n:
:5:O:
:5;?;g;r;
:5;B<y<
:5;M;
:6;?<
:6;T;[;
:6<@<U<j<
:7:>:V:m:
:7:e:o:
:7:K:^:
:7;\;f;
:7;i;
:7;k;
:7;O;V;
:8:@:D:H:P:T:X:\:
:8:@:H:T:t:|:
:8:@:L:l:x:
:8:d:
:8:D:[:
:8:I:
:8:l:
:8:T:d:p:x:
:8;?;X;
:8;h;
:8;q;v;
:8;X;
:9:D:c:
:9:H:a:v:
:a:V;#<6<
:A;K;p;
:A;Y;`;u;
:AM:am:PM:pm
:B:]:
:B:i:
:B:M:
:B:W:k:
:B>K>b>n>|>
:C:[:e:q:
:C;a;m;y;
:D:|:
:D;}<
:D;H;L;P;T;X;\;`;
:D;R;_;g;o;
:E:Y:x:
:e;|;
:e;l;
:E;Y;
:F:`:
:F:a:
:f:p:
:F:V:[:~:
:F;\;
:F<z<
:G:i:
:G:x:
:G;};
:G;L;u;z;
:G;T;
:H:c:z:
:h:r:
:H:T:`:l:
:h;c<
:H;x;
:H<v<
:hH[F
:I:b:
:I:w;
:i;p;
:I;W;e;s;
:I<h<~<
:J:q:
:j:v:
:J;O;
:J< =
:J=g=
:Jan:January:Feb:February:Mar:March:Apr:April:May:May:Jun:June:Jul:July:Aug:August:Sep:September:Oct:October:Nov:November:Dec:December
:k;b<
:L:]:c:h:|:
:L:k:w:
:L:S:k:
:L;T;|;
:M:r:|:
:m:z:
:N;X;q;
:N>@?
:o:y:}:
:O;a;v;
:o;v;
:P:a:g:l:
:P:U:Z:b:
:P:v:
:p;z;
:Please, install necessary language-pack-XX and configure locales
:Q:i:
:Q:q:
:R:p:w:
:R:X:^:v:
:r:z:8;D;P;
:r;w;
:S:q:
:S:q:}:
:S:y:
:S;R=
:Sun:Sunday:Mon:Monday:Tue:Tuesday:Wed:Wednesday:Thu:Thursday:Fri:Friday:Sat:Saturday
:T;7<
:U;\;u;
:V:q:
:V:x:
:V;u;
:V<`<u<
:w:/;Q;[;p;
:W:a:q:
:W;{;
:W;g;3<O=?>3?
:W;o;
:Y;`;u;
:Z:a:z:
:Z;g;
:Z;i;#<
:Z\@~
; :vk
; ;$;(;,;0;4;8;<;@;D;H;L;P;T;X;\;`;d;h;l;p;t;x;|;
; ;$;(;,;0;4;8;<;@;D;H;L;P;T;X;\;`;d;h;p;t;x;|;
; ;$;(;P;X;\;`;h;l;p;t;
; ;$;,;0;4;8;`;h;l;p;x;|;
; ;$;4;
; ;(;0;8;D;d;l;x;
; ;(;8;@;d;t;|;
; ;(;8;D;L;T;\;d;l;|;
; ;*;/;L;\;g;s;x;
; ;*;@;J;
; ;,;L;T;\;h;
; ;:;a>
; ;@;H;P;\;|;
; ;@;H;P;X;`;h;t;
; ;@;L;l;x;
; ;@;O;
; ;=;_;~;
; ;0;@;P;`;p;
; ;0;@;P;`;p;x;
; ;0;8;@;H;P;`;h;p;x;
; ;3;E;
; ;5;J;v<'=
; ;7;H;Y;j;{;
; ;8;Q;
; ;e;
; ;r;
; <0<<<D<d<
; <4<H<W<
; <f<
; expected 
; last read: '
;!;&;:;F;W;
;!;/;s;
;!;:;K;
;!;';<;B;X;^;t;z;
;!;-;9;E;d;
;!;-;9;E;m;
;!;-;9;H;h;
;!;1;E;U;i;y;
;!;2;O;d<n<
;!;9;J;P;U;a;k;u;
;!;D;f;
;!;f;
;!;F;a;
;!;G;[;
;!;K;c;j;
;!;Z;q;
;!?|?
;!<B<
;!<k<
;!<Z<q<
;";&;*;.;2;6;:;G;
;";(;8;=;T;e;k;p;|;
;";(;B;H;b;h;
;";.;:;F;R;^;j;v;
;";.;?;
;";';>;C;Z;
;";4;G;Q;e;k;o;
;";4;s;
;";f;
;"<(<=<C<U<\<
;"<)<A<o<
;"<?<Z<d<w<
;"<A<s<
;"<r<
;#;(;
;#;);l;
;#;.;>;E;U;\;l;s;
;#;/;I;e;u;{;
;#;@;`;
;#;+;3;i;1<
;#;4;E;V;g;l;};
;#;4;Q;
;#;9;
;#;A;M;Y;e;
;#;H;e;
;#;s;
;#<;<B<d<
;#<A<M<Y<e<
;#<c<
;#<f<
;#<h<
;#<I<T<s<
;#<J<q<
;#<R<s<
;$;(;,;0;X;`;d;h;p;t;x;|;
;$;(;,;4;8;<;@;h;p;t;x;
;$;,;`;h;p;x;
;$;,;4;@;`;h;p;|;
;$;,;4;@;`;l;
;$;,;4;@;d;l;t;|;
;$;,;4;<;D;L;T;\;d;l;t;
;$;,;4;<;D;L;T;\;d;l;t;|;
;$;,;4;<;D;L;T;\;d;l;x;
;$;,;4;<;D;L;T;\;d;p;
;$;,;4;<;D;L;T;\;d;p;x;
;$;,;4;<;D;L;T;`;
;$;,;4;<;D;L;X;x;
;$;,;4;<;H;h;p;x;
;$;,;8;X;`;h;p;|;
;$;/;5;;;J;S;Y;_;h;q;{;
;$;;;K;Q;Y;n;z;
;$;;;V;
;$;=;M;X;};
;$;0;8;X;t;
;$;0;P;\;|;
;$;0;P;X;`;h;x;
;$;0;T;t;|;
;$;5;;;};
;$;5;O;x;
;$;6;=;V;g;m;z;
;$;C;
;$;c;{;
;$;D;
;$;D;P;p;|;
;$;D;P;p;x;
;$;x;
;$<\<
;$<Y<m<v<
;%;=;w;
;%;1;=;I;h;
;%;9;E;Q;];|;
;%;a;
;%;Z;f;r;
;%<*<
;%<5<A<M<l<
;%<J<T<t<
;%<v<
;%=,=E={>4?
;&;?;w;
;&;6;Z;j;
;&<A<
;&=1=
;(;/;B;H;\;b;w;};
;(;;;T;j;w;
;(;4;@;L;k;
;(;4;<;p;
;(;4;B;_;
;(;4;T;`;
;(;9;
;(;9;z;
;(;F;M;f;w;
;(;H;P;X;`;h;x;
;(;L;T;\;d;l;t;|;
;(;N;Y;
;(;s;
;(<9<|<
;(<B<t<
;(<J<j<
;);.;:;G;`;p;{;
;);=;J;_;e;|;
;);0;I;b;
;);3;G;`;m;
;);E;
;);L;Y;};
;);R;
;)<E<s<y<
;)<S<^<t<
;)>9>p?{?
;*;;;|;
;*;8;t;
;*;A;Z;q;
;*;C;I;
;*;L;V;n;
;*<@<Y<_<
;*<A<
;*<H<
;*<q<
;*<T<$=t=
;,;;;@;R;Y;f;r;
;,;\;
;,;1;K;T;j;
;,;1;x;
;,;4;@;d;l;t;|;
;,;4;<;D;L;T;\;d;p;
;,;4;<;H;h;p;x;
;,;8;@;`;|;
;,;8;@;d;x;
;,;8;X;`;h;t;
;,;a;
;,;D;\;d;|;
;,;I;j;t;
;,<_<
;,<{<
;,<B<
;,<d<
;,<G<
;,<N<g<q<
;,<w<
;.;`;
;.;3;B;G;V;[;j;o;~;
;.;8;B;L;V;
;.;H;
;.<U<r<
;.=8=J=g=
;/;@;];
;/;<;H;Y;
;/;J;V;[;q;v;
;/;L;Q;`;e;
;/;s;
;/<:<
;/<_<i<u<
;/<}<
;/<9<E<Q<p<
;/<L<a<w<
;:;];n;
;:;F;O;T;a;y;
;:;G;n;
;:;p;
;:;Q;z;
;:<|<
;:<D<
;:<H<L<P<T<X<\<`<d<h<l<p<t<x<|<
;:<N<
;:<O<
;:<x<
;';:;K;
;;;F;
;;;W;d;p;
;';<;Q;n;
;;<^<
;-;'<w<
;;=v=
;-;>;
;';>;D;^;
;';3;D;
;-;7;I;^;{;
;';B;Y;
;';e;k;q;
;';h;
;';m;t;
;-;Z;~;
;?;a;
;?;I;U;a;
;?;s;
;?;U;
;?<9=
;@;U;
;@<`<i<
;@<K<V<
;[;b;t;
;[<e<z<
;\$ r
;\$,|
;\;f;
;\<>=
;\<f<|<
;\<v=
;];u;
;]@}Y
;]lu";Epu
;^,r;w
;^,tA
;^HrU
;^HrY
;^L|\
;_$}2
;_L|Y
;{@})
;{<}~
;{<}+
;|$,|
;|84u
;~ ~[3
;~HrU
;+;:;Z;y;
;+;0;E;J;O;e;j;o;
;+;1;D;r;
;+;4;+<
;+;5;q;
;+;L;V;o;
;+<4<~<
;+<5<H<c<}<
;+<X<c<
;<;^;
;'<;<
;<;A;M;_;d;x;};
;<;D;L;T;
;<;D;L;T;\;d;l;t;|;
;<;D;L;T;\;d;p;
;<;H;P;
;<;L;X;`;
;<;L;X;|;
;<;t;
;<<~<
;<<F<[<p<
;'<=<
;'<B<
;'<f<
;'<P<
;=;];
;=;N;[;};
;=;T;l;s;
;=<X<t<
;=0+Q
;=4+Q
;-=G=
;><b<|<
;><E<L<T<
;0;4;L;\;`;d;|;
;0;8;@;H;P;X;d;
;0;G;^;u;
;0;P;X;`;l;
;0<]<q<
;0<U<m<
;0B0T0i0
;0S0Z0s0
;0v:f
;0V0i0
;1;:;W;
;1;_;i;
;1;=;I;U;};
;1;=;I;U;t;
;1;c;t;X?
;1;L;
;1;Q;v;
;1;S;l;
;1;z;
;1<8<
;1<j<
;1<n<
;1<r<
;1<t<
;1<X<
;2;?;t;
;2;7;E;R;h;
;2;b;
;2;C;
;2;k;r;y;
;2<r<
;3;@;];{;
;3;C;\;l;w;
;3;j;
;3;P;K<R<g<|<
;3<7=
;3<A<O<z<
;3<c<
;4;@;`;h;t;
;4;@;Y;_;c;m;
;4;<;D;L;l;x;
;4;<;D;L;T;\;d;|;
;4;<;D;L;T;\;d;l;t;|;
;4;<;D;L;T;\;h;p;
;4;<;D;L;T;`;
;4;<;D;L;X;x;
;4;<;H;h;p;|;
;4;<;H;h;p;x;
;4;>;P;p;
;4;J;P;h;~;
;4<a<
;4<l<
;4<n>
;4<z<
;5=V=]>~>
;6;@;`;
;6;[;e;
;6;B;
;6;G;M;R;\;a;x;
;6;J;h;z;
;6<Q<
;6<v<
;6<w<
;7;?;K;X;h;r;
;7;|;
;7;A;T;r;
;7;m;{;
;7;N;
;7;R;\;
;7;T;o;
;7<><W<
;7<D<W<
;8;D;L;
;8;T;d;p;x;
;8<?<T<i<
;8<_<2=f=
;8<r<
;8sLf
;9;C;h;
;9;D;
;9;D;c;
;9;h;y;
;9;O;
;9;R;t;
;9<><
;9<Q<
;9<s<
;A(r3
;a;k;
;A;Q;e;
;A;u;
;A;w;
;ALtK
;B;l;
;B<c<
;B<L<t<
;B<m<z<
;B=|==>
;C sV
;C;a;m;y;
;C;f;
;C;h;r;
;C;w;
;C<f<
;C<H<
;C<l>
;CHrg
;D$ |
;D$ t/
;D$ t0
;D$ u
;D$$u
;D$@r
;D$4r
;D$dQ
;D;|;
;D;d;
;D;k;
;d<n<
;D<Q<
;E vG
;E vJ
;E,vG
;e;&>+>]>b>
;e;1<
;e;l;
;E@}P
;F t+
;F t9jp_9
;F(v4
;F,^u
;F;i;
;f<~<
;F<d<n<
;F<i<
;F<X<
;FHrU
;FHrV
;FL|\
;FL|`
;Fp^u
;G u7hL
;G<c=
;G0s';Gxu
;GHrU
;h;;<
;H;Q;
;H<V<
;H<W<x<
;i;w;
;J;};
;J<S<v<
;j<t<
;K;R;d;y;
;k;u;
;K;x;
;k<3>
;K<a<z<
;K<P<
;L$$|
;L$`|
;L$4r
;L$X|
;L;`;m>
;L<f<
;L<r<
;L<V<o<
;M;`;0<r<
;M;R;W;
;m<t<
;MD}C
;n;{;
;N;|;
;N<a<
;N<k<
;NhsU
;O<Y<~<
;OHrR
;P;u;
;P<`<m<v<
;P<5=?=T=i=
;P0td
;Q<h<
;R;W;
;R;Y;q;G=_=
;r<y<
;S;o;y;
;S;x;
;S;y;
;s<t=
;T$ t
;t$$|
;t$$r
;t$(r
;T$,u@3
;t$,v-
;t$0r
;t$dr
;T;^;h;x;
;T=u>
;u t5
;u te
;u tzk
;u$s8
;U,|@
;u,s/
;U;k;
;u;S<X=N>
;U;z;
;uD}P
;V;\;b;z;
;v;};
;V;'<d<M=y=
;V<a<l<w<
;V<c<r<z<
;VL|\
;VL|]
;W<p<
;WL|\
;X;l;};
;X;x;
;X<a<g<
;Y;~;
;YY7|
;Z;q;
? ?$?(?,?0?4?8?<?@?D?H?L?P?T?X?\?`?d?h?l?p?t?x?|?
? ?$?L?T?X?\?d?h?l?p?
? ?(?@?H?l?|?
? ?(?0?@?P?X?`?p?x?
? ?(?0?<?\?d?p?
? ?(?0?8?@?L?l?t?|?
? ?(?0?8?<?D?X?t?x?
? ?(?0?8?D?d?l?t?
? ?(?8?\?d?l?t?|?
? ?,?=?
? ?,?4?L?p?
? ?,?L?T?`?
? ?,?L?X?x?
? ?:?A?X?n?
? ?@?H?P?X?d?
? ?@?H?T?t?
? ?@?L?l?x?
? ?[?
? ?<?L?X?`?
? ?0?@?P?`?p?x?
? ?0?8?H?P?`?h?x?
? ?0?T?\?d?l?
? ?3?N?
? ?C?
? ?d?
? ?D?L?T?\?d?l?t?|?
? ?E?L?e?~?
? ?o?v?
? ?U?
? ?Z?
? @ T T 3
?!?%?)?-?1?A?
?!?*?A?J?a?q?
?!?:?Q?j?
?!?'?<?B?W?]?r?x?
?!?-?9?E?d?
?!?-?9?X?
?!?4?:?R?c?i?n?z?
?!?5?A?M?Y?x?
?!?5?I?
?!?6?
?!?6?S?
?!?Q?e?
?!?S?]?i?u?
?"?&?*?.?2?6?X?
?"?&?:?L?n?x?
?"?)?6?>?D?N?X?b?f?
?"?'?6?;?J?O?^?c?r?w?
?"?6?J?^?r?
?"?7?<?R?W?i?t?z?
?"@x@
?"_^t
?#?.?4?9?M?]?r?
?#?+?@?L?f?
?#?8?>?V?g?m?r?~?
?#?I?u?
?#?j?
?#?q?
?#?s?
?$?(?,?0?4?8?@?X?h?l?p?t?x?|?
?$?*?=?C?U?[?o?u?
?$?,?`?p?|?
?$?,?4?<?D?L?T?\?d?l?t?|?
?$?,?4?<?D?L?T?\?d?l?x?
?$?,?4?<?D?L?T?\?d?p?
?$?,?4?<?D?L?T?\?t?
?$?,?4?<?d?l?t?|?
?$?,?4?<?D?L?X?`?
?$?,?4?<?D?L?X?|?
?$?,?4?<?D?P?p?x?
?$?,?4?<?D?P?X?x?
?$?,?4?<?H?l?t?|?
?$?,?8?X?`?h?p?x?
?$?,?8?X?`?h?x?
?$?,?A?M?g?
?$?/???F?V?]?m?t?
?$?/?G?L?X?q?
?$?;?R?i?
?$?@?d?l?t?|?
?$?`?
?$?<?L?P?`?d?t?x?|?
?$?0?:?D?N?X?b?l?v?
?$?0?8?X?t?
?$?0?T?\?d?l?t?|?
?$?9?N?k?
?$?A?N?
?$?A?R?
?$?C?
?$?C?a?m?y?
?$?D?`?
?$?D?L?T?`?
?$?D?P?p?|?
?$?p?
?$?X?`?h?p?x?
?%?/?:?R?W?c?|?
?%?-?T?\?d?n?v?~?
?%?1?=?\?
?%?1?=?I?h?
?%?2?Q?
?%?3?
?%?E?J?h?m?
?%?H?Y?u?
?%?M?
?%?M?y?
?&?.?6?
?&?+?@?F?K?a?f?}?
?&?+?0?E?J?O?e?j?o?
?&?>?
?&?7?=?B?N?X?j?o?
?&?B?
?&?G?`?
?&?I?
?&?J?Z?~?
?&?O?
?&TpxAD
?(?,?4?8?<?D?H?L?P?t?x?
?(?-?Y?d?m?v?
?(?0?8?D?d?l?t?
?(?4?@?h?
?(?4?T?`?
?(?7?
?(?8?@?P?T?d?h?x?
?(?8?H?P?`?d?t?
?(?8?H?X?h?x?
?(?f?l?r?
?(?H?P?\?|?
?(?s?
?)?:?
?)???X?d?
?)?<?H?
?)?0?F?L?c?~?
?)?2?7?d?
?)?4?S?q?
?)?5?N?T?X?b?
?)?5?T?
?)?8?X?
?)?k?r?
?)?Q?
?)?Z?i?
?*?/?A?M?R?h?m?
?*?_?q?
?*?4?C?c?
?*?6?
?*?6?B?N?
?,?<?`?
?,?=?C?P?Z?l?s?
?,?3?8?L?S?h?u?
?,?4?\?d?l?t?|?
?,?4?<?D?L?T?\?d?l?x?
?,?5?a?
?,?8?X?`?h?x?
?.%>s
?.?>?
?.?I?\?h?
?.?S?
?.?X?
?/?_?s?
?/?A?F?X?e?|?
?/?B?
?/?I?R?
?/?J?
?/?T?
?;?A?F?R?\?l?q?
?;?F?`?d?h?l?p?t?x?|?
???Y?
?-?@?
?'?\?
?'?6?Z?a?i?|?
?'?a?
?'?K?V?
?-?Y?_?u?{?
?@?J?c?
?@?L?]?
?@s-f
?^?k?w?
?^?o?u?z?
?`a?i,
?+???S?c?w?
?+?<?
?+?5?J?_?
?+?O?\?m?
?+?V?}?
?+?V?x?
?<?|?
?<?D?H?L?T?X?\?`?
?<?D?L?h?p?x?
?<?D?L?X?|?
?<?H?h?p?|?
?<?H?h?t?
?<?R?c?i?
?<?t?
?<?T?^?
?>?~?
?0?@?P?t?|?
?0?<?\?d?p?
?0?8?@?H?P?\?|?
?0?8?@?H?P?\?d?
?0?C?M?R?c?n?t?
?0?L?V?k?
?1?=?I?U?t?
?1?E?Q?]?i?
?1?F?
?1?F?c?
?1?Q?s?~?
?1?z?
?1?Z?q?
?1W1^1s1
?2?8?R?X?r?x?
?2?K?^?v?
?2?O?l?
?3?]?
?3?`?
?3?`?j?
?3?E?X?
?3?M?
?3?T?
?3?V?
?333333
?4?:?]?w?
?4?;?T?e?
?4???
?4?<?@?D?L?P?T?X?
?4?<?D?L?T?\?d?l?t?|?
?4?<?D?L?T?\?l?t?
?4?<?D?P?p?x?
?4?<?H?h?p?|?
?4?B?
?4?B?~?
?4?g?~?
?4?L?V?k?
?5?:?
?5???T?i?
?5?=?N?w?
?5?e?
?5?F?R?_?e?k?q?
?5?J?c?y?
?5?u?
?6?E?
?6?F?j?z?
?6?v?
?7?A?Z?
?7?B?J?f?n?v?
?7?j?
?8?@?H?P?X?`?l?
?8?@?L?l?t?
?8?@?P?t?|?
?8?J?
?8?r?
?8?r?y?
?9?D?
?9?D?\?
?9?D?c?
?9?M?Y?j?
?A?O?Y?o?u?y?
?B?g?q?
?B?I?a?
?B?N?
?b?x?
?c?w?
?C?W?`?
?C?W?k?
?D?|?
?D?L?P?T?\?`?d?h?
?d?u?{?
?d?z?
?e?k?s?
?e?l?
?F?d?
?F?M?r?
?gSv0t
?h?{?
?H?N?d?k?
?h?o?
?J?^?
?J?k?
?J?o?
?J?T?m?~?
?J?z?
?L?`?x?
?l?}?
?L?d?l?t?|?
?L?S?Z?b?
?L?T?\?d?l?x?
?M?^?o?
?O?e?
?O?i?
?O?q?
?P?\?h?v?
?P?]?
?P?`?l?t?
?P?|?
?Q?h?
?q?x?
?QPSjpZ
?R?W?
?s?}?
?S?n?
?S?q?
?U?k?
?V?]?
?V?x?
?V?y?
?w;WV
?W?x?
?w6VS
?wBVS
?wpVW
?X?b?
@ 90u
@$QPW
@.data
@.reloc
@?QWh
@@QLinearGlobalAveragePool ImageSize too large!
@^l}#=
@_^[]
@`WVQ
@|+Cx
@>D>H>L>d>h>x>|>
@>t!zl
@0d0l0t0|0
@0Q0Y0f0q0}0
@h"OZ
@H;AH
@H;AHt.h
@t";7r
@t"VS
@t$;;r
@Vj(j
@Wi!_
[%hs(%hs)]
[%hs]
'[', '{', or a literal
[:^alnum:]
[:^alpha:]
[:^ascii:]
[:^blank:]
[:^cntrl:]
[:^digit:]
[:^graph:]
[:^lower:]
[:^print:]
[:^punct:]
[:^space:]
[:^upper:]
[:^word:]
[:^xdigit:]
[:alnum:]
[:alpha:]
[:ascii:]
[:blank:]
[:cntrl:]
[:digit:]
[:graph:]
[:lower:]
[:print:]
[:punct:]
[:space:]
[:upper:]
[:word:]
[:xdigit:]
[]^-\
[^\x00-\x{10ffff}]
[9_ t
[h$2E
[h@>D
[h`MD
[h4<E
[hD$E
[hDQE
[hPNE
[hxaD
[json.exception.
[k/.pt
[libprotobuf %s %s:%d] %s
[Memory] SessionStateInitializer statically allocates 
[ONNXRuntimeError]
[ParseError at position 
[ShapeInferenceError] 
[tX9]
[TypeInferenceError] 
\$ ;^
\$ ;}
\$(;_
\$(;X
\$,;|$$
\$8VQ
\$D;]
\$D9]
\$H;L$X
\$L;X
\$T;\$X
\*V%<
\0%3~3b4E6
\hl]D
\i80)W
\M1N"uc
\RichF
\x%02x
\x{%x}
] != number of classlabels[
] (usually, this means you 
] already exists with value [
] because it's the graph's output.
] for now
] is not supported this build 
] is not supported!
] is not supportted!
] not in lexicographic sorted order.
] not in sorted order.
] op_type [
] out of range [0, 
] out of range.
] should not be greater than specified axis dim value [
], "core": 
], could not find NodeArg 
], Value=
], while 
]. Actual value is 
]. It will be overwritten
]. Its actual value is: 
]<+u8
]=1C:
]4;]@}<
]h$2E
]h4<E
]S9SH
^|*W?
^|+^x
^hD)E
^VhXdI
^xjtX
_^[Y]
_^[YY]
__strncnt
_Cast
_D$ YY
_DmlExecutionProvider
_DmlExecutionProvider_
_dummy
_fence_after
_fence_before
_FusedMatMulAndScale
_initterm
_initterm_e
_Int32
_kernel_time
_lock_locales
_m\p?S
_min_zero_constant
_o____lc_codepage_func
_o____lc_collate_cp_func
_o____lc_locale_name_func
_o____mb_cur_max_func
_o___acrt_iob_func
_o___pctype_func
_o___std_exception_copy
_o___std_exception_destroy
_o___stdio_common_vfprintf
_o___stdio_common_vsnprintf_s
_o___stdio_common_vsprintf
_o___stdio_common_vsprintf_s
_o___stdio_common_vswprintf
_o__aligned_free
_o__aligned_malloc
_o__beginthreadex
_o__callnewh
_o__calloc_base
_o__cexit
_o__CIcosh
_o__CIfmod
_o__CIpow
_o__CIsinh
_o__CItanh
_o__close
_o__configure_narrow_argv
_o__create_locale
_o__crt_atexit
_o__dclass
_o__difftime64
_o__errno
_o__execute_onexit_table
_o__fdclass
_o__fdsign
_o__free_base
_o__free_locale
_o__fseeki64
_o__fstat64i32
_o__get_errno
_o__get_stream_buffer_pointers
_o__Getdays
_o__Getmonths
_o__Gettnames
_o__gmtime64_s
_o__initialize_narrow_environment
_o__initialize_onexit_table
_o__invalid_parameter_noinfo
_o__invalid_parameter_noinfo_noreturn
_o__libm_sse2_acos_precise
_o__libm_sse2_asin_precise
_o__libm_sse2_atan_precise
_o__libm_sse2_cos_precise
_o__libm_sse2_exp_precise
_o__libm_sse2_log_precise
_o__libm_sse2_pow_precise
_o__libm_sse2_sin_precise
_o__libm_sse2_sqrt_precise
_o__libm_sse2_tan_precise
_o__localtime64_s
_o__lock_file
_o__malloc_base
_o__mktime64
_o__purecall
_o__read
_o__realloc_base
_o__register_onexit_function
_o__seh_filter_dll
_o__set_errno
_o__sopen_s
_o__stat64i32
_o__Strftime
_o__strnicmp
_o__strtoi64
_o__towlower_l
_o__towupper_l
_o__unlock_file
_o__W_Getdays
_o__W_Getmonths
_o__W_Gettnames
_o__wcsdup
_o__Wcsftime
_o__wfsopen
_o__write
_o__wsopen_s
_o_abort
_o_acoshf
_o_asinhf
_o_atanhf
_o_bsearch
_o_calloc
_o_ceil
_o_fclose
_o_fflush
_o_fgetc
_o_fgetpos
_o_floor
_o_fputc
_o_fread
_o_free
_o_frexp
_o_fseek
_o_fsetpos
_o_fwrite
_o_isalnum
_o_isalpha
_o_isdigit
_o_islower
_o_isspace
_o_isupper
_o_iswspace
_o_ldexp
_o_localeconv
_o_log2
_o_log2f
_o_malloc
_o_nearbyintf
_o_remainderf
_o_rint
_o_rintf
_o_roundf
_o_scalbn
_o_scalbnf
_o_setlocale
_o_setvbuf
_o_strerror
_o_strncpy_s
_o_strtod
_o_strtof
_o_strtol
_o_strtoll
_o_strtoull
_o_terminate
_o_tolower
_o_ungetc
_o_wcsftime
_RuleBasedTransformer
_sum_transformed
_token_
_transformed
_unlock_locales
_Unused
_WhHDI
`.rdata
`@33{@33{@33{@33{@
``PJn
`0^0\
`2e2$3
`anonymous-namespace'::GetDataTransfer
`anonymous-namespace'::GetExternalDataInfo
`anonymous-namespace'::GetIndicesTensor
`anonymous-namespace'::ReadExternalDataForTensor
`anonymous-namespace'::ValidateFillInputArgs
`c` - cell gate
`dNn\
`f` - forget gate
`h$2E
`h\?D
`h` - hidden gate
`H` - Hidden state
`hXLD
`i` - input gate
`num_directions` - 2 if direction == bidirectional else 1
`o` - output gate
`P[iof]`  - P peephole weight vector for input, output, and forget gates
`PB[iof]`  - P peephole weight vector for backward input, output, and forget gates
`r*wigWk
`R[iofc]` - R recurrence weight matrix for input, output, forget, and cell gates
`R[zrh]` - R recurrence weight matrix for update, reset, and hidden gates
`r` - reset gate
`Rb[iofc]` - R bias vectors for input, output, forget, and cell gates
`RB[iofc]` - R recurrence weight matrix for backward input, output, forget, and cell gates
`Rb[zrh]` - R bias vectors for update, reset, and hidden gates
`RB[zrh]` - R recurrence weight matrix for backward update, reset, and hidden gates
`RBb[iofc]` - R bias vectors for backward input, output, forget, and cell gates
`RBb[zrh]` - R bias vectors for backward update, reset, and hidden gates
`RBbi` - RR bias vectors for backward input gate
`Rbi` - R parameter bias vector for input gate
`RBi` - R recurrence weight matrix for backward input gate
`Ri` - R recurrence weight matrix for input gate
`t` - time step (t-1 means previous time step)
`tf_half_pixel_for_nn` is deprecated since opset 13, 
`W[iofc]` - W parameter weight matrix for input, output, forget, and cell gates
`W[zrh]` - W parameter weight matrix for update, reset, and hidden gates
`Wb[iofc]` - W bias vectors for input, output, forget, and cell gates
`WB[iofc]` - W parameter weight matrix for backward input, output, forget, and cell gates
`Wb[zrh]` - W bias vectors for update, reset, and hidden gates
`WB[zrh]` - W parameter weight matrix for backward update, reset, and hidden gates
`WBb[iofc]` - W bias vectors for backward input, output, forget, and cell gates
`WBb[zrh]` - W bias vectors for backward update, reset, and hidden gates
`WBbi` - WR bias vectors for backward input gate
`Wbi` - W parameter bias vector for input gate
`WBi` - W parameter weight matrix for backward input gate
`Wi` - W parameter weight matrix for input gate
`X` - input tensor
`z` - update gate
{"cat" : "
{"main_thread": {
{%d,%d}
{%d,}
{.BMi
{\pu"
{additionalDocumentation}
{e/j;vL
{Ks_!
{name}
{Xpu!
|$  w
|$ ;~
|$ YY
|$$;}
|$(;{L|n
|$,+D$
|$,+L$
|$`PW
|$<QP
|$0;{
|$49\$4|
|$89},
|$D;x
|$Du/3
|$h;x
|$HPQ
|$HQP
|$L;8
|$LPW
|$PA;H
|$T;}
|$T9}
|$TPW
|]dHH
|49.V
|8h(GD
|jyd&1
|pj\O
|u$`a
} for per-channel quantization. Actual:
} for per-tensor/layer quantization or shape {
}, "sub_threads": {
}, actural: 
}, Got: 
}, input shape = {
}. Actual:
}. Got: 
}.VPW
}.wMSN
}4&6&
}L+L$
}m7lu
}N+L$
~,9~$t
~,j4j
~_;;r
~0;;r
~3a*~3a*~3a*~3a*
~4;;r
~4;=8+Q
~4;7r
~6;7r
~8;7r
~a;;r
~B;~,r
~h_^]
~h_^Y]
~X_[^
+ cannot be safely updated to 
+/+E+F+M+s+v+
+|$ Q
++<>||~~
++index < c.size()
++Q5@.
++Q5@.Q5@.Q5@.Q5@.
+080i0x0
+0Sh\
+0U0T2
+3J >
+D$ YY
+GduJ
+L$,xd
+N +N
+O(+W
+SAME_LOWER
+T$ ;
+v$x+v$xv$+xv+$xv$+x+$vx+$vx$v+x+$vx$+vx+v $+v $v $+v +$v $++$ v+$ v$ v++$ v$+ v+xv$+ v$v$ +v+ $v$ ++x$v+ $v$v ++ $v$ +v
+YieldOp
< <$<(<,<0<4<8<<<@<D<H<L<P<T<X<\<`<d<h<l<p<t<x<|<
< <$<(<,<4<8<<<@<T<X<h<l<p<t<x<|<
< <$<(<,<T<\<`<d<l<p<t<x<
< <$<L<T<X<\<d<h<l<p<
< <(<0<@<P<X<`<p<x<
< <(<0<<<\<d<l<x<
< <(<0<8<@<P<t<|<
< <(<0<8<D<d<p<
< <(<L<\<h<p<
< <,<L<T<\<d<l<t<
< <,<L<T<\<d<l<t<|<
< <,<L<X<x<
< </<4<C<H<W<\<k<p<|<
< <@<H<T<\<
< <@<H<T<t<|<
< <@<L<l<t<
< <@<P<|<
< <+<P<c<|<
< <<<L<X<`<
< <<<L<X<|<
< <0<4<D<H<X<h<x<
< <8<q<
< <D<L<T<\<d<l<t<|<
< <f<
< =(=,=0=8=<=@=D=l=t=x=|=
< =/=k=
< =@=c=
< =P=
< =X=
< >d>i>n>
<!<:<S<u<
<!<+<H<Y<_<d<p<
<!<-<9<E<d<
<!<-<C<X<k<~<
<!<-<U<
<!<0<<<D<T<\<l<t<
<!<5<A<M<\<|<
<!<5<A<M<Y<x<
<!<6<
<!<F<a<
<!<J<a<
<!<o<
<!=-=2=H=M=^=h=s=
<!=c=
<!=H=
<"<(<><D<\<t<
<"<(<B<H<b<h<
<"<<<
<"<-<8<
<"<7<<<A<X<]<t<y<
<"<h<
<"=_=
<"=~=
<"=C=k=
<"=s=
<"=y=
<">&>*>.>2>6>:>T>`>
<#<|=
<#<'<+</<3<7<;<?<C<G<K<O<S<e<
<#<<<`<g<
<#<7<D<P<a<
<#<A<M<Y<e<
<#<f<k<
<#<L<W<
<#<m<
<#<p<
<#=`=
<#=0>
<#=5=D=[=
<#=A=M=Y=e=
<#=A=U=a=m=y=
<#=H=R=r=
<#=I=h=
<#=I=T=
<#=I=T=s=
<#=L=
<$<,<4<@<`<h<p<x<
<$<,<4<@<`<h<t<
<$<,<4<@<d<l<t<|<
<$<,<4<<<D<L<T<\<d<l<t<|<
<$<,<4<<<D<L<T<\<d<l<x<
<$<,<4<<<D<L<T<\<d<p<
<$<,<4<<<D<L<T<`<
<$<,<4<<<D<P<p<x<
<$<,<4<<<H<h<x<
<$<,<4<<<H<l<t<|<
<$<,<8<\<d<
<$<,<8<X<`<h<x<
<$<,<8<X<`<l<
<$<,<8<X<d<
<$<<<D<P<t<|<
<$<0<8<X<t<
<$<0<8<X<t<|<
<$<0<P<X<`<p<
<$<0<T<\<d<l<t<|<
<$<1=
<$<5<v<
<$<7<
<$<D<L<T<\<h<
<$<D<L<T<`<
<$<E<]<d<
<$<F<
<$<s<
<$<S<
<$=?=
<$=\=
<$=o=
<$>g>
<%<,<3<E<Z<c<j<q<x<
<%</<H<Y<v<
<%<:<L<W<]<g<
<%<6<w<
<%<a<o<;=
<%<Q<
<%<Y<o<x<
<%=_=
<%=9=X=j=
<&<+<8<=<L<Q<`<e<
<&<A<
<&<B<[<w<
<&<F<
<&<s<
<&<U<p<<=\=f=
<&=c=
<&=H=
<&=K=U=
<&=L=]=|=
<&>v>
<(<,<<<H<X<h<p<
<(<:<
<(<@<V<
<(<><
<(<0<8<@<H<P<X<h<
<(<0<8<D<d<p<
<(<3<
<(<4<E<
<(<8<H<P<`<l<|<
<(<H<P<X<`<l<
<(<H<P<X<t<|<
<(<L<T<\<d<l<t<|<
<(<s<
<(<W<b<p<
<(<Y<h<
<(=`=
<(=2=K=x=
<(=3=
<(=7=
<(=P=
<)<3<=<G<Q<[<
<)<3<L<S<o<y<
<)<4<S<q<
<)<4<S<q<}<
<)<9<?<G<\<h<
<)<9<{<
<)<g<w<
<)<k<r<
<)<N<
<)<U<u<
<)=0=E=Z=w=
<)=3=\=g=
<)=6=g=v=
<)=A=H=]=r=
<)=B=
<*<:<^<n<
<*<;<
<*<[<p<
<*<<<^<h<|<
<*<0<@<E<\<m<s<x<
<*<1<J<
<*<4<
<*<4<><
<*<6<B<N<Z<f<r<~<
<*<A<j<
<*<b<
<*<n<
<*=@=I=n=
<*=5=1>
<*=7=h=w=
<*=D=
<,<=<N<_<p<
<,<4<@<`<l<
<,<4<<<D<L<T<\<d<l<t<|<
<,<4<<<D<L<T<\<d<l<x<
<,<4<<<D<L<T<d<l<t<|<
<,<4<P<t<|<
<,<6<~<
<,<8<W<j<
<,<8<X<`<l<
<,<8<X<d<
<,<A<^<
<,<D<U<f<p<*=g=
<,<I<j<
<,<X<~<
<,=`=
<,=1=9=A=
<,=4=<=D=L=X=`=
<,=l=
<,=W=
<,=X=_=t=
<.<?<
<.<5<><g<u<{<
<.<b<
<.<P<r<
<.<Q<
<.=3=w=
<.=7=j=w=
<.=B=h=|=
<.=G=Z=
<.=x=
</<5<w<
</<i<s<
</<N<m<
</<n<z<
</<Q<
</=_=
</=<=}>
</=>=
</=e=
</assembly>
<:<Q<u<
<:<Q<z<
<:<t<{<
<:=e=x=
<:=I=
<:=Q=
<:=R=Y=n=
<:t%<,t!</u%
<:u WSj<j
<:u#W
<;=a=
<;=B=T=i=
<;=D=
<;=H=T=e=
<;=s=
<;=V=y=
<?<c<
<?=a=
<?=D=
<?xml version='1.0' encoding='UTF-8' standalone='yes'?>
<@<l<
<@<M<Y<j<
<@<p<
<@<Q<j<n<r<v<z<~<
<@<x<
<@<z<
<@=p=
<@=p=w=
<@=V=o=u=
<\<d<l<t<|<
<]=b=
<^<h<
<_<c<g<k<o<s<w<{<
<_=i=
<_u2A;
<+<4<l<
<+<E<
<+=^=
<+=2=[=f=
<+=J=
<'<;<c<o<}<
<<<_<
<'<-<1<E<W<j<t<
<<<D<`<h<p<x<
<<<D<H<L<T<X<\<`<
<<<D<L<T<\<d<l<t<
<'<-<E<V<\<a<m<w<
<<<F<k<
<<<i<
<<<t<
<<=C=\=f=
<<=F=o=z=~>
<-<><[<
<<>W>
<'<1<8<I<N<q<
<'<2<8<><M<V<\<b<k<t<~<
<'<3<D<
<'<7<=<E<Z<f<
<-<D<[<r<
<-<n<
<'<q<B=k=
<-<t<
<-<T<y<
<-<V<a<x<
<'=`=
<=<D<W<u<
<=<H<v<}<
<=<Q<]<n<
<=<w<
<==G=\=q=
<==I=T=_=
<'=1=F=[=x=
<><}<
<><c<{<
<><E<]<
<><n<
<>=t=
<0<[=\>
<0<\<~<
<0<7<
<0<8<@<H<P<X<`<l<
<0<8<@<H<T<t<|<
<0<8<D<d<l<t<|<
<0<E<b<
<0<E<Z<{<
<0<J<z<
<0<n<
<0<S<
<0=:=I=f=
<0=@=L=T=
<0=4=8=<=@=D=
<0=h=
<0=I=
<0P0d0}0
<1<(=
<1<;<
<1<;<Y<y<[>X?y?
<1<=<I<U<}<
<1<=<I<U<t<
<1<><J<[<
<1<6<V<[<m<t<
<1<G<k<
<1<j<
<1<J<a<
<1<N<
<1<z<
<1=^=
<1=6=J=
<1=T=e=
<1=w=
<1>6>
<1Q1c1k1s1
<2<=<H<
<2<9<R<~<
<2<9<R<a<l<x<
<2<C<c<
<2<O<
<2<O<g<~<
<2<O<I=N=
<2<S<q<}<
<2=^=
<2=2>
<2=d=
<2=k=
<2=r=
<2>v>{>
<3<=<R<g<
<3<8<
<3<F<K<b<q<v<
<3<H<h<
<3<I<_<{<
<3<O<_<e<m<
<3<Q<]<i<x<
<3<R<q<
<3<V<g<
<3=b=
<3=G=P=
<3=Q=]=i=u=
<3=Q=e=q=}=
<4<;<T<e<
<4<<<@<D<L<P<T<X<
<4<<<D<L<X<x<
<4<P<v<
<4=_=
<4=B=
<4=l=
<4=m=r=
<5<:<
<5<{<
<5<|<
<5<><
<5<a<q<}<
<5<K<
<5<r<
<5=:=d=n=
<5=K=T=
<6<@<\<f<{<
<6<J<V<g<
<6<r<
<7<><W<l<
<7<A<Q<b<
<7<h<y<
<7<N<
<7=G=
<7=J=b=y=
<7=Z=q=
<8<@<L<l<t<|<
<8<@<L<l<x<
<8<{<
<8<D<d<l<t<
<8<G<
<8<H<T<\<
<8<H<T<t<|<
<8<z<
<9<?<T<Z<r<
<9<@<V<g<
<9<@<Y<
<9<_<
<9<C<X<l<
<9<D<c<
<9=T=
<9>I>
<a=j=
<assembly xmlns='urn:schemas-microsoft-com:asm.v1' manifestVersion='1.0'>
<b<l<
<B<n<t<
<B=_=
<B=I=b=s=
<B=L=a=v=
<B=T=g=|=
<B>`>j>
<Bu#j
<C<}<
<C=]=l=r=
<C=b=l=
<c=h=
<C=r=
<D<|<
<D<i<s<
<D<L<P<T<\<`<d<h<
<D<L<T<\<d<
<D<L<T<\<d<p<
<D=`=
<D=X=
<E<O<[<|<
<E<Q<n=u=
<E=j=t=
<e=r=
<F<v<
<F<y<
<F=i=
<F=S=_=p=
<F=y=
<F>A?
<G=T=
<G=W=N>i>
<H<W<
<H<z<
<H=$>
<i<6=q=
<I<l<
<J<a<
<J<f<s<
<J<l<
<j<q<
<j=(?o?
<J=N?
<juy7
<k=G?Q?
<K=T=
<L<q<
<L=a=s={=
<L=o=
<m<z<
<M=s=
<m=t={=
<M>d?
<N<]<b<t<
<N<u<
<n=u=
<n=x=
<n>y>
<O=W=
<p_fd> is less than 0.
<p_fd> less than 0.
<P<`<l<
<P<p<
<P<w<
<P<X<`<h<p<|<
<P<Z<
<P=m=
<parse error>
<Pujj
<Q<v<
<Q<z<
<Q=_=e={=
<R=Y=n=
<S<y<
<s=#>
<S=|>
<S=q=}=
<SVWj
<T<h<t<
<t<y<
<T=l=s=
<U+%.4X>
<u=.>r>
<U>_>
<uninitialized>
<V<`<x<
<V=[=v?
<W<\<
<W<k<
<W<u<
<X=j=
<X=k>)?
<xt <Xt
<xt"<Xu!
<xt><Xu=
<Y=x=
<zw:jaY;
= =$=(=,=0=4=8=<=@=D=H=L=P=T=X=\=`=d=h=l=p=t=x=|=
= =$=(=0=4=8=<=d=l=p=t=|=
= =%=5=:=J=O=^=c=r=w=
= =(=@=P=T=d=h=
= =(=0=<=\=d=l=t=
= =(=0=8=D=L=
= =(=4=T=\=h=
= =(=8=@=P=X=h=p=
= =,=3=<=B=M=S=Z=r=
= =,=L=X=`=x=
= =.=Q=j=
= =@=\=l=x=
= =@=H=P=X=d=
= =@=H=P=X=h=
= =@=H=T=t=|=
= =@=L=l=t=
= =@=P=t=|=
= =+=
= =0=@=P=`=p=
= =0=@=P=X=h=t=
= =9=H=S=_=p=
= =D=L=T=\=d=l=t=|=
= >*>G>d>
= >->=>X>2?
= >0>
= >9>Z>
= >e>j?o?
= >O>
= >O>v>
= >y?
=!=:=Q=j=
=!=:=s=
=!=;=
=!=?=V=
=!=-=9=a=
=!=>=
=!=5=;=U=
=!=5=>=y=
=!=5=A=M=Y=x=
=!=6=
=!=8=>=C=P=W=p=|=
=!=9=d=
=!=9=O=
=!=F=a=
=!=i=p=}=
=!=i=v=,>4>O>V>
=!=J=a=z=
=!=y=
=!=z=
=!>]>w>
=!>d>
=!>D>e>
="=(=B=H=b=h=
="='=x>
="=>=E=Z=o=
="=5=;=N=T=g=m=
="=5=@=a=t=
="=5=P=
=">)>A>
=">/>`>o>
=">+>4>=>
="><>
=">f>m>
=">O>z>
=">R>
=#=*=?=T=n=
=#=:=
=#===C=S=X=g=l={=
=#=1=c=s=z=
=#=6=
=#=A=M=Y=e=
=#=X=t=~=
=#=Y=k=v=
=#>`?o?
=#>`>
=#>-><>Y>
=#>>>W>
=#>->u>
=#>A>M>Y>e>
=#>C>
=#>M>
=#>R>s>
=$=,=4=@=`=l=
=$=,=4=<=D=\=d=l=
=$=,=4=<=D=L=T=\=d=l=t=|=
=$=,=4=<=D=L=T=\=d=l=x=
=$=,=4=<=D=L=T=\=d=p=
=$=,=4=<=D=L=T=\=h=
=$=,=4=<=D=L=T=\=t=|=
=$=,=4=<=D=L=T=`=
=$=,=4=<=D=L=X=|=
=$=,=4=<=D=P=p=x=
=$=,=4=<=D=P=X=x=
=$=,=4=<=H=h=t=
=$=,=8=@=`=|=
=$=,=8=\=d=l=t=|=
=$=,=8=X=`=h=p=|=
=$=,=8=X=`=h=p=x=
=$=>=E=\=m=r=
=$=0=
=$=0=P=\=|=
=$=2=E=K=
=$=5=F=W=h=y=
=$=9=N=l=
=$=C=
=$=D=L=T=\=d=l=x=
=$=D=L=X=x=
=$=D=P=p=|=
=$=K=}>|?
=$=s=
=$=u=
=$>,>4><>D>P>p>x>
=$>:>
=$>@>l>
=$>\>
=$>1>]>~>
=$>4>M>z>
=$>D>
=$>X>i>
=%=*=4=@=Q=
=%={=
=%=5=<=I=T=Y=_=h=n=r=
=%=6=O=h=
=%=6=Z=
=%=7=>=K=W=h=
=%=8=
=%=8=^=e=x=
=%=A=H=a=r=
=%=g=
=%>/>H>Y>v>
=%>/>W>b>
=%>}>
=%>Y>i>u>
=&=0=<=M=
=&=2=>=J=V=b=n=z=
=&=k=p=
=&=L=
=&=L=W=
=&=X=
=&>->N>g>
=&>H>
=&>i>
=&>K>U>
=(=,=<=H=X=h=x=
=(=.=3=L=e=
=(=.=6=K=W=k=
=(=_=
=(=0=@=d=l=t=|=
=(=0=@=H=X=`=p=x=
=(=0=<=\=d=l=t=|=
=(=0=<=\=d=p=
=(=0=4=8=@=D=H=L=t=|=
=(=4=E=
=(=7=
=(=8=D=
=(=9=U=l=*>1>X>
=(=f=l=r=
=(=H=
=(=H=h=
=(=H=P=X=`=h=p=
=(=L=T=\=d=l=t=|=
=(=o=
=(>^>r>
=)=;=\=w=
=)=@=
=)=0=I=Z=
=)=5=A=M=l=
=)=a=
=)=t=
=)>T>~>&?p?
=)>Y>
=*=?=D=
=*=`=
=*=6=;=P=U=Z=q=v=
=*=8=y=
=*=A=j=
=*=A=X=o=
=*=C=a=m=y=
=*>[>
=*>5>E>g>q>
=*>K>
=*>w>
=,?Z?
=,=<=H=h=t=
=,=2=D=O=U=Z=n=u=
=,=4=@=d=l=t=|=
=,=4=<=D=L=T=\=d=l=t=|=
=,=4=<=D=L=T=\=d=l=x=
=,=4=<=D=L=T=`=
=,=4=<=D=L=T=d=l=t=|=
=,=4=<=X=`=h=p=x=
=,=6=J=P=T=h=z=
=,=7=
=,=8=X=`=h=t=
=,=C=Z=
=,=F=U=n=
=,=G=j=
=,=H=X=d=l=
=,>4>T>\>d>l>t>|>
=,>d>
=.=:=M=]=v=
=.=>=b=r=
=.=4=A=O=T=d=i=u=
=.=8=J=j=
=.=C=]=y?
=.=F=M=f=
=.=P=
=.=z=
=.>]>c>u>{>
=.>o>
=/=:=
=/=?=^=h=
=/=D=^=
=/>9>E>Q>p>
=/>9>V>t>~>
=/>f>
=/>Q>
=:?R?d?
=:=g=
=:=J=P=X=m=y=
=:=N=h=w=
=:=N=Z=k=
=:>/?
=;=\=u=
=;=e=
=;>O>
=;F(r
=?=_=
=?=}=
=?=d=|=
=?=J=U=
=?=Q=s=
=?=w=
=?>D>
=?>k>
=?>Q>f>
=?>y>
=@=c=
=@=z=
=@>s?
=[=e=}=
=[=v=
=\?q?
=]>j>
=_=i=
=_>k>v>
=`=m=
=`=z=
=+=<=M=^=o=
=+=o=v=
=+=W=^=w=
=+>?>X>g>
=+>2>D>Y>v>
=+>B>X>n>
=+>k>
=+>N>
=<=\=d=l=t=|=
=<=D=L=T=\=d=p=
=<=D=L=T=\=h=
=<=D=L=X=x=
=<=H=h=t=
=<=J=T=
=<=t=
=<>P>\>h>t>
=<>T>[>
=<Jr?]s
='=,=A=G=L=a=f=k=
='=,=L=Q=g=s=
='=.=J=Q=j={=
='=/=l=
=-=:=F=W=
='=\=v=
='===N=c>
===m=
===M=S=[=p=|=
===s=
===T>
===W=\=|=
==> Context: 
==>Q>w>
='=1=V=x=
='=5=q=
='=k=
='=L=_=x=
=-=P=a=}=
=-=S=`=
=->:>
=>=`=
=>=H=\=b=f=z=
=>>_>
=>>d>
=>>T>
=>>u>
='>B>L>e>
='>D>d>
='>D>p>w>
='>r>
=->s>
='>t>{>
='>T>Y>k>r>
=0?V?
=0=8=@=H=P=\=|=
=0=8=<=@=H=L=P=T=|=
=0=8=D=d=p=
=0=n=
=0>=>l>s>
=0>5>d>n>
=0>7>O>z>
=0B0f2
=1?T?{?
=1===I=U=}=
=1===I=U=t=
=1===I=X=x=
=1===N=
=1=C=
=1=E=Y=e=q=
=1=j=
=1=L=
=1=x=
=1=Z=q=
=1>{>
=1>k>
=1>L>^>d>}>
=1>L>f>
=1>x?
=2=<=Q=f=
=2===k=
=2=B=
=2=E=J=[=n=s=
=2><>T>m>
=2>f>
=2>l>
=2>t>
=3=K=T=
=3=l=u=
=3=V=
=3=V=g=
=3=X=g=
=3>@>
=3>p>|>
=3>Q>]>i>u>
=3>Q>e>q>}>
=3>Q>e>y>
=3>Y>d>
=4=<=D=L=T=\=d=l=t=|=
=4=<=H=h=p=x=
=4=9=u=
=4=A=S=z=
=4=D=P=X=x=
=4=J=
=4=P=]=i=z=
=4><>$?L?
=4><>l>x>
=4>l>
=5=:=
=5>e>
=5>L>m>
=5>T>^>w>~>
=6=I=N=^=c=v={=
=6>v>
=7=J=c=~=
=7>>>W>
=8=@=D=H=P=T=X=\=
=8=@=H=P=X=d=
=8=@=H=P=X=d=l=
=8=_=
=8=G=
=8=I={=
=8=T=\=d=l=t=
=8>{>
=8>m>
=8>T>[>t>
=8>X>
=9=C=O=
=9=D=c=
=9=F=
=9=I=O=W=l=x=
=9>{>
=9>+?
=9>r>
=9>v>
=a>n>
=AMDiu!
=B=c=
=B=H=N=f=
=B=X=h=v=
=b>>?z?
=c=j=
=C=l=
=C=u=
=C>a>m>y>
=C>h>
=C>i>t>
=C>J>
=C>r>
=D?T?h?|?
=D=[=g=x=
=d={=
=D=|=
=D=J=P=h=
=D=M=
=D>L>T>\>d>l>
=E=Y=b=
=E>O>a>
=F=a=
=F=M=T=\=
=F>N>
=F>Y>l>
=G=t=
=G>w>|>
=Genuu
=H=h=p=x=
=H>\>|>
=H>3?
=H>V>y>
=i?p?u?
=I=n=
=I=O=d=j=
=I>a>
=I>f>
=J=O=
=J=u=
=J>^>
=J>4?F?
=J>b>z>
=J>T>i>~>
=k=u=
=K>X>
=L=|=
=L=g=
=L=Z=q=
=M=t=
=M>a>}>
=N>#?-?F?P?k?u?
=O>\>
=o>z>
=P=}=
=p>}>
=P>h>
=P>s>
=q>v>,?P?
=R=d=
=R=h=
=R=h=y=
=R=m=w=
=R=w>
=r>4?
=r>C?n?u?
=S=~=
=S=Z=
=s>|>
=T=y=
=T>g>
=T>r>y>
=U>0?{?
=V={=
=V=q=
=V=x=
=V>`>x>
=W>a>v>
=W>g>1?q?
=X=b=w=
=X=j=v=
=X>b>h>
=Y=_=e=}=
> ?'?@?w?~?
> ?@?`?
> ?B?|?
> ?X?
> ?Z?
> [R8
> >$>(>,>0>4>8><>@>D>H>L>P>T>X>\>`>d>h>l>p>t>x>|>
> >$>(>P>X>\>`>h>l>p>t>
> >$>,>0>4>8>`>h>l>p>x>|>
> >$><>@>X>h>l>p>t>x>
> >(>0><>\>h>
> >(>4>T>\>d>l>t>
> >)>A>I>P>Y>o>|>
> >,><>L>T>d>x>
> >,>L>T>\>d>l>t>|>
> >,>L>X>x>
> >@>H>P>\>|>
> >@>H>P>X>`>h>p>|>
> >@>H>T>t>
> >@>H>T>t>|>
> >@>H>X>|>
> >@>L>l>t>|>
> >'>?>I>b>s>
> >0>@>H>X>\>l>
> >0>8>H>P>`>h>p>
> >5>
> >D>L>T>\>d>l>t>|>
> >D>L>T>d>t>
> >H>
> >H>P>T>X>`>d>h>l>
> >s>,?
>!?*?
>!?@?~?
>!?7?a?
>!?E?o?
>!>%>)>->1>5>R>
>!>&>F>K>Y>f>|>
>!>:>Q>
>!>;>
>!>>>
>!>5>A>M>Y>
>!>6>
>!>f>
>!>F>a>
>!>s>
>"?(?I?n?
>"?'?2?
>"?R?
>">(>B>H>b>h>
>">.>
>">.>:>F>R>^>j>v>
>">5>t>
>">9>>>P>W>p>
>">j>w>
>">K>V>
>">l>
>">U>
>#?A?\?
>#?A?M?Y?e?
>#?j?o?w?
>#>*>7>C>T>
>#>,><>A>U>b>n>
>#>@>]>z>
>#>3>9>A>V>b>|>
>#>3>m>
>#>A>M>Y>e>
>#>M>
>#>q>
>$?;?G?z?
>$?7?U?c?
>$?T?
>$>(>,>0>X>`>d>h>p>t>x>|>
>$>(>,>4>8><>@>h>p>t>x>
>$>,>4>@>d>l>t>|>
>$>,>4><>D>L>T>\>d>l>t>
>$>,>4><>D>L>T>\>d>l>t>|>
>$>,>4><>D>L>T>\>d>l>x>
>$>,>4><>D>L>T>\>h>
>$>,>4><>D>L>T>`>
>$>,>4><>D>L>t>|>
>$>,>4><>D>L>X>|>
>$>,>4><>D>P>p>x>
>$>,>4><>H>l>
>$>,>4><>H>l>t>|>
>$>,>4>l>t>|>
>$>,>8>X>d>
>$>=>]>q>z>
>$>0>8>X>t>
>$>0>P>\>|>
>$>0>P>X>`>h>x>
>$>0>T>\>d>l>t>|>
>$>4>:>B>W>c>}>
>$>5>v>
>$>D>L>X>x>
>$>D>P>X>x>
>$>J>U>
>$>P>}>
>$>s>
>$>X>h>t>
>%?E?
>%?f?
>%?n?
>%>+>H>M>{>
>%>0>N>a>z>
>%>1>=>I>h>
>%>D>
>%>M>
>&???
>&?-?F?
>&?8?D?U?
>&?g?
>&?H?
>&?J?|?
>&>?>K>
>&><>P>V>
>&><>t>
>&>=>Y>w>
>&>->F>p>w>
>(?:?[?x?
>(?-?
>(?^?
>(?`?
>(?8?\?l?
>(?j?
>(>.>2><>c>s>y>
>(>/><>D>J>T>^>h>l>
>(>:>
>(>:>n>
>(><>B>F>Z>l>
>(>->=>B>Y>h>
>(>0>@>D>T>X>h>l>|>
>(>0>@>H>X>`>p>x>
>(>0><>\>h>
>(>4>T>\>d>p>
>(>6>S>
>(>7>
>(>9>C>
>(>9>s>
>(>H>P>\>|>
>(>H>T>t>|>
>(>L>T>\>d>l>t>|>
>(>P>
>(>P>t>
>(>Q>[>m>
>(>s>
>)?S?X?h?m?
>)?w?
>)>.>>>C>O>t>
>)>0>E>Z>
>)>4>
>)>4>V>
>)>5>]>
>)>5>T>
>)>E>
>)>F>
>)>H>
>)>Z>n>
>*?5?@?K?
>*?5?=?E?M?U?
>*?a?
>*?A?M?^?
>*?A?z?
>*?m?
>*?r?
>*>/>9>E>S>
>*>;>L>]>n>
>*>_>s>
>*>1>J>[>x>
>*>A>j>
>*>A>j>{>
>*>H>b>x>
>*>q>
>,?9?
>,?d?
>,>:>T>_>
>,><>A>O>T>b>g>u>z>
>,><>D>L>T>\>d>l>|>
>,>=>
>,>4><>D>`>h>p>x>
>,>4><>D>L>X>x>
>,>4><>H>h>p>|>
>,>8>@>`>|>
>,>8>X>d>
>,>f>m>
>,>L>i>c?
>,>W>y>
>.?5?
>.?h?
>.?k?
>.?w?
>.?W?
>.>?>E>R>\>h>y>
>.>^>r>
>.>3>V>o>
>.>C>h>
>.>k>N?
>.>M>f>
>.>U>|>
>.>v>}>
>/?@?J?Q?u?
>/?<?H?Y?
>/?9?K?k?
>/?E?[?f?
>/?i?
>/?Q?
>/?T?Y?p?u?
>/?z?
>/>B>[>o>t>
>/>E>^>j>
>/>H>a>z>
>/>K>
>:?H?q?~?
>:?Q?
>:>f>
>:>Q>j>
>:>Q>z>
>:>V>
>;?B?T?i?
>;?G?k?
>??f?
>??w?
>-?]?
>'?m?
>-?M?
>-?n?
>'?z?
>@?x?
>@>i>
>@>x>
>@s5f
>[>b>w>
>\?f?~?
>]?t?
>_?i?
>_>I?r?
>`lBPe
>+?0?]?g?
>+?5?H?f?p?
>+?5?J?_?y?
>+?m?
>+?n?
>+>?>_>
>+>>>W>p>
>+>E?
>+>L>X>u>
>+>r>
><?N?
><>_?
><>C>\>
><>D>L>T>\>d>l>t>|>
><>D>L>T>\>d>l>x>
><>D>L>T>\>h>
><>H>h>p>|>
><>H>h>t>
><>t>
>=?{?
>=>_?
>>?b?
>>?q?
>'>=>
>'>>>U>l>
>>>a>
>>>g>x>
>>>O>m>
>'>2>
>->3>M>
>'>4>]>
>'>4>e>l>
>'>8>[>z>
>'>B>
>->B>a>w>
>'>c>
>'>F>
>->I>t>
>'>L>
>->u>
>0?=?n?}?
>0?f?
>0?K?U?j?
>0>5>L>Q>^>c>r>w>
>0>O>t>
>0m]Z
>1?>?o?~?
>1?Q?
>1>8>=>
>1>C>K>S>
>1>D>q>
>1>E>h>z>
>1>J>a>
>1>U>\>u>
>1>v>
>1>Z>q>
>2?]?j?
>2?=?H?
>2?t?y?
>2?u?
>2>8>O>U>k>q>
>2>B>f>v>
>2>C>
>2>D>N>j>
>2>p>w>
>3?g?
>3?Q?]?i?u?
>3?s?
>3>n>
>3>W>k>
>4?=?
>4?l?
>4>`>o>
>4><>D>L>T>\>d>l>t>|>
>4><>D>L>T>\>d>p>
>4>8>X>`>d>
>4>B>
>4>I>P>h>
>4>U>o>t>
>5?:?Y?^?n?s?
>5?d?j?}?
>5?Y?o?
>5><>Q>f>
>6?|?
>6?t?
>6?y?
>6>W>
>7?b?u?
>7>H>N>[>e>q>
>7>t>~>
>8?G?
>8>@>H>P>X>`>l>
>8>@>H>P>X>h>
>8>@>H>T>t>|>
>8>@>L>l>x>
>8>[>
>8>b>
>8>G>
>8>L>T>t>
>8>r>y>
>8>T>d>p>x>
>9?C?_?
>9>D>c>
>A?{?
>a?{?
>a?p?
>A?Q?
>A?s?
>A>Q>]>i>
>A>R>u>
>Anu 
>b?k?
>b?r?
>b>{>
>B>d>
>B>g>q>
>b>l>
>B>S>o>
>c?h?
>C?H?
>C>d>
>C>M>e>{>=?G?_?u?
>C>M>t>y>
>D?_?
>D?K?d?u?
>d?p?
>D?R?e?k?
>D>\>f>
>D>K>R>Z>
>E>j>t>
>E>N>
>E>X>
>F?a?
>F?a?n?
>F?P?c?
>f?s?
>F>k>
>F>M>f>
>G>[>x>
>G>f>m>
>H?U?
>H>\>d>i>
>H>e>
>h>u>
>H>X>d>l>
>H>X>r>
>http://www.microsoft.com/pki/certs/MicRooCerAut_2010-06-23.crt0
>i?p?
>I?q?
>I?S?h?}?
>I>b>{>
>I>S>h>}>
>J?d?
>J?S?[?
>K>_>
>K>U>m>
>L?T?
>L>\>h>p>
>L>b>j>r>
>L>k>
>L>q>
>L>s>
>l>s>
>M>a>r>
>M>b>g>
>M>h>w>
>N>|>
>N>X>|>
>NGdx
>o>|>
>O>g>o>
>O>v>
>P?d?
>P?n?x?
>p?z?
>P>T>X>\>`>d>h>l>p>t>x>|>
>PWPW
>Q?[?s?
>Q?b?
>Q?e?
>Q?n?z?
>Q>X>m>
>S>~>
>S>j>
>S>q>}>
>S>w>
>U?m?t?
>U?t?{?
>V=IXb
>V>]>j>v>
>v>^?
>V>l>
>V>n>
>X?g?
>X>m>
>Y?o?
>Y>c>
>Y>d>n>w>
>Y>o>
>Z?{?
>Z>P?Z?o?
>Z>q>
0 <= p && p_int < shape_proto->dim_size()
0 == center_point_box_ || 1 == center_point_box_
0 == memory_size % kMinAllocationSize
0 0 06070>0?0
0 0$0
0 0$0(0,0004080<0@0D0H0L0P0T0X0\0`0d0h0l0p0t0x0|0
0 0$0(0004080<0d0l0p0t0|0
0 0$0,040
0 0&0>0T0n0
0 0(0,00040\0d0h0l0t0x0|0
0 0(0@0L0_0i0}0
0 0(000@0d0l0t0|0
0 0(00080@0H0P0`0h0
0 0(00080@0L0l0t0|0
0 0(00080@0P0t0|0
0 0(040T0\0h0
0 0(040T0`0
0 0)0K0R0g0
0 0*0O0l0
0 0,0<0P0`0p0
0 0,040h0x0
0 0,0L0T0`0
0 0@0\0l0x0
0 0@0H0P0\0|0
0 0@0L0l0t0|0
0 000@0P0`0p0
0 00040L0P0T0X0\0`0h0l0p0t0x0
0 00070<0?0
0 040`0
0 050R0
0 0D0L0T0\0d0
0 0D0L0T0\0d0l0t0|0
0 1%1?1E1[1`1w1~1
0 1*1>1K1b1y1
0 1f1
0 1R1o1
0 1X1
0!0)080:0
0!0)080;0
0!0@0Q0W0\0h0r0|0
0!0@0Y0v0
0!040:0M0_0e0z0
0!090d0
0!0E0
0!0E0u0
0!0f0
0!0K0z0
0!0S0]0i0u0
0!0z0
0!0Z0q0
0!1(1=1R1s1
0!1;1P1c1~1
0!141
0!1i1
0!1V1j1r1
0"0*0C0W0_0
0"0-0v0}0
0"040A0M0[0
0"080F0P0h0~0
0"0A0S0
0"0J0
0"0R0f0
0"1)1A1q1
0"1,1Z1k1
0"1`1d1h1l1p1t1x1|1
0"1+1M1Z1}1
0"1E1r1
0"1j1
0"1U3o3
0"1V1j1r1&2W2q2
0"2=2P2d2
0#0(070<0K0P0`0e0u0z0
0#0=0B0T0
0#0-0H0R0k0r0
0#040Q0
0#060
0#0c0
0#0G0h0
0#101b1
0#191&222U2
0#1A1M1Y1e1
0#1C1
0#1F1W1s1
0#1I1k1
0#1I1T1s1
0#1J152
0#1x1
0#2m2
0$0*0
0$0,040@0`0h0p0|0
0$0,040@0`0h0p0x0
0$0,040@0`0h0t0
0$0,040@0`0l0t0
0$0,040@0d0l0t0|0
0$0,040<0D0L0T0\0d0l0t0|0
0$0,040<0D0L0T0\0h0
0$0,040<0D0L0X0|0
0$0,040<0H0P0p0x0
0$0,080X0`0p0
0$0,080X0d0
0$0.0:0K0
0$000P0\0|0
0$000P0X0d0
0$03080G0L0[0`0t0
0$0A0
0$0C0a0m0y0
0$0q0
0$0R0j0u0
0$0s0
0$0X0h0t0|0
0$1?1
0$1\1
0$1a1
0$1f1
0$1m1
0%0*0=0B0]0b0w0|0
0%0:0O0r0
0%0>0O0
0%010=0I0h0
0%020:0@0J0T0^0b0
0%0D0
0%1{1
0%1f1
0%1O1
0%1O1q1
0&0,0D0U0[0`0l0v0
0&0:0`0t0
0&0<0v0
0&0-0
0&0X0
0&1,1B1H1`1q1w1|1
0&1>1H1a1
0&1A1f1
0&1a1w1
0&1d1
0&1F1a1n1
0&1H1
0&1p1
0&2C2a2
0&2l3q3
0(0,0@0D0X0\0`0h0
0(0@0O0V0
0(0^0
0(0<0l0
0(0=0R0o0
0(000@0H0P0X0`0h0p0x0
0(000<0\0h0
0(0004080@0D0H0L0t0|0
0(00080H0l0t0|0
0(0-0B0H0M0c0h0y0
0(040@0_0
0(080\0d0l0t0|0
0(080H0X0h0x0
0(0H0P0\0|0
0(0H0P0X0`0h0p0x0
0(0H0P0X0`0l0
0(0H0T0t0
0(0H0T0t0|0
0(0H0X0|0
0(0L0T0\0d0l0t0|0
0(1`1
0(1>1X1
0(10181@1H1T1\1
0(1-1
0(171
0(1a1
0(1F1 2b2
0(1j1
0)0.0:0F0W0
0)01090F0N0V0s0{0
0)030K0Q0g0
0)040S0q0}0
0)050]0
0)0q0
0)1;1m2
0)1r1
0)1Y1
0*0?0R0^0o0
0*0@0S0_0l0y0~0
0*0@0Z0v0
0*00080M0Y0r0x0|0
0*030b0x0
0*060P0l0|0
0*090Z0
0*0G0
0*0I0[0
0*0k0
0*0s0
0*1}1l2
0*141I1^1z1
0*1Q1
0*1s2
0*4c7
0*4p8a:
0,0<0L0T0d0l0|0
0,0=0C0H0\0i0u0
0,040@0`0h0p0x0
0,040<0D0L0T0\0h0
0,040<0D0l0t0|0
0,060E0e0
0,080X0h0
0,090A0I0
0,0N0X0l0r0v0
0,1|1
0,1d1
0,1D1K1
0,1H1h1p1x1
0,1j1-2
0,1s1
0.0:0K0
0.0?0[0r0
0.0?0E0R0\0f0p0z0
0.040J0P0e0k0}0
0.0D0
0.0f <= ratio_value && ratio_value < 1.0f
0.0q0
0.1o1
0.1Q1_1
0.3@3R3
0/0;0m0
0/0@0F0
0/0}0
0/050:0F0[0j0}0
0/0B0G0S0e0j0
0/0m0
0/0P0s0
0/1Q1z1
0/1r1
0:0}0
0:0J0n0~0
-0;0M0S0X0v0~0
0;0td
0;0u=
0;1E1
0;1S1]1{1
0;1W1a1
0?1y1
0@0n0
0@0S0l0
0@0x0
0@2W2a2w2
0[0e0
0[0Y1
0[1k1
0]0s0
0]0u0|0
0]1a2
0^071]1
0^0h0
0_^[]
0_0d0i0
0_0t0
0_1x1
0~0d1
0+0<0
0+000B0O0[0i0
0+070Q0m0}0
0+080D0U0
0+1{1
0+171C1O1n1
0+1B1~1
0+1F1
0<0<0A0
0<0D0L0T0\0d0l0t0
0<0D0L0T0\0d0l0t0|0
0<0h0
0<0H0h0p0|0
0<0H0h0t0
0<0X0m0
0<1f1
0<1l1
0<1v1
0<1y1
'0=0|0
0=0=0
0=0G0l0
0=0m0
0=1$2
0=1J1
0=1U1_1t1
'0>0/1F1
0>0e0
0>0E0^0o0
0>1z1
0'0/0
00;0N0Y0
0'0@0Y0r0
000`0}0
000`0t0
000<0\0h0
00000
00000=0=0
00080<0@0H0L0P0T0|0
00080D0d0l0t0
00080L0T0h0p0x0
000D0X0p1
001<1H1V1s1
001D1t1
001J1
001u1#2f2
0'020?0K0
0'080U0k0|0
0'090~0
0-0G0h0
0'0n0
0-0S0
0-0W0
0-0Z0^1
0-1^1
0-1=1]1
010@0
'010`0k0y0
010=0I0U0t0
01050;0;0
01050;0<0A0
010A0Q0a0q0
010E0Q0]0i0
010j0
011;1a1q1
011M1h1
011R1
0123456789-
0123456789-+Ee
0123456789ABCDEFabcdef-+Xx
0123456789ABCDEFabcdef-+XxPp
0123456789abcdefghijklmnopqrstuvwxyz
014;4T4e4
0'141e1t1
0'1D1I1
0'1f1
0-1g1
0-1k1
0-1K1e1
0-1o1
0-1s1
02080R0X0r0x0
02090R0
020X0
02171
021F1v1
021r1
021S1d1l1
0272`2k2
030q0
030Q0e0q0}0
031`1e1w1~1
031c1
031S1
031v1
032Q2]2i2u2
040@0
040@0L0t0
040<0D0L0T0\0d0l0t0
040<0H0h0t0
040<0H0l0t0|0
040<0T0l0
040<0X0`0h0p0x0
040904E4
040d0
040D0P0X0x0
040g0z0
040I0c0
040X0`0h0t0
041;1T1
041[1
041]1
041`1
041>1S1h1
041l1
041n1
041N1{1
041q1
050H0a0z0
050I0O0g0q0
051h2
051l1v1
051Y1
060;0H0
060=0b0
06041
060G0
060J0`0
060l0
060l0|0
060Y0
061`1b2
061N1l2
061w1
070n0{0(121D1c1u1
070P0`0k0
071>1g1r1
071>1S1
071D1u1
080D0
080F0b0
080G0
080h0
080K0n0{0
080L0r0
080p0
080r0y0
080s0
080T0d0p0x0
080U0
081E1v1
081G1X1v1
081H1T1x1
081r1
081W1
090C0}0
090D0c0
090y0
091i1s1
091s1
09T$$
09T$P
0a0f0k0
0A0l0r0
0A0O0]0k0
0A0V0>1}1
0A0X0
0A1&4E4l4.5E5
0A1a1
0B0G0
0B0M0
0B1`1
0B1U1
0C0_0i0
0c0m0
0C1|1
0c1~1
0c1=2M2
0C1a1m1y1
0C1a1u1
0D0|0
0d0l0
0D0X0
0D1.2W2G3
0D1t1
0E012;2S2l2w4v5
0E0-1m1
0E0l0
0E0O0d0y0
0E1;2I2g2
0E1|1
0e1q1
0e1r1
0e1s1
0E1z1
0F0n0O1w1
0f0s0
0F1V1d1
'0G0]0
0g0{0
0G0q0'1w1
0G0Q0c0
0g0t0
0G1^1i1t1
0G1p1
0G1T1`1q1
0-g-o-p-
0h ?H
0h,QH
0h`5H
0H0X0d0l0
0H1e1
0H1h1p1x1
0ht+H
0hXGH
0J0^0u0
0J0~0
0J0a0z0
0J0l0u0A1]1
0J0O0T0
0J0S0i0
0J0z0
0J1i1w1
0j1q1
0J1V1s1
0K0\0m0
0K0c0{0
0K0i0v0
0K0o0
0K0R0d0y0
0K1Y1y1
0L0\0h0p0
0L0\0u0
0L0S0f0
0L1Q1
0l1q1x3}3
0M0,2:2Y2m2w2
0m1z1u2
0N0_0
0O0{0
0O0c0r0z0
0O0T0
0O1a1v1
0O2n2x2
0P0\0h0v0
0Q0l0
0Q0t0
0Q0u0
0r0L1Y1e1
0R1[1
0S0\0l0u0
0S0~0
0S0q0}0
0S1]1r1
0S1y1
0T0|0
0T1o1v1
0U1b1
0V0[0`0
'0V0[0i0n0~0
0V0{0
0V0u0|0
0w0|0
0W1>2-3
0W1a1y1
0X0_0w0
0X0x0
0X1x1
0Y0p0
-0Z0}0
1 == capability.nodes.size()
1 1$1(1,1$7(7,707
1 1$1(1,1014181<1@1D1H1L1P1T1X1\1`1d1h1l1p1t1x1|1
1 1$1(1,1014181<1@1D1H1L1P1T1X1\1`1d1h1u1
1 1$1(1,10181P1`1d1t1x1|1
1 1$1(1P1X1\1`1h1l1p1t1
1 1$1,1014181`1h1l1p1x1|1
1 1$1<1@1X1\1`1d1x1|1
1 1(101@1d1l1t1|1
1 1(101<1\1d1l1t1|1
1 1(10181D1d1l1t1|1
1 1(10181D1d1l1x1
1 1(141T1\1d1p1
1 1(141T1\1h1
1 1(181@1P1X1h1p1
1 1,161B1S1
1 1,1L1T1`1
1 1:1V1f1l1t1
1 10141D1P1`1p1
1 1d1
1 1D1L1T1\1d1l1t1|1
1 1H1P1T1X1`1d1h1l1
1 1O1
1 1X2
1 2(242W2l2z2
1 2)2H2O2Q2_2
1 2_2
1 202t2
1 2P2
1 2U2
1 2Y2
1 2Y2c2{2
1!1/191O1U1Y1m1
1!1;1W1g1m1u1
1!1+1B1Y1d1m1r1
1!1'1?1W1o1
1!11191A1|1
1!111A1Q1a1q1
1!1-191a1
1!1-191E1d1
1!1-191H1h1
1!1-1L1
1!12181=1T1c1h1z1
1!151A1M1Y1x1
1!1J1a1
1!1J1d1
1!1N1
1!2(2M2
1!2[2
1!2=2U2o2
1!2>2~2
1!2-2Z2~2
1!252>203n3
1!2D2
1!2s2
1"1'1<1A1U1Z1f1x1}1
1"151`1q1
1"151Q1[1p1
1"171L1i1
1"1g1
1"1L1S1l1
1"2(2.2F2q2
1"2)2B2n2u2
1"212
1"2'2,242
1#1(1-1D1I1Z1d1o1
1#1(1D1I1e1k1~1
1#1=1
1#1'1+1/13171;1?1C1G1K1O1S1W1[1_1c1g1k1o1s1w1{1
1#181=1W1\1v1{1
1#1A1U1a1m1y1
1#1H1
1#1Q1
1#1s1
1#2_2i2u2
1#202k2v4
1#222
1#2c2
1#2D2
1#2I2T2s2
1#3o3
1$1(1,101X1`1d1h1p1t1x1|1
1$1,1@1H1P1X1l1t1|1
1$1,141@1`1h1p1x1
1$1,141<1D1\1d1l1
1$1,141<1D1L1T1\1d1l1t1
1$1,141<1D1L1T1\1d1l1t1|1
1$1,141<1D1L1T1\1d1p1
1$1,141<1D1L1T1\1d1t1|1
1$1,141<1D1L1T1\1h1
1$1,141<1D1L1T1\1h1p1
1$1,141<1D1L1T1`1
1$1,141<1D1L1X1|1
1$1,141<1D1L1X1x1
1$1,141<1D1P1t1|1
1$1,141<1H1
1$1,141<1H1l1t1|1
1$1,141<1T1\1d1p1
1$1,141y1
1$1,181\1d1l1t1|1
1$1,181t1
1$1,181X1`1h1t1
1$1,181X1`1h1x1
1$1,181X1d1
1$1,1D1h1x1
1$1.181P1]1i1z1
1$1<1D1L1T1\1d1l1t1|1
1$10181\1d1t1|1
1$10181X1
1$10181X1t1
1$101P1X1d1
1$151S1
1$181\1d1l1t1|1
1$191K1R1j1
1$1D1P1p1|1
1$1n1
1$1R1
1$1s1
1$1S1
1$1s1<2R2c2i2
1$2,20242<2@2D2H2p2x2|2
1$2\2
1$2`2
1$2-2<2l2
1$24383<3@3D3H3L3P3
1$2g2l2q2y263q3
1$2I2V2o2
1$2n2
1$2P2|2
1$2Q2v2
1%1*1<1I1U1f1
1%1/1
1%181Q1j1x1
1%1A1p1u1
1%2,2S2^2
1%282 3
1%2s2
1%2t2
1&1{1
1&101I1
1&1C1U1g1
1&1g1q1
1&1V1
1&2>2H2a2
1&202C2^2
1&243a3
1&2I2l2
1&2T2k2w2
1(0&0
1(1=1R1s1
1(10181@1H1P1X1h1
1(141T1\1h1
1(141T1`1
1(171J1_1l1x1
1(181\1d1l1t1|1
1(181H1X1h1x1
1(181x1
1(191z1
1(1H1P1\1|1
1(1H1T1t1|1
1(1L1\1h1p1
1(1M1T1[1a1
1(1O1]1
1(2<2D2L2T2\2d2l2x2
1(252A2R2
1(272<3I3
1(2y2
1)1.1B1O1f1~1
1)1/1<1C1H1_1n1s1
1)1?1a1h1
1)1?1U1k1
1)1\1
1)151]1
1)151T1
1)171w1
1)1B1
1)1E1t1~1
1)1F1
1)2|2
1)2i2
1)2O3x3
1)2T2|2
1)2Z2{2
1*1A1
1*1n1
1*2;2
1*2d2
1*2D2q2
1*2F2u2z2
1*2o2
1*2Q2
1*2S2q2}2
1*444c4n4|4
1,1@1
1,1`1e1
1,1=1C1H1T1j1o1
1,101@1L1\1l1t1
1,141:1D1N1X1\1}1
1,141@1`1h1p1x1
1,171
1,181@1`1h1p1x1
1,1B1l1
1,1D1L1d1|1
1,1S1
1,2d2
1.1?1g1
1.1>1q1
1.10.0
1.10.220126-2359.1.dml-1.8.89dd732
1.181L1R1V1j1|1
1.1A1Z1p1}1
1.1b1
1.1G1}1
1.2_2x2I3_3q3|3
1.2w2
1/0-0
1/1_1
1/111
1/1F1
1/1g1
1/2l2
1/2P2
1:2N2~2
1;1`1
1;2[2
1;2J2O2a2v2
1?1l1
1?2a2
1?2L2
1?2Q2f2
1?2X2
1?2Y2
1?2Z2
1?4L4
1@1H1L1P1X1\1`1d1
1@1P1V1^1s1
1@2}2
1@2J2
1[2`2
1[2u2
1\1c1
1\1d1~1
1\1t1
1\2Y3
1]1b1
1]2z2
1^2A3l3.4
1_2d2W3d3
1`1p1
1`2~2
1{375
1}1C2i2
1+1d1
1+1H1
1+1j1
1+1w1
1+2?2Z2v2
1+2+353H3h3
1+2a2
1+2i2
1+2S2
1<1D1L1T1\1d1l1t1|1
1<1D1L1T1`1
1<1F1Y1i1
1<1h1
1<1t1
1<2B2P6
1<2F2o2z2
1<2M2U2d2u2
1<2P2X2x2j3
1<2Q2c2k2s2
1=1E1K1r1
1=1L1
1=1Q1i1
1=1W1:2l2
1=1X1g1
1=1Z1`1
1=2\2
1=2^2
1=2N2
1>1E1^1
1>1E1P1\1
1>1n1
1>1N1r1
1>2V2
101\1|1
10171\1y1
10181@1H1P1\1|1
10181@1H1P1X1d1
10181@1H1X1|1
10181=1Q1V1[1q1v1
101E1b1
101P1\1|1
101X1
102<2H2V2|2
102h2
102J2
102m2
102x2
1-1#2-2@2[2
1'1.1G1X1
1'1?1E1\1v1
1'101r1
111=1I1U1t1
111019184142Z
111B1H1
111C1c1
111D1]1v1|1
111E1Y1m1
111G1
111j1
1'1-1o1
112k2
112Q2
1'131D1
1'1A1H1a1r1
1'1D1
1-1J1
1-1S1
1-1w1
1-2}2
1-2<2
12181R1X1v1
121B1N1p1
121D1O1U1_1o1t1
121d2
121F1h1z1
121H1q1
121x1
122~2
1-222
122C2t2
122r2
1-272L2a2
1'2D2l2
1-2V2
1'2V2c2x2
1'3>3
131H1]1r1
132\2h2t2
132=2R2g2
132I2
132Q2]2i2u2
132Y2d2
141<1D1L1T1\1d1l1t1|1
141<1D1L1T1\1d1l1x1
141<1H1h1t1
141D1P1p1|1
141D1P1X1p1
141E1K1P1\1f1p1|1
141l1
141M1
141n1u1
141p1w1
141Y1
142d2
142z2
1494c4
151^1v1
151a1q1}1
151d1j1|1
151s1y1
151U1
152I2U2a2r2
153:3S5q5
161@1O1o1
161<1S1Y1j1o1
161Q1
161Q1a1n1
162E2\2
162f2
171A1
171B1
171e1
171G1g1
171Q2_2l2t2|2\3
171U1{1
172[2
172[2i3
172^2e2
172}2
172<2A2
172q2
172w2
181?1F1Y1u1
181@1H1P1X1h1
181@1H1T1t1|1
181@1L1T1t1
181e1
181G1a1t1
181J1
181J3`3p3
181T1d1p1x1
181X1b1{1
181y1
181Y1m1v1h2
182~2
182D2
182G2X2v2
183G3
191D1c1
191D1l1
191M1]4
191N1U1\1o1
191S1l1~1
192Q2t2
192x2
193@3T3n3v3
196C6X6m6
1A1e1o1
1A1M1X1c1o1z1
1A1N1q1
1A1U1h1t1
1A1U1x1
1A2e2
1A2J2
1A2P2U2g2n2{2
1B1I1a1
1b2l2
1B2l2x2
1C1c1j1
1C1H1]1l1q1{1
1C1M1d1
1C2a2
1C2g2
1c2r2
1C2r2
1D input tensor
1D output tensor
1-D tensor of 2 elements: [crop_height, crop_width]. All cropped image patches are resized to this size. Both crop_height and crop_width need to be positive.
1-D tensor of axes that `starts` and `ends` apply to.
1-D tensor of ending indices (exclusive) of corresponding axis in axes
1-D tensor of floats
1-D tensor of shape (num_rois,) with each element denoting the index of the corresponding image in the batch.
1-D tensor of starting indices of corresponding axis in `axes`
1D1|1
1D1i1
1D2q2
1D2t2
1e1}1
1E3L3q3
1f1o1
1f1p1
1F2O2
1G1M1b1h1
1H1d1
1H1j1
1H1O1T1X1\1`1
1H2B3R3
1H2q2z2
1H2R2d2
1h8R?
1hD@D
1hi#7
1I1Y1w1
1I2a2
1J1[1
1J1a1z1
1J1c1s1~1
1j2q2
1k2u2
1l1(2H2R2w2N3
1L1`1z1
1L1k1u1
1L3)5
1m132P2{2
1M1r1|1A2j2
1M2w2
1m3t3{3
1M3T3m3w3
1N1h1
1n1s1
1N2~2
1N2d2
1N2X2
1O1c1l1
1O1t1
1O1z1
1o2{2
1P1a1
1P2Z2r2
1p3{3
1Q1]1
1Q3,4I4S4x4-575I5^5x5
1R1[1
1R1r1y1
1R1Y1q1
1R2\2t2
1R4"7
1S1y1
1S1Z1j1
1s2}2
1S2x2
1S3C6
1S3i3
1t1~1
1Ta`-,
1U1z1
1V2m2
1v2z3
1v3N4m4B7
1W2r2
1Y1c1
1Y1c1x1
1Y2^2
1Y2|2
1Y2f2
1Y2q2{2
1Z1q1
2 2$2(2,2024282<2@2
2 2$2(2,2024282<2@2D2H2L2P2T2X2\2`2d2h2l2p2t2x2|2
2 2$282<2L2P2`2p2t2
2 2(202@2d2l2t2|2
2 2(20282@2L2l2t2|2
2 2(20282D2d2l2t2|2
2 2(20282H2l2t2|2
2 2(242T2\2h2
2 2(282@2H2P2`2h2p2
2 2,2=2y2
2 2,2f2m2t2
2 2,2L2T2\2h2
2 2,2L2T2`2
2 2@2\2l2x2
2 202@2P2`2p2
2 20282H2X2h2x2
2 252{2~3
2 2D2L2T2\2d2l2t2|2
2 2D2p2
2 2L2
2 3(3,30383<3@3D3l3t3x3|3
2 3^3
2 303=3D3Q3X3e3u3
2 343Z3n3
2 3B3
2 3D3L3T3\3d3l3t3|3
2 3P3
2 3X3
2 4-4
2!2.2{2
2!2:2
2!2:2K2
2!2:2K2Q2^2h2z2
2!2@2J2Y2y2
2!2]2
2!2=2B2e2~2
2!212A2Q2a2q2
2!2-292E2d2
2!252I2]2n2
2!282O2j2
2!2F2a2
2!2l2
2!3}3
2!3H3u3
2!3l3
2!4`4
2"2)2@2Y2g2}2T5
2"2/2;2L2
2"2?2Z2m2y2
2"2_2|2
2"2=2^3
2"24263H3S4`4
2"2a2
2"2F2X2j2|2
2"2K2V2
2"3@3J3
2"3_3
2"3>3E3^3o3
2"333O3k3
2"3-383
2"3g3
2"3Z3
2"pb'
2#2(242>2J2[2
2#2:2b2
2#2@2
2#2'2;2M2`2j2~2
2#2'2+2/23272;2?2C2G2K2O2S2W2[2_2c2g2k2o2s2w2{2
2#282p2
2#292
2#2H2f2p2
2#2H2Z2z2
2#2Q2
2#2T2c2
2#3`3t3|3
2#3-3?3_374
2#3A3M3Y3e3
2#3A3M3Y3h3
2#3B3
2#3c3
2#3G3o3{3
2#598
2$2)2:2E2K2P2g2w2
2$2*2u2
2$2,2`2h2p2x2
2$2,242@2
2$2,242@2d2l2t2|2
2$2,242@2H2h2
2$2,242<2D2
2$2,242<2D2|2
2$2,242<2D2L2T2\2d2l2t2|2
2$2,242<2D2L2T2\2d2p2
2$2,242<2D2L2T2`2
2$2,242<2D2P2p2x2
2$2,242<2D2P2t2|2
2$2,242<2H2h2p2x2
2$2,242L2T2\2t2|2
2$2,282@2h2p2x2
2$2@2P2V2^2s2
2$2<2`2p2|2
2$20282X2t2
2$202P2X2`2h2p2
2$202P2X2`2l2
2$202P2X2d2
2$202P2X2h2
2$202P2X2h2x2
2$242D2H2X2h2l2p2t2x2
2$282@2H2P2X2`2h2p2x2
2$2D2L2T2\2h2
2$2D2L2X2|2
2$2D2L2X2x2
2$2V2`2x2
2$3.3
2$383R3f3
2$3G3j3
2$3V3
2%2)2=2R2q2
2%2:2O2q2
2%2+2B2V2\2o2y2~2
2%2-2
2%262<2w2
2%262x4
2%2A2Q2W2_2t2
2%2B2N2Z2f2r2
2%2J2]2v2
2%3\3r3
2%3=3X3]3
2%3r3
2%3Y3
2&2.242>2H2R2V2w2
2&2.2M2
2&2;2N2Z2y2
2&2@2
2&2@2c2}2
2&2>2V2n2
2&212r2'313U3a3l3
2&2M2X2
2&2O2b2|2
2&3A3
2&3T3d3h3l3p3t3x3
2&3U3
2&4f4
2(2/2U2[2`2l2v2
2(2:2_2u2
2(2<2S2j2
2(202@2H2X2`2p2
2(20282X2|2
2(202P2l2|2
2(202P2l2t2|2
2(202P2X2x2
2(222<2\2a2k2w2
2(242B2
2(242T2\2d2l2x2
2(242T2`2o2
2(252K2c2t2z2
2(282C2d2w2
2(282D2d2p2
2(282E3a3h3}3
2(282H2X2h2x2
2(292f2p2
2(2D2[2v3
2(2G3
2(2H2P2\2|2
2(2H2P2X2`2p2
2(2L2T2\2d2l2t2|2
2(3<3R3
2(323E3`3
2(3b5g5
2(3T3l3x3
2(4-4Q4V4
2)2.2=2B2Q2V2j2z2
2)2:2v2
2)252T2
2)292{2
2)292D2e2x2
2)2B293C3\3m3
2)2F2a2q2~2
2)2p2}2
2)2T2"3
2)3C3V3l3
2)3X3}3
2*2:2F2h2x2
2*2;2w2
2*262G2
2*272S2]2v2
2*2A2e2
2*2C2H2R2q2
2*2D2
2*2F2]2x3
2*2G2P2P2`2
2*343I3^3{3
2*3H3f3F4#5
2,2?2
2,212E2J2i2n2|2
2,212F2K2
2,22272C2X2e2z2
2,242<2D2L2T2\2d2l2t2|2
2,242<2D2L2T2\2d2p2
2,242<2D2P2p2|2
2,242<2D2P2X2|2
2,242<2H2l2t2|2
2,24282<2D2H2L2P2x2
2,272
2,282@2`2|2
2,2H2W2
2,3[3a3s3z3
2,3d3
2,4I4h4
2.24292E2Z2i2|2
2.2Y2
2.3;3
2.304
2/2C2h2z2
2/2J2
2/3^3d3v3}3
2/3y3
2:2Q2j2
2:3{3
2:3|3
2:3P3`3n3
2:3Q3
2:4?4
2;3R3m3
2;4e4
2?2]2r2o3v3
2?3I3a3w3
2?3k3
2?3L3}3
2?3Q3
2?3Q3f3
2?3T3
2?3x4
2@2E2Y2f2r2
2@2T2Z2r2|2
2@2x2
2@3`3l3
2@3E3f5
2[3v3
2\2p2y2
2\3o3
2]2g2|2
2]2t2
2^2h2t2
2^2r2
2^3m4
2_2d2|2
2`2~2`
2`2q2
2+2 4*4B4[4
2+2?2S2g2{2
2+2{2
2+2<2u2
2+21252?2f2v2|2
2+252<2C2N2U2_2r2
2+252H2k2y2~2
2+353Y3f3j3n3
2+393Y3
2+3P3
2+3X3~3
2<2D2H2L2T2X2\2`2
2<2D2L2T2\2d2l2t2|2
2<2D2L2T2`2
2<2H2
2<2t2
2<3_5
2<3M3u3
2<3P3j3
2<3Q3a3i3q3
2<SH3
2=2h2z2
2=3J3}3
2=4T4X5o5!6>8U8
2>2H2w2
2>2n223b3
2>2p2~2
2>2x2
2>3K3)414V4|4
2>3x3
2>3Y3
202`2h2
202=2T2k2
20220506181423Z
20220506220735.059Z0
20220507181423Z0w0=
20262;2G2Y2`2m2y2
20282@2L2l2t2|2
20282D2d2l2x2
202L2T2\2d2l2x2
202o2
202U2e2
202v2
20383@3H3P3\3d3
203L3f3m3
20A0U0u0
210930182225Z
212:2G2Q2]2i2~2
212:2K2
212=2I2U2}2
212=2I2U2t2
21262H2S2Y2c2~2
212A2M2Y2x2
212E2Q2]2i2
212E2Q2b2
212E2Y2e2q2}2
212G2d2
212L2h2
212O2d2
212R2
212S2q2F3K3
212z2
2'2.2G2Q2q2x2
2-2[2
2-2>2G2M2
220302185121Z
220310192419Z
222A2Y2
222F2
222I2
222W2s2}2
223{3
223f3
223r3
223Y3
2'282T2k2
2'2B2R2
2'2C2g2
2-2J2
2-2x2)3Q3p3
230308192419Z0p1
230511185121Z0
232^2
232770+4695750
23282t2
232C2W2}2
232L2W2f2
232m2
232Q2]2i2u2
232Q2e2q2}2
233p3
233p3|3
233Q3]3i3u3
233Q3e3y3
233s3
233Y3d3
242@2l2t2
242~2
242<2@2D2L2P2T2X2
242<2D2L2T2\2d2l2t2|2
242<2H2h2p2x2
242<2V2^2
242P2`2l2t2
242T2\2d2l2t2|2
24393{3
243l3
248W8
252^2z2
252Q2h2
253l3v3
253m3
253N3
253v3
261019185142Z0
262`2
262J2`2t2z2
262J2h2w2
262l2
263@3]3
263=3D3
273D3P3a3
273M3
273P3
273P3T3X3\3p3t3
273q3
273t3
279U9_9
282?2X2
282@2H2T2t2|2
282@2L2l2t2|2
282=2\2a2
282c2u2
282k2
282p2
283G3X3v3
292K2U2l2|3
292t2
293V3d3
2A2Q2]2i2
2A2w2
2A3a3
2A3I5g5n5
2A3r3
2b2G3]3
2B2H2Z2a2z2
2B2Z2
2b3}3
2B3d4A5
2B3P3
2b3t3
2B3V3r3
2B4h4]5
2C2_2;3Z3}3
2C2n2t2
2C2R2
2C2s2
2C2u2
2c2v2
2C3^3/5_5)?7?
2c334
2C3c3
2C3I3b3g3
2c3l3
2C3O3u3z3
2C3r3
2C3S4C5
2D matrix with shape (K,N)
2D matrix with shape (M,N)
2D2^2
2D2L2P2T2\2`2d2h2
2d2n2
2D2P2t2|2
2D2s2y2
2D3\3c3
2D3I3
2-dimensional sparse matrix A. Either COO or CSR format
2E2K2]2d2}2
2E2o2@3
2E3h3
2e3y3
2F2h2
2F2s2
2F3.434
2F3c3
2F3P3x3
2F4v4
2G2i2
2G3Q3
2H2L2P2T2X2\2
2H2P2\2|2
2H2T2k2
2H3w5
2I2f3s3
2I2S2
2i3p3
2I3V3
2J2Q2i2
2J3e3
2K3T3
2L2|2
2L2f2
2L3|3
2L3g3q3
2L4j4
2M2e2l2
2M3R3
2N2b2k2
2N2f2p2
2O2{2
2o2}2
2O2t2
2o3<4Y4r4
2O3T3
2P2Z2
2p3q4z5
2p3U4n4
2p4u4
2Q3r3%4C4]4
2Q4l4v4
2R2d2
2S2d3
2S2y2
2S3q3}3
2V2{2
2V3[3
2V3\3x3}3
2V3y3
2W2\2g2
2W2g2
2W3a3
2W4\4
2Z2_2
2Z3q3
3 3$3(3,3034383<3@3D3H3L3P3T3X3\3`3d3h3l3p3t3x3|3
3 3$3(3,3034383<3@3H3L3T3X3`3d3l3p3x3|3
3 3$3(3,3T3\3`3d3l3p3t3x3
3 3$3(3034383<3d3l3p3t3|3
3 3$3L3T3X3\3d3h3l3p3
3 3(3,30343\3d3h3l3t3x3|3
3 3(3.383B3L3P3q3y3
3 3(30383H3l3t3|3
3 3(343T3\3d3l3x3
3 3(343T3\3p3
3 3(383\3d3l3t3|3
3 3*343>3H3c3t3z3
3 3,343T3p3
3 3,383D3l3
3 3,3L3T3\3d3l3t3
3 3,3L3T3`3
3 3.393I3P3`3g3w3~3
3 3:3T3n3
3 3@3H3h3p3
3 3@3H3P3\3|3
3 303@3H3P3`3h3x3
3 30343D3H3L3d3h3
3 30343D3H3X3h3x3
3 353H3d3n3
3 383<3@3T3d3h3
3 383v3
3 3A3T3m3
3 4%4]4
3 4A4U4
3 4G4v4
3!3(353=3
3!313A3Q3a3q3
3!323O3Y3u3
3!333
3!3e3
3!3f3
3!3j3
3!3J3a3
3!3T3^3v3
3!4.4:4K4
3!4;4I4
3!4B4L4p4{4
3!4D4g4
3!4g4
3!4m4y5
3!4S4
3!4s4
3!4S4v4
3"3&3*3.32363:3G3
3"3,3@3F3J3^3p3
3"3.3>3C3b3g3
3"323>3`3p3|3
3"343R3
3"363
3"373L3i3
3"373V3r3
3"393P3g3~3
3"3a3
3"3c3
3"3j3~3
3"4,4A4V4
3"4\4
3"464K4`4
3"4h4
3"4k4
3#3?3[3b3w3
3#3_3i3u3
3#3<3
3#3=3B3Z3j3{3
3#323E3J3k3p3
3#3-3
3#3-373A3K3]3j3v3
3#343:3z3
3#343u3
3#3B3]3w3
3#4*4C4o4v4
3#4\4s4
3#434W4
3#4n4
3#5W5
3$3,343@3`3l3
3$3,343@3`3p3
3$3,343<3D3`3
3$3,343<3D3L3T3\3d3l3t3
3$3,343<3D3L3T3\3d3l3t3|3
3$3,343<3D3L3T3\3d3l3x3
3$3,343<3D3L3T3\3d3p3
3$3,343<3D3L3T3`3
3$3,343<3d3l3t3|3
3$3,343<3D3L3X3|3
3$3,343<3D3L3X3x3
3$3,343<3D3P3p3x3
3$3,343<3D3P3t3|3
3$3,343<3D3P3X3x3
3$3,343<3H3h3x3
3$3,343<3P3X3`3h3|3
3$3,343L3T3\3d3l3t3|3
3$3,383X3`3l3
3$3,383X3d3
3$3,3D3L3T3\3h3
3$3,3D3T3p3
3$3,3L3h3x3
3$3:3?3V3[3r3w3
3$3@3d3
3$303P3\3|3
3$303P3X3`3h3p3|3
3$303P3X3`3l3
3$303P3X3h3
3$323|3
3$343D3L3\3h3x3
3$3C3a3u3
3$3D3L3T3`3
3$3e3
3$3L3T3
3$3x3
3$4\4
3$6+6@6U6s6
3%3+303<3F3R3c3
3%3=3B3N3g3w3
3%3B3M3e3j3x3
3%3d3
3%3D3
3%3D3N3a3
3%3U3i3x3
3%3Y3u3
3%4,4G4a4
3%4;4K4b4
3%4N4X4l4
3%4U4
3&3,3D3Z3a3w3}3
3&3?3I3d3n3
3&3`3g3
3&373=3}3
3&3C3a3u3
3&3L3
3&3p3
3&4:4a4
3&4}5
3&414i4p4
3&4-4:4F4W4
3&4A4
3&4c4
3&4D4
3&4k4u4
3&4s4
3&5v5
3(3,3<3H3X3h3x3
3(3/3E3K3`3f3
3(3/3H3a3
3(3:3i3}3
3(3[3o3
3(3034383@3D3H3L3t3|3
3(30383@3H3X3h3p3
3(303P3X3`3|3
3(303X3`3h3p3
3(383H3X3h3x3
3(3H3P3X3`3h3t3
3(3H3P3X3`3l3
3(3H3P3X3d3
3(3H3X3|3
3(3L3T3\3d3l3t3|3
3(4`4
3(424E4`4w4
3(4B4\4v4
3)3/3@3E3U3Z3s3
3)3:3y3
3)3@3
3)3}3
3)3>3[3
3)333>3S3X3k3}3
3)353]3
3)3B3
3)3F3
3)3k3r3
3)3p3}3
3)3Q3
3)3s3
3)3X3g3
3)4D4
3)4s4
3)4T4Y4f4
3)4v4
3)4Y4
3*3:3@3H3]3i3
3*343>3H3R3\3c3s3z3
3*3A3j3
3*3j3
3*3N3~3
3*464K4
3*494
3*4A4j4
3*4r4
3,343<3D3L3T3\3d3l3t3|3
3,353k3
3,383Q3W3[3e3
3,3X3
3,444<4D4L4X4`4
3,4l4
3,4V4
3.3?3|3
3.3@3e4
3.363k3
3.3g3
3.3x3
3.4|4
3.4p4
3.5v5
3/3@3R3V3Z3^3b3f3j3n3r3v3z3~3
3/3A3_3q3
3/3I3
3/3N3X3h3
3/3N3y3
3/454;4S4~4
3/4Q4
3/5m5
3:3\3
3:3C3P3b3g3}3
3:3D3i3
3:3O3u3
3:4W4
3;3E3j3
3;3G3S3_3~3
3;3H4M4h4m4
3;4~4
3;4D4
3;4T4
3?3I3[3x3
3?3S3
3?3w3
3?4L4
3@3L3X3f3
3@3Z3
3@4l4v4
3[3o3x3
3[4I5
3\3f3{3
3\4f4y4
3\4m5
3\4t4{4
3\5q5
3]4d4
3^3h3t3
3^3h3z3
3_4h4
3_Gn=
3`4q4
3{4#5^5
3|4x5
3+3;3A3I3^3j3
3+3@3T3
3+313D3a3{3
3+3b3g3
3+3G3^3
3+3J3u3
3+4N4q4
3<3D3L3T3\3d3l3t3|3
3<3F3Y3i3
3<3J3\3k3
3<3t3
3<4`4
3<4~4
3<4B5^5h5}5
3<4l4
3<4L4f4
3<4u4z4
3<4y435
3=3C3[3a3p3|3
3=3H4v4
3=4J4
3=4p4
3>3c3p3
3>3p3
3>4a4}4
3>4x4
3>4Z4u4
30=0R0g0
300930183225Z0|1
303<3\3h3
30353F3P3[3f3w3|3
30383T3r3
303H3X3v3
303X3
30444H4L4X4\4`4l4p4
304h4
304J4O6l8u8
30Q0]0i0u0
30Q0e0q0}0
31=L=V=o=
313=3I3U3t3
313=3I3X3x3
313>3n3
31373F3M3\3a3p3|3
313A3U3i3}3
313Q3q3
313z3
314}4
314Q4v4
323^3~3
323<3N3S3a3l3
323F3W3
323F3Z3n3
324N4
324r4
32-bit hash value.
3'3,3C3T3Z3_3k3u3
3'3.3G3Y3q3x3
3-3:3Q3h3
3'3<3Q3n3
3'303:3D3
333]3q3z3
333=3W3^3u3
3'3-32383C3I3T3[3`3i3w3|3
333b3
333D3J3W3a3k3u3
333f3
333H3e3
333Q3e3q3}3
333T3Y3
334=4b4
334C4{4
334p4|4
334Q4]4i4u4
334Q4e4q4}4
334v4
3'3A3
3-3C3J3b3w3}3
3'3h3x3
3-3K3
3'3R3
3'3W3k3
3-4?4
343;3S3i3p3
343;3T3e3k3p3|3
343@3`3h3p3x3
343<3`3p3|3
343<3D3L3T3\3d3l3
343D3P3X3x3
343H3\3
343K3
343R3
344j4
344l4
344Y4q4
3'454Y4
3'464
3-4A4q4
3-4A5
3'4H4
3-4i4=5
3-4s4
3'4S4m4
3-4v4
3-4w4
354)5R5
354B4
354j4
354x4
355?5
363F3Y3v3
363N3e3~3
364S4
364U4q4
373_3m3
373h3
373K3
373K3q3
373R3Y3n3
374D4p4
383@3H3P3X3`3h3t3
383@3H3T3t3|3
383@3L3l3t3
383[3
383\3|3
383I3f3
383T3\3d3l3t3
384a4/5Z5`5v5|5
384H4a4
393>3V3[3p3
393d3
393u3
394O4
395\5_6
3A3'4j4
3A4[4
3a4f4y7
3A4N4
3B3M3
3b4{4
3b4m4x4
3C3a3m3y3
3c4~4`5
3C435
3c485
3C4a4m4y4
3C4d4
3C4g4
3c4R6
3C6w6
3D3|3
3D3g3
3D3S3
3D4I4
3d8o8|8
3E4_4
3E4b4
3E4J4
3F3,4
3F3i3n3
3f3m3
3F4D:h>G?
3F4M4f4p4
3G3n3
3G3Q3
3g8}8
3h3o3v3}3
3h4,515y5
3H4m4
3h4o4
3H4o4x4
3H4o5
3h4ZF
3http://www.microsoft.com/pkiops/Docs/Repository.htm0
3I4i4
3J4$7`879
3J4i4s4
3jpZjoY
3K3g3t3
3K3P3U3
3K4]4
3L3T3\3t3
3L4_5
3M3^3
3M3j3p3
3N5[5
3O3`3m3
3O4:5~5
3O5[6
3P3\3
3p3}3
3p4w4
3P4Z4o4
3p57638
3q3v3c4
3Q4#5
3R3f3
3S3y3
3s425
3S4q4}4
3S5 6
3T3~3%4u4
3U3Z3
3U4G5
3v4I5S5h5}5
3W3i3a4n4
3WVVQ
3y4i5
3Z3q3
3Z4c4l4
4 4$4(4,4044484<4@4D4H4L4P4
4 4$4(4,4044484<4@4D4H4L4P4T4X4
4 4$4(4,4044484<4@4D4H4L4P4T4X4\4`4d4h4l4p4t4x4|4
4 4$4(4<4@4P4T4X4p4t4
4 4$4(4P4X4\4`4h4l4p4t4
4 4$4,40484<4@4D4H4L4P4T4X4\4`4d4h4l4p4t4x4|4
4 4$444@4P4`4h4x4
4 4$444@4P4X4h4x4
4 4(404<4\4h4p4
4 4(40484@4H4P4`4h4x4
4 4,4L4T4\4d4p4
4 4/444C4H4W4\4j4v4
4 4@4H4T4t4|4
4 4@4L4l4x4
4 4+4<4G4X4`4u4
4 404@4P4`4h4x4|4
4 424E4O4c4i4m4
4 454R4
4 494J4g4
4 4D4L4T4\4d4l4t4|4
4 4H4P4T4X4`4d4h4l4
4 4H4P4X4`4t4|4
4 5(50585@5L5T5
4 5*575t5
4 5,585I5g5
4 505<5D5d5
4 5D5L5T5\5d5l5t5|5
4 5P5
4 5V5
4 5x5
4!4&454:4F4[4`4z4
4!4.4{4
4!4:4Q4o4
4!414A4Q4a4q4
4!444:4O4y4
4!444Q4k4q4
4!4-494a4
4!4-494E4d4
4!4-494E4m4
4!454I4U4a4m4
4!464
4!4b4
4!4J4a4z4
4!4J4U4
4!5.6S6
4!6f6
4"4(4-4>4I4O4T4e4p4v4{4
4"4:4P4~5
4"4;4A4\4b4x4~4
4"414=4J4P4V4\4
4"4'474<4J4V4[4q4v4
4"4'4A4F4d4i4w4
4"474L4i4
4"4c4
4"4f4
4"4F4x4
4"5,5
4"5,5D5J5b5y5
4"5B5
4"5c5
4"5C5a5m5y5
4"5H5R5g5{5
4"5R5
4#4;4S4k4
4#4?4V4
4#444E4J4a4l4
4#464<4U4f4l4q4{4
4#4q4x4
4#5*5C5
4#5\5s5
4#5`5l5x5
4#575g5{5
4#5A5M5Y5h5
4#5A5U5i5u5
4#5N5c5
4#5U5s5
4$4(4,404X4`4d4h4p4t4x4|4
4$4*424G4S4m4
4$4,4`4h4p4x4
4$4,444@4`4h4p4|4
4$4,444<4D4L4T4\4d4l4t4|4
4$4,444<4D4L4T4\4d4l4x4
4$4,444<4D4L4T4\4d4p4
4$4,444<4D4L4T4\4h4
4$4,444<4D4L4X4`4
4$4,444<4D4L4X4x4
4$4,444<4D4P4p4|4
4$4,444<4D4P4p4x4
4$4,444<4H4h4p4|4
4$4,484@4`4|4
4$4,484\4d4l4t4|4
4$4,4D4L4d4l4x4
4$4=4\4
4$4=4B4{4
4$4=4N4s4
4$40484\4d4l4t4|4
4$40484X4`4h4p4
4$404P4\4|4
4$404P4X4`4h4p4x4
4$404P4X4d4
4$404T4\4d4
4$404T4\4d4l4t4|4
4$44484<4@4D4H4P4X4`4d4h4|4
4$444D4L4T4\4d4l4x4
4$4c4
4$4D4P4p4|4
4$4G4Q4v4
4$4h4
4$4s4
4$4X4f4
4$5,50545<5@5D5H5p5x5|5
4$5\5
4$5B5[5|5
4$5T5m5
4%4:4O4l4
4%4:4O4q4o5
4%4+434H4T4n4
4%464P4
4%4M4
4%4N4]4
4%4P4r4
4%565
4&4?4O4Z4{4
4&424C4
4&43484O4`4f4k4w4
4&434G4[4o4
4&444u4
4&4B4L4
4&4D4[4y4
4&4y4
4&5~5
4&5P5i5
4&7+7f7w7
4&7-7v7
4(4/4H4
4(4?4[4
4(4<4b4
4(40484@4L4l4x4
4(40484D4d4l4t4
4(40484H4l4
4(404P4l4|4
4(414D4J4\4b4z4
4(444S4
4(464\4
4(474
4(484H4X4h4x4
4(4B4^4n4t4|4
4(4H4P4\4|4
4(4H4T4t4|4
4(4H4X4|4
4(4l4
4(4L4\4h4p4
4(4L4T4\4d4l4t4|4
4(4s4
4(5`5
4(575
4(595a5u5
4(5c5
4(5g5
4(5q5
4(5r5
4(5v5
4)444S4q4
4)454T4
4)4F4
4)4F4}4
4)4k4r4
4)4S4X4
4)525p5
4)535b5s5
4)535X5u5
4)555F5
4)5C5]5p5t5x5|5
4)5D5w6}7
4)5U5k5
4*4[4j4
4*43494
4*444M4T4p4z4
4*464X4h4t4
4*5&606E6Z6w6
4*5>5J5[5
4*5O5g5w5
4*5r5
4,4`4g4z4
4,414E4W4\4l4~4
4,424G4^4e4|4
4,444<4D4L4T4\4d4l4t4|4
4,444<4D4L4T4h4p4x4
4,4A4^4$7.7C7X7
4,5)606I6
4,5;5
4,565K5`5}5
4,5d5
4,5R5\5o5
4.4C4c4
4.4C4P4n4x4
4.4C4V4r4|4
4.4Y4{4
4.5;5G5X5
4/4;4
4/4;4[4g4
4/454=4R4^4w4}4
4/494A4G4\4
4/4p4
4/4v4
4/545
4/545q5
4:5}5:6n6
4:5F5R5^5
4:5t5
4;;0}
4;4k4
4;4x4
4;5_5
4;5B5T5i5
4;5E5b5v5
4;5M5
4;5T5d5o5
4;6@6
4?4_4
4?4f4
4?5I5_5i5
4?5Q5f5
4@4|4
4@4H4L4P4X4\4`4d4
4@4L4X4f4
4@5l5
4@6\6c6x6
4^4 5j5B6
4^6c6|7
4_5h5
4_5y5
4`4m4
4|5g7
4+4;4u4
4+4@6N6o6
4+434?4X4_4
4+464
4+4B4
4+4C4Y4_4s4y4
4+4E4_4i4
4+505
4+5H5c5
4+5T5^5q5
4<4C4[4
4<4d4
4<4D4L4T4\4d4l4t4
4<4D4L4T4\4d4l4t4|4
4<4D4P4p4x4
4<4D4T4\4l4t4|4
4<4H4h4p4x4
4<4H4h4t4
4<4S4_4p4
4<4t4
4=4F4Z4
4=4G4l4
4=4L4
4=4Y4n4
4=5a5
4=5B5
4=5c5y5
4>4|4
4>5X5r5
40;0P0e0
404@4L4T4t4
404<4\4h4
40484
40484@4L4l4x4
40484@4P4t4
40484<4@4H4L4P4T4|4
404A4f4
404E4f4p4
404I4Y4d4
404Q4V4[4p4x4}4
404T4d4p4
40575P5Z5
405q5
405u5
405v5
405w5
40R0a0y0
40W0O3s3
414;4P4e4
414?4Z4f4
414@4E4W4^4k4w4
414>8/;
414C4K4S4
414E4Q4]4i4
414k4
414Q4q4
415F5K5c5r5
415k5
416t8
424<4O4o4y4
42475O5Y5r5
424R4Y4r4
424W4
425r5
425W6^6s6
43494N4U4l4}4
434A4X4f4}4
434b4
434b4h4{4
434D4d4
434H4f4
434l4q4
434m4
434Q4
434x4
435E5X5
435o5y5
435Q5]5i5u5
435Q5e5y5
435s5
435Y5d5
4-4;4
4'414P4W4p4
4-424A4F4V4[4o4|4
444<4D4L4T4\4d4l4t4|4
444<4D4L4T4`4
444<4H4h4p4|4
444<4H4h4p4x4
444P4X4`4h4p4|4
444Q4
444X4
445l5
446Q6o6
4'4q4
4-4T4
4-4U4
4'5.5G5k5r5
454@4K4d4
454@4K4V4l4w4
454H4m4z4
455w5
4-5H5W5
4'5X5
464a4
464G4M4R4^4h4t4
464K4Q4l4
464M4d4{4
464v4
465Q5
465Z5d5
4686<6@6D6H6L6P6T6X6\6`6d6h6l6`<h<x<
474A4f4~5
474a4x5
475D5u5
484@4D4H4P4T4X4\4
484@4H4P4`4
484@4H4P4X4d4
484_4d5v5k6}6r7
484D4d4l4t4|4
484g4
484T4d4p4x4
484W4v4
485E5
485t5
486V6o6
494k4s4
494M4h4w4
495S6
495V5i7
495z5
4A4|4
4A4F4
4A4I5p6
4A5a5
4A5O5
4B4m4
4b4t4
4B5`5
4B5G5L5
4B5J5]5d5
4b5l5
4b6A7m8
4B7L7_7
4C4q4
4c4q485E5
4C4u4
4c5~5
4C5a5m5y5
4C5g5
4C5J5_5t5
4D mask in attention cpu kernel is not supported
4-D tensor of shape (N, C, H, W), where N is the batch size, C is the numbers of channels, H and W are the height and width of the input data.
4-D tensor of shape (N, C, H_out, W_out).
4D4|4
4D4L4T4\4
4D4x4
4d5}5
4D5S5b5s5{5
4E4t4z4
4E5|5
4E5J5
4E7O7h7z7
4F4a4
4F4p4
4F5b5l5
4f5C6#7
4F5S5
4F5W5
4F5W5h5r5,6=6N6X6
4G4^4~4
4H4\4h4y4
4H4t4
4H4T4e4
4H4T4k4
4H4W4
4H4Y4
4h5r5
4H5W5
4H627
4hDAD
4i4}4
4I4n4
4i4n4
4J4^4t4
4J5e5
4K4R4
4K4U4
4K5U5n5
4L4j4
4L4V4i4y4
4L5j5
4L5S5m5
4L6w7
4m;z;
4M4Y4
4M5n5
4M8c8y8Y9m9
4N495>5
4n4M6S:
4o4t4
4O5a5v5
4O5q5
4P4\4h4v4
4P4^4
4P4_4
4P4U4
4p4w4
4p4z4
4Q5j5
4Q5q5
4Q5z5
4R5]5h5
4R5Q7
4r5y5
4r6y6
4S4y4
4S5@6
4s6/?
4S6q6
4t4$5"7
4T4r4y4
4T5r5
4U4q4
4U5\5q5
4U5u5
4V4j4s4h5
4V4q4
4V4q4{4
4x+L](
4X4j4
4X4Z5
4X5g5x5
4X8]8
4Y5[:|;w<
4Y5d5
4Y5f5
4Y5k5W6i6q7
4Y6c6
4Y6s6
4Z4q4
4Z4t4
4Z5d5y5
5 5$5(5,50545@5D5H5L5P5T5X5\5`5d5h5l5p5t5x5|5
5 5$5(5,50545<5@5D5H5L5T5l5|5
5 5$5(5,5054585<5@5D5H5L5P5T5X5\5`5d5h5l5p5t5x5|5
5 5$5(5,545L5P5h5l5
5 5$5,5054585`5h5l5p5x5|5
5 5(50585@5H5T5t5|5
5 5(545<5p5
5 5(585\5d5l5t5|5
5 5(5H5P5p5x5
5 5)5A5U5
5 5,5L5X5x5
5 5@5L5l5x5
5 5{5
5 505@5d5l5t5|5
5 505<5D5x5
5 505<5L5\5l5|5
5 50585H5P5`5h5x5
5 585Q5
5 5D5L5T5\5d5l5t5|5
5 5D5L5T5\5l5t5|5
5 5F5
5 6(6,60686<6@6D6l6t6x6|6
5 6h6r6
5 6j6
5 6X6
5!5]5
5!515>5
5!515A5Q5a5q5
5!535:5G5S5d5
5!555A5M5Y5
5!565w586B6W6l6
5!585P5f5m5
5!5y5
5!5Z5q5
5!6,7Z7
5!6\6
5!656e6y6
5!676K6q6
5!7a7
5!7P7z7
5!8!0-g-
5"5.5P5`5l5
5"565J5Z5j5z5
5"5C5N5|5
5"6,6A6V6s6
5"6.6?6
5"666X6g6
5"6d6
5"6m6
5"6O6|6
5"6Q6W6j6p6
5"6s6
5#5)5.5:5D5N5X5b5l5v5
5#5*5
5#5*515d5
5#5*5A5Y5q5
5#5;5f5
5#5_5
5#505<5M5
5#52575K5[5t5
5#545u5
5#555<5I5U5f5
5#5H5R5d5
5#6;6E6Z6o6
5#6\6s6
5#6A6M6Y6e6
5#6A6q6
5#6c6
5#6I6T6s6
5#7o7
5$5(5,54585<5@5h5p5t5x5
5$5)5f5[6z6
5$5*515
5$5,545@5d5l5t5|5
5$5,545@5H5h5
5$5,545<5
5$5,545<5D5L5T5\5d5l5t5
5$5,545<5D5L5T5\5d5l5t5|5
5$5,545<5D5L5T5\5d5l5x5
5$5,545<5D5L5T5\5d5p5
5$5,545<5D5L5T5\5h5
5$5,545<5D5P5p5x5
5$5,545<5D5P5t5|5
5$5,545<5H5l5
5$5,545<5H5l5t5|5
5$5,545<5H5P5
5$5,585\5d5l5t5|5
5$5,585X5`5h5p5x5
5$5,5H5h5p5
5$5@5P5\5d5
5$5+5C5M5f5w5
5$5<5D5L5T5\5d5l5t5|5
5$5=5
5$50585X5t5
5$50585X5t5|5
5$505P5X5`5h5t5
5$505P5X5`5l5
5$505T5\5d5l5t5|5
5$555;5@5J5O5a5k5
5$595V5
5$595V5q5
5$5A5[5a5t5
5$5I5t5z5
5$5L5T5\5d5|5
5$5U5t5
5$6+:
5$6+686D6U6
5$616b6q6
5$696b6
5$6f6
5$6N6
5$6P6p6x6
5%5*5=5C5]5
5%5:5i5{5
5%5:5W5
5%5+5/5C5U5h5r5
5%525?5L5Y5f5|5
5%5D5
5%5I5P5i5z5
5%5I5W5n5
5%5M5
5%5n5
5%6_6
5%626A6~6
5%6c6z6
5%8$9)9|9
5&505E5Z5w5
5&525@5
5&5E5b5v5
5(5.5F5]5x5
5(5@5W5k5r5
5(50585D5d5l5t5|5
5(50585D5d5p5
5(50585H5l5t5|5
5(565@5J5[5`5q5~5
5(575
5(585H5l5t5|5
5(585H5P5`5p5
5(585H5X5h5p5
5(585H5X5h5x5
5(5c5
5(5F5`5
5(5H5h5p5|5
5(5H5P5X5`5h5x5
5(5L5T5\5d5l5t5|5
5(6:6>7L7q8
5(6`6
5(6<6f6~6
5(6r6
5)5.5F5K5c5s5
5)5/5C5M5i5
5)5~5
5)5=5m5
5)535=5G5Q5[5e5o5y5
5)535>5S5X5k5}5
5)535L5]5
5)545V5v6}6
5)555]5
5)5C5
5)5O5
5)6<6
5*5&6/6
5*5;5@5M5R5q5v5
5*5@5K5]5h5z5
5*565G5
5*5A5j5
5*5G5
5*5h5n5t5
5*5M5
5*5M5^5z5
5*6{6
5*6Z6
5,5>5F5R5^5j5v5
5,545<5D5L5T5\5d5l5t5|5
5,545<5H5h5p5x5
5,54585<5D5H5L5P5x5
5,565T5^5w5
5,5A5T5p5z5
5,5H5X5^5f5{5
5,5O5`5|5
5,5P5n5
5,5s5
5,657q7
5,6b7
5,6d6
5,6G6
5,6i6
5.5^5~5
5.5B5h5|5
5.5V5o5{5
5.6\6
5.6^6q6
5.637
5.656N6_6
5.6p6
5.6r6
5/555J5
5/577
5/5H5V5r5
5/5L5s596k9
5/6F6q6
5:5C5[5g5
5:5P5a5g5
5;5k5
5;5R5
5;5s5
5;6B6[6u6|6
5;9v:
5?*BL?fff?
5@5g5p5
5@5L5p5
5@5L5X5f5
5@5r5
5@5x5
5@6e6o6
5@6G6`6
5@7J7_7t7
5@7r7}7
5[6c7
5\5;<D<
5\5f5
5\5p5
5\6f6
5\6f6{6
5^5r5{5
5^6s7
5_6i6
5_6r6
5+5<5}5
5+505@5E5Z5_5k5p5~5
5+555D5
5+565<5F5X5h5}5
5+6o6
5+6q6
5<5D5H5L5T5X5\5`5
5<5D5L5T5\5d5l5t5|5
5<5H5h5t5
5<5t5
5<6b6
5<6R6
5=5m5
5=5U5a5r5
5=6]6n6
5>5a5
5>5C5
5>5D5J5b5
5>5e5
5>5E5^5
5>5v5}5
5>6a6
5>6E6^6
5>6E6^6o6
50?0T0i0>2
505@5d5l5t5|5
50545H5L5P5
50585@5H5P5X5d5
50585@5L5l5t5|5
50585H5T5t5|5
50585L5T5h5p5
505C5p5
505j5
505L5c5z7
505q5
505R5
505U5x5
505z5
506<6H6V6s6
50676e6y6
506U6
506v6
50R0u0
515\5
515=5I5U5}5
515=5I5U5t5
515>5J5e5
51585E5Q5b5
515E5N5
515E5Q5]5i5
515F5c5}5
515P5z5
515Q5q5
515x5
525<5Q5f5
525>5X5t5
525N5V5j5~5
525S5q5}5
526L6j6p6
526r6
535>5
535C5Z5
535G5[5q5
535J5a5x5
535Q5e5q5}5
53696Y6|6
536c6
536I6X6h6
536Q6]6i6u6
536X6
545<5@5D5L5P5T5X5
545<5D5L5T5\5d5l5t5|5
545<5D5P5p5x5
545<5D5P5t5|5
545<5L5X5x5
545>5W5h5
545P5`5l5t5
546H6c6}6
546l6
5'5.5F5W5]5j5|5
5-5:5F5W5
5'5[5
5-5]5
5'5^5e5~5
5-5=5j5
5-5=5R5e5q5
5-5>5R5
555?5U5
555m5
555N5
556~6
556j6
5'585
558k8
5'5D5`5j5
5'6,6<6B6U6[6s6
5-6`6
565D5T5d5
566]6
566J6}6
566s7
566X6
566Z6a6
5-6g6
5-6H6W6
5'6L6b6
575<5O5[5`5v5{5
575<5V5c5o5
575A5Y5
575E5c5q5
575h5w5
57687=7I8N8
576b6v6
576e6
576O6
576Y6
585@5H5P5X5h5p5
585@5H5T5t5
585@5H5T5t5|5
585{5
585>5^5m5
585B5U5p5
585N5
585r5y5
585Y5
586n6
586T6k6
587B7W7l7
595>5W5\5q5}5
595D5c5
595R5g5
596d6
596r6
596v6
5A5k5Q6
5A5Q5]5i5
5a5u5
5A6x6
5B5G5
5B5g5
5B5L5u5
5B5M5
5B6|6
5B6j6
5b6l6
5B6L6e6v6
5B6v6
5B9@:v>
5C5a5u5
5C6a6m6y6
5C6i6t6
5C6m6
5D5y5
5D6P6e6p6
5E5_5r5w5
5E5{5
5E5U5[5c5x5
5E6`6
5E6|6
5f5U6\6
5F6M6
5F6P6h6
5f6V;
5g5~5
5G6l6
5Genu
5H5\5p5|5
5H5_5u5
5H5W5
5H5X5{5
5H6m6
5H6p6
5H6U6
5h6x6
5h6z6
5I6c6
5ineI
5J5K6c6~6
5j6!7
5j6q6
5J6T6m6
5J6z6
5j7t7
5K5_5(676{6
5K5r5
5K5s5
5K5X5d5u5
5K5Z5
5L6Q6
5l6q6
5m5w5
5M6q6
5N5f5m5
5N5i5
5O6T6
5O6Y6n6
5P5Z5l5
5P6`8
5P7U7
5Q5k5
5q5r6
5Q6d6z6
5Q6n6
5Q6V6
5Q6z6
5R5W5
5R5Y5q5
5R6j6q6
5R6q6
5S5`5
5S5y5
5S6C7
5S6q6
5S7x7
5T5]5
5T5v5
5t6P8Z8
5U5c5i5
5W5}5
5W5~5
5W6a6z6
5W6u6
5X5q5
5y5F7:8t8
5Y6j6~6
5Y6k6
5Y7^7
5y8P9
5Z5q5
6 6$6(6,6064686<6@6D6H6L6P6c6
6 6$6(6,6064686<6@6D6H6L6P6T6X6\6`6d6h6l6p6
6 6$6(6,6064686<6@6D6H6L6P6T6X6\6`6d6h6l6p6t6x6|6
6 6$6(6,6T6\6`6d6l6p6t6x6
6 6$6(6064686<6d6l6p6t6|6
6 6$6L6T6X6\6d6h6l6p6
6 6%616C6J6W6c6t6
6 6(6,60646\6d6h6l6t6x6|6
6 6(60686D6d6l6t6|6
6 6(686D6T6l6|6
6 6(6H6d6t6
6 6,6A6Q6X6]6q6x6
6 6,6L6T6\6d6l6t6
6 6,6L6T6\6d6l6t6|6
6 6,6L6X6x6
6 6:6
6 6@6\6d6l6t6|6
6 6@6H6P6\6|6
6 6@6H6T6t6
6 6<6\6d6p6
6 6<6D6L6T6\6h6p6
6 606@6P6`6p6
6 60686\6l6t6|6
6 636C6\6l6w6
6 656J6Z6e6
6 676=6S6Y6k6v6|6
6 676H6M6f6
6 6B6
6 6D6L6T6\6d6l6t6|6
6 6P6f6
6 787R7r7
6 7b7
6 7E7]7
6 7i7
6 7X7
6!6*60666?6H6R6\6
6!6:6Q6
6!6+6@6U6h6
6!626R6Y6r6
6!646b6
6!6'6-63696?6F6M6T6[6b6i6p6x6
6!6-696E6m6
6!686T6x6
6!6Z6q6
6!7Q7t7
6"6(6@6M6Y6j6
6"6.6?6~6
6"6]6
6"61696G6\6a6w6|6
6"63696F6P6\6m6
6"666J6\6
6"6-6N6a6z6
6"6E6t6
6"6O6
6"6x6
6"7/7E7k7
6"7-7H7O7x7
6"7W7}7
6#6.6E6R6f6z6
6#636`6w6
6#666<6O6V6n6
6#6A6M6Y6e6
6#6A6M6Y6h6
6#6A6U6a6m6y6
6#6K6
6#6o6
6#7`7
6#707<7M7
6#777E7
6#7A7M7Y7e7
6#7A7U7a7m7y7
6#7c7
6#7d7x7
6#7q7
6#7U7Z7e8j8
6#8J9q:
6$6(6,6D6H6`6d6|6
6$6,646@6`6h6p6x6
6$6,646@6d6l6t6|6
6$6,646<6D6L6T6\6d6l6t6
6$6,646<6D6L6T6\6d6l6t6|6
6$6,646<6D6L6T6\6h6
6$6,646<6D6L6T6`6
6$6,646<6D6L6X6|6
6$6,646<6H6l6t6|6
6$6,686\6|6
6$6,686X6`6h6p6
6$60686P6t6
6$606T6\6d6l6t6|6
6$646D6T6h6x6
6$6D6`6p6|6
6$6D6I6p6u6
6$6D6L6T6\6h6
6$6n6
6$6s6
6$7[7g7s7
6$7\7
6$7A7H7a7
6$7F8
6$7Q8+9z9
6$8B8[8~8
6$8k8x:
6%6.6
6%6/6D6Y6
6%616?6
6%636
6%657l7
6%666
6%696E6Q6]6
6%6D6
6%7>7]7v7
6%7e7
6%7H7Z7
6&6<6
6&626G6L6c6h6u6z6
6&6C6
6&6C6Z6a6
6&6H6X6d6
6&6w6
6&707E7Z7
6&7Q7a7
6(6,6<6@6P6\6l6|6
6(6,6D6H6`6d6h6l6t6
6(606<6\6h6
6(60686H6l6t6|6
6(606d6t6
6(646T6`6
6(666A6a6s6~7
6(666S6
6(686@6H6X6`6h6p6x6
6(686>6F6[6g6
6(6B6^6n6t6|6
6(6L6T6\6d6l6t6|6
6(6P6
6(6X6i6
6(7`7
6(7c7}7
6(7P7
6(7u7
6)6@6
6)646S6q6
6)656]6
6)686X6
6)696{6
6)6H6
6)6k6r6
6)6L6V6]6
6)6Z6o6x6
6)7i7
6)7M7r7|7
6)7Y7
6*646L6b6
6*6A6j6
6*6M6
6*7E7
6*7l7
6,6?6I6]6c6g6{6
6,6<6L6T6\6d6l6t6
6,6=6C6H6T6^6j6{6
6,616?6[6`6
6,636Y6_6d6p6z6
6,646<6D6L6T6`6
6,646<6H6h6p6x6
6,646H6P6X6`6h6p6x6
6,656>6G6P6
6,686@6t6
6,6c6
6,6D6L6T6\6d6l6t6|6
6,6J6Z6
6,6k6
6,7@7X7j7
6,717i7
6,737H7]7j7
6,747<7D7L7X7`7
6,7v7
6,9O9r9~9
6.;,<6=
6.6?6E6R6d6k6x6
6.6|6
6.636B6G6V6[6n6s6
6.686r6|6
6.6h6o6
6.6L6e6
6.6z6
6.7N7
6.7r7w7
6.8c9{9
6/656:6F6[6j6
6/696b6m6
6/72878
6/7A7V7
6/7P7i7
6:6?6U6Z6q6
6:6Q6x6
6:6S6l6
6:7M7
6:7p7
6:7Q7
6:8~8
6;6R6v6
6;7D7
6?6E6K6c6
6?7M7[7
6?7Q7
6?7V7
6@6D6L6P6T6\6`6d6h6
6@6N6
6@6p6
6@6S6l6
6@6x6
6@7M7x7~7
6[6o6
6[6q6
6[7b7{7
6^6|7
6^7o7
6}8%9
6+6@6N6[6H7V7r7
6+6A6
6+6X6]6i6v6
6+787
6+8{8
6<6^6
6<6_6k6
6<6D6L6T6\6d6l6t6|6
6<6D6L6X6x6
6<6D6P6p6x6
6<6D6t6|6
6<6H6h6p6|6
6<6t6
6<6X6
6<7\7
6<7_7
6<7y7
6=6D6]6
6=6D6]6n6
6=6F6
6=7u7
6>6a6
6>6N6g6
6>7`7s7|7`8
6>8k8
606<6\6d6l6x6
60666:6D6k6{6
60686@6H6P6X6`6p6
60686D6d6l6t6|6
606B6[6k6v6
606G6e6y6
607?7
607h7
607M7T7f7
616*7
61696?6D6M6a6k6
616F6f687Z7a7z7
616F6N6V6e7
616G6M6c6i6
616J6c6
616K6Q6d6
616v6
616V6z6
616z6
61767b8p8~8
617H7?8U8
617L7c7
617r8
618;8U8i8
626[6
626h6
626K6p6w6
626S6
627@7N7
627E8
627N7
627r7
628G9
636A6
636i6p6
636L6W6f6
637b7h7z7
637O7t7
637Q7]7i7u7
637s7
637Y7d7
646<6D6L6l6t6|6
646<6D6L6T6\6d6l6t6|6
646<6D6L6X6x6
646<6H6l6t6|6
646<6T6\6d6l6t6|6
646D6P6X6x6
646g6
646P6X6`6h6p6|6
646T6\6d6l6t6|6
646X6i6
647D7U7q7
647H7T7`7l7
647l7
647Y7C8
656<6Q6f6
656=6K6P6i6}6
656x6
656X6i6
657:7
657?7R7
657l7v7
657z7
658G8R8]8
658k8
6'6.656<6C6W6l6
6-626I6N6e6
666p6
667J7p7
667Q7
6-6s6
6-6Y6_6f6s6z6
6'7.7S7y7
6-7:7
6'7@7
6'7]7
6'7_7
6-7}7
6'7>7
6'717[7f7q7
6-72777
6'747 8f8
6-747A7M7^7
6-767E7i7
676E6O6^6x6
676O6V6
676Q6d6i6
677A7
677W7
679d9n9
6-7a7
6'7A7K7a7
6-7j7
6-7J7P7p7
6'7q7
6-7Q7V7
686@6`6h6
686@6H6P6X6d6l6
686=6
686I6O6T6`6j6t6~6
686J6U6[6h6z6
686p6
687B7T7
696D6c6
696S6k6
697O7
697R7e7
697v7
6a6h6}6
6a6r6
6A6w6~6
6a798
6A8N8
6B6c6
6B6d6
6B6g6t6
6B7|7
6C6}6
6C6a6m6y6
6C6P6
6C7]7
6c7|7
6C7~7
6C73:%<
6C7a7m7y7
6C7d7
6C7w8
6C7Z78<
6D6|6
6D667p7
6D6c6
6D6L6T6\6
6d6n6
6D6O6
6D6o6
6D6x6
6d7k7
6D7K7`7u7
6d8|8
6d8n8
6E6e6
6E7l8
6f6p6
6F6P6q6x6
6F6Z6
6G6}6
6g6t6
6g7K<l=
6g7l7
6H6]6b6z6
6H7c7
6h7w7x8
6I6{6
6I6l6
6j7|798K8
6J7a7
6JCy7JCy7JCy7JCy7
6K6r6
6k6u6
6K7T7
6L6`6l6x6
6L6l6
6L6q6
6L6V6
6l7t9
6M<h<w<
6M6p6
6m7%8
6M7{7
6M9}9
6N6X6k6
6N7t7
6N7U7n7
6O7e7
6P6U6
6p7u7;8v8
6Q7k7
6R6l6
6R6Y6q6
6R7\7t7
6S6]6
6S6r6y6
6S738C9
6S7y7
6SVQRQ
6T7j7z7
6t8/9
6V6f6
6V6p6
6V7]7v7
6v779&:
6v7P=l=
6W6^6v6
6X6g6x6
6X7b7
6X7j7
6X7l7
6X7s7
6Y6\7f7x7
6Z7f7
7 :7:
7 7$7(7,7074787<7@7D7H7L7P7T7X7\7`7d7h7l7p7t7x7|7
7 7$7(7,7074787<7@7D7X7d7h7
7 7$7(7P7T7X7\7`7d7h7l7p7t7x7|7
7 7$747@7P7`7p7
7 7%7<7A7X7]7t7y7
7 7(707@7H7X7`7h7x7
7 7(707<7\7d7l7t7
7 7(707<7\7d7p7
7 7(70787@7H7T7t7|7
7 7(70787@7L7l7t7|7
7 7(70787@7T7\7d7l7t7|7
7 7(70787D7d7l7t7|7
7 7(747T7\7h7
7 7(787\7d7l7t7|7
7 7*777k7
7 7,767@7J7T7^7h7r7
7 7,7L7T7\7d7l7x7
7 7,7L7T7\7d7p7
7 7;7@7i7n7
7 7@7H7T7t7
7 707@7P7`7p7
7 707T7\7d7l7t7|7
7 717s7
7 717z7
7 73797Q7b7h7m7t7
7 757J7[7N8
7 787<7@7D7H7L7T7X7\7`7d7h7|7
7 7H7P7T7X7`7d7h7l7
7 7N748
7 8%8
7 8,888F8l8
7 8?8
7 8'8<8V8
7 8X8
7 8Z8a8y8
7 9d9i9n9
7!7/7;7@7V7[7o7
7!7:7Q7
7!7;7A7T7q7
7!7<7j7
7!717A7Q7a7q7
7!737E7W7b7x7
7!7'7?7P7V7[7b7}7
7!7-7`7
7!7'7+7?7Q7
7!7-797E7m7
7!788J8
7!7f7
7!7R7a7
7!8(8}8
7!8(8A8O8
7!8[8
7!8_8
7!888S8v8
7!8q8
7"7/:<:k:|:
7"737Q7u7
7"7'737=7G7Q7[7e7o7y7
7"797Z7w7
7"7G7R7o7
7"7j7w7
7"8,8V8a8
7"8-8
7"8a8q8
7"8C8a8m8y8
7"8E8u8
7"8l8
7#7)7>7N7g7w7
7#7,7
7#7+737<9Q9c9k9s9
7#747>7
7#787M7j7
7#7A7M7Y7e7
7#8;8K8i8y8
7#8`8l8x8
7#8A8M8Y8e8
7#8I8T8s8
7#8O8c8o8}8
7#9>9H9]9r9
7$7(787<7L7P7`7p7
7$7)7
7$7,747@7`7h7p7x7
7$7,747@7`7l7
7$7,747<7
7$7,747<7D7L7T7\7d7l7t7
7$7,747<7D7L7T7\7d7l7t7|7
7$7,747<7D7L7T7\7h7
7$7,747<7D7L7T7`7
7$7,747<7D7L7T7`7h7
7$7,747<7D7L7X7`7
7$7,747<7D7L7X7|7
7$7,747<7D7L7X7x7
7$7,747<7D7P7p7x7
7$7,747<7D7P7t7|7
7$7,747<7H7P7p7
7$7,747L7T7\7d7l7t7|7
7$7,787@7t7|7
7$7,7L7T7\7d7
7$7:7?7P7Z7e7z7
7$7\7
7$7^7e7}7
7$707P7\7|7
7$707P7X7d7
7$747@7d7l7t7|7
7$747<7D7L7T7\7d7l7t7|7
7$7C7
7$7C7a7m7y7
7$7D7L7T7\7d7l7x7
7$7D7L7X7x7
7$8,80848<8@8D8H8p8x8|8
7$8\8
7$8c8
7$8T8
7%7/7H7e7o7
7%7;7|7
7%7;7Q7g7}7
7%7]7g7
7%7<7S7j7
7%717=7I7q7
7%717=7I7U7a7m7y7
7%717Y7
7%7F7S7
7%8*8i8
7%8;8
7%858
7%8O8
7&7;7G7a7}7
7&7C7
7&7E7d7
7&7o7
7&7Y7~7
7&8A8
7&8u8
7&8Y8r8x8
7&95:
7&9F9
7(7/7?7X7h7s7
7(707@7L7\7d7t7x7
7(7074787@7D7H7L7t7|7
7(70787D7d7l7t7|7
7(707P7X7x7
7(707X7`7h7p7
7(727M7^7d7n7x7
7(747T7`7
7(767S7
7(7a7
7(7E7
7(7e7m7
7(7h738=8V8
7(7H7P7X7`7h7t7
7(7H7T7t7|7
7(7s7
7(8/8H8Y8v8
7(8:8`8s8
7(818e9
7(838Q8
7(888^8=9
7(888H8P8t8
7(8h8
7(8j8
7(8L8T8\8d8l8t8|8
7)7<7^7g7l7
7)757T7
7)7u8
7)7V7`7x7
7)8<8X8d8y8
7)8c8s8
7)8K8
7)8V8
7)8z8
7)909I9m9t9
7*70757A7S7Z7g7s7
7*747I7^7
7*747I7P7r7
7*7A7j7
7*7A7Z7q7
7*7C7x8
7*7F7R7n7z7
7*7l7
7*8h8m8|8
7*8x8
7*8y8
7*X*r5
7,7@7
7,7@7o7
7,7@7X7g7
7,737L7e7
7,747@7H7|7
7,747<7D7L7d7p7
7,747<7D7L7T7\7d7p7
7,767F7V7f7
7,787@7`7h7p7x7
7,797>7U7Z7j7p7
7,797Q7X7p7
7,7d7
7,7D7P7a7
7,7G8`8
7,7H7X7^7f7{7
7,7J7T7m7~7
7,7k7
7,7o7
7,7Y7
7,898
7,8a8
7,8d8
7,8Q8[8{8
7,9f9
7.7?7P7a7
7.737A7K7d7i7|7
7.7E7
7.7I8b8
7.7K7
7.8^8
7.8{8
7.8p8
7/7w7
7/848F8M8c8i8
7/898E8Q8p8
7/8A8V8
7/8q8
7:7t7
7:7U7x7
7:8[8u8
7;3tQ
7;7H8
7;7O7
7;8H8T8e8
7?7F7x7
7?7o7
7?8Q8f8
7@7H7L7P7X7\7`7d7
7@7K7
7@7p7
7@7P7\7~7
7@7x7
7@7x8
7@8v8
7@8X8b8w8
7@PVW
7\7|7
7\7c7|7
7\8c8r8
7^7c7h7
7^8k8
7^8O9#:]:}:
7_;'=
7_8v8
7`7m7t7y7\8q8
7{9]:b:,>J>Q>f>{>
7+717D7r7
7+757
7+8]8~8
7+888i8x8
7+8u8z8
7<7\7}7
7<7]:j:
7<7D7P7p7|7
7<8H8t8\9r9
7<8I8U8f8
7<8J8P8c8h8
7<8Z8t8
7=7_7
7=7D7Z7f7
7=7Q7w7
7=7U7u7
7=8O8$9
7>7h7
7>7H7]7r7
7>8C8W9-=2=a=
7>8R8
707<7\7d7l7t7|7
707<7\7h7
707=7O7_7
70787@7H7P7\7d7
70787@7H7P7X7d7
70787@7H7T7\7
70787<7@7H7L7P7T7|7
70787D7d7l7t7
707e7
707P7\7|7
70878L8a8t8x8|8
708W8
70A0Z0
71:K:p:z:
717=7V7\7`7j7
71767G7N7d7x7
717E7Q7]7i7
717E7Y7e7q7}7
717F7
717F7c7
717J7c7~7
717V7j7|7
718;8P8e8
718;8X8
718]8q8}8
718D8`8l8
718R8n8
727p7
728|8
72888T8
728r8
728W8a8
737]7
737=7Z7}7
73797O7V7n7
737B7
737e7y7
737H7W7
737m7
737Q7]7i7u7
73888
738C8I8Q8f8r8
738G8P8U8
738m8
738M8h8
738Q8]8i8u8
738Q8e8q8}8
738s8
738t8
747<7D7L7T7\7d7l7t7|7
747<7D7P7p7x7
747E7K7P7\7f7r7
747j7q7y7
747P7`7l7t7
747W7
747Z7e7r7
748#9
74898>8
748k8w8
748l8
748U8
757:7K7R7i7
757`7
757`7w7
758<8Q8f8
76:M:
767@7P7l7
767I7k7p7
767X7r7
768P8|8
7'7?7]7b7r7
7'7?7R7^7}7
7-7{7
7'7<7Q7n7
7-747G7
7'767@7`7
777}7
777=7R7X7j7q7
777S7
777Y7x7
778^8
7-787H7O7_7f7v7}7
778M8
778X8l8
7-7D7J7`7e7
7-7S7}7
7-7X7z7
7'8^8r8
7'8~8
787@7D7H7P7T7X7\7
787@7H7P7X7`7h7t7
787@7H7P7X7d7
787@7L7l7t7
787\7
787e7
787g9
787O7m7
787p7
787q7
787T7d7p7x7
788}8
7'8l8
7-8o8
79?e?
797>7~7
797C7X7m7
797D7c7
797I7T7u7
798^8h8
798n8
7A7G7\7b7r7w7
7a7k7
7A8[8_8c8g8k8o8s8w8{8
7a8|8
7A8a8
7A8T8
7B7O7g7m7
7B7P7g7
7B7w7
7b7y7
7B8g8
7b8s8|8
7B8V8
7C:a:m:y:
7C;^$|
7C7I7_7e7y7
7C8a8
7C8i8t8
7c8m8
7C8r8
7D7^7
7D7|7
7D7~7
7D7K7c7
7d7q7
7d7q7}7
7D8j8
7e7~7
7E7U7[7
7E8]8d8}8
7E8U8.9
7eME~
7F7a7
7f7k7
7F7p7
7F7W7_7l7w7
7F8\8
7f8Y9
7f9p9
7g;q;
7G7b7|7
7G7r7
7G8_8i8
7G8T8g8q8
7G9S:
7g9u9
7GY;}
7H7P7X7d7
7H8/9
7H8h8p8
7H8R8e8
7H8W8
7H8Z8
7I7n7
7I7W7;:
7i8U9
7I8W8!9?9[9q9
7J7o7$8x8
7j8{8
7J9l9s9
'7JCy7
7K7{7
7K7~7
7K7Q7
7K7Q7W7o7
7k7u7
7K8u9
7L7\7
7L7i7
7l8s8
7N7j7
7N7S7o7t7
7N7W9
7n9*;
7O7\7h7y7
7O798w8
7P7g7y:
7P7V7\7t7
7p7w7
7P8o8
7P8o8v8
7Q8}8
'7QVh
7R7\7
7s:y;
7S7q7
7S8@9g9
7S8p8
'7SkipLayerNormalization
7T7{7
7t8{8
7T8p8z8
7V;n;s;
7V7[7`7h7
7V7w7
7v8\9
7V8h8
7w8(<
7W8u8
7y8 9
7Y8k8
7z7;8D8
7Z7q7
8 8$8(8,8084888<8@8D8H8L8P8T8X8\8`8d8h8l8p8t8x8|8
8 8$8(8,848L8\8l8p8
8 8$8(8084888@8D8L8P8T8\8`8h8l8p8x8|8
8 8$8,8084888`8h8l8p8x8|8
8 8$84888H8X8h8x8
8 8%8<8M8S8X8d8n8z8
8 8(808<8\8d8l8t8
8 8(80888@8H8T8t8|8
8 8(80888D8L8
8 8(848<8\8x8
8 8(888@8P8X8d8p8
8 8(888\8d8l8t8|8
8 8,8=8~8
8 8,848h8x8
8 8,848T8p8
8 8,8L8T8\8d8l8t8|8
8 8,8L8X8x8
8 8:8Y8q8
8 8@8H8P8X8`8l8
8 808@8P8`8p8
8 80888H8P8X8`8h8p8
8 878R8
8 8-8A8U8i8}8
8 8H8T8t8|8
8 8K8Q8f8l8
8 8q8
8 8t8
8 8u8
8 9m9
8!8/848B8G8^8m8r8
8!868<8Q8W8k8r8
8!878j8
8!8-898X8
8!8A8a8
8!8f8
8!8F8a8
8!8J8a8
8!8J8T8i8
8!8S8]8i8u8
8!9;9
8!9~9N;N<
8!9+9D9K9j9t9
8!959e9y9
8"8(8:8@8S8Z8s8
8"8(8B8H8b8h8
8"82888@8U8a8z8
8"82888I8Z8j8o8
8"848I8f8
8"878V8
8"898^8{8
8"898P8g8~8
8"8c8
8"9,9A9V9s9
8"9,9H9M9Y9r9
8"9'9,9
8#:A:K:N<X<
8#848z8
8#8A8M8Y8e8
8#8D8W8p8
8#8F8P8u8
8#8I8]8x8
8#8N8p8
8#9@9V9}9
8#919M9
8#9a9v9
8#9e9
8#9I9T9x9
8#9s9
8$8(8,808
8$8(8,808X8`8d8h8p8t8x8|8
8$8(8,84888<8@8h8p8t8x8
8$8*8?8V8]8t8
8$8,848<8D8L8d8l8t8|8
8$8,848<8D8L8p8
8$8,848<8D8L8T8\8d8l8t8|8
8$8,848<8D8L8T8\8d8l8x8
8$8,848<8D8L8T8\8d8p8
8$8,848<8D8L8T8\8h8
8$8,848<8D8P8p8x8
8$8,848<8D8P8X8x8
8$8,848<8L8T8d8p8
8$8,888X8`8h8p8x8
8$8,8h8
8$8:8Q9[9s9
8$80888l8t8|8
8$80888X8t8
8$808A8
8$808P8X8`8h8p8|8
8$848D8X8h8x8
8$8D8L8T8\8h8
8$8D8P8X8
8$8o8z8
8$9.9S9p9
8$9?9f9
8$9\9
8$9`9
8$909<9H9g9
8$9G9j9
8$9I9S9
8$9T9
8%8>8
8%808C8I8^8
8%878<8e8j8
8%8D8
8%8M8
8%8N8]8
8%999O9c9i9
8%9e9
8%9k9
8%9s9
8%9v9
8%9Z9
8&:+:v:{:
8&;+;`;e;
8&8,8?8J8P8U8f8q8w8|8
8&8|8
8&898R8m8y8~8
8&8A8Q8^8
8&8C8
8&8W8
8&9`9
8&9<9I9V9
8&939Z9g9
8&979S9j9S:
8&9A9
8&9V9
8(8:8g8
8(8>8H8^8h8o9y9
8(80888@8H8T8t8|8
8(80888D8d8l8t8|8
8(808P8l8|8
8(848
8(888@8P8`8p8
8(888H8X8h8x8
8(8b8i8
8(8F8V8^8n8
8(8H8P8X8d8
8(8L8T8\8d8l8t8|8
8(9`9
8(979<9N9[9g9u9
8(9f9z9
8)8:8@8E8Q8c8j8w8
8)8>8K8W8h8
8)848?8K8k8p8
8)858]8
8)858A8M8u8
8)868C8T8i8s8}8
8)8b8
8)8H8
8)8H8[8t8
8)8O8Y8r8
8)8S8X8
8)9N9e9
8)9o9
8)9S9$:t:
8)9x9
8*868G8
8*8A8j8
8*8N8^8
8*8Q8~8
8*8Z8q8
8*9{9
8*979G9V9i9
8*9Q9
8*9s9
8*9t9
8*9x9
8,8084888<8@8D8H8L8P8T8X8\8`8d8h8l8p8t8x8|8
8,828D8O8U8Z8c8v8{8
8,828G8a8
8,848<8`8
8,848<8D8L8T8\8d8p8
8,848<8D8L8T8`8
8,848<8H8h8p8x8
8,84888<8D8H8L8P8x8
8,878=8G8Y8~8
8,888X8`8h8p8|8
8,888X8d8
8,8O8
8,969I9Y9
8,9d9
8,9h9
8,9P9,:
8,9t9
8,t'V
8.8~8
8.8h8
8.989M9b9
8.989r9
8.9I9W9
8/8;8U8q8}8
8/8=8g8
8/8v8
8/9_9
8/9J9T9
8/9P9
8:8?8K8X8m8s8
8:8Q8z8
8:8t8
8;8@8g8l8
8;8j8p8
8?8,939B9_9
8?8S8
8@:x:
8@8G8Y8u8
8@8J8j8
8@8P8\8d8
8@8S8l8
8@9c9
8@9v9
8@9W9
8[8b8t8
8[9|9
8[9z9
8\9l9|9
8\u"j
8^:c:
8^9c9
8^hYYj`XjPY
8^u(@
8_,t*j
8_^[]
8_9^<5=
8_9y9
8`8{8
8`8j8
8{un@
8|9J:h:
8+818D8a8{8
8+8B8h9
8+8G8^8u:
8+8I8d8~8
8+8U8
8+929J9z9
8+D$ f
8<;H;T;`;
8<8D8L8T8\8d8l8t8|8
8<8D8L8T8\8d8p8
8<8T8
8<8t8
8<9_9
8<9a9
8<9F9o9z9D:T:
8<9L9
8<9l9
8<9o9
8<9T9
8=8l8
8=8w8
8=9\9
8>8O8U8Z8n8{8
8>8x8
8>9{9
8>9j9t9
8>9o9
8>9U9p9
80=0p0
80858I8P8g8
80888@8H8T8t8
80888@8L8l8t8
80888]8h8t8
80888D8d8p8
80888H8l8t8|8
808P8l8
808q8
808R8\8t8
808U8
80959
809J9S9
809n9
809p9
809R9
809v9
81;};
818?8I8]8c8g8{8
818@8
818B8H8]8d8y8
818C8K8S8<:Q:c:k:s:,<A<S<[<c<l>
818C8w8
818F8V8a8
818H8`8x8
818J8a8z8
818K8Q8d8
818M8q8
818W8s8
818X8j8
818Z8q8
819;9a9l9
819}9
81989M9b9
819S9
82888P8g8y8
828B8H8P8e8q8
828C8_8v8Q9X9
829{9
829J9_9l9
829L;m;u;
829l9
829r9
829x9
838>8L8
838F8R8q8
839:9G9S9d9
839D9q9
839I9_9
839Q9
839Q9]9i9u9
839Q9e9y9
839v9
839X9
839y9
84:N:o:
848@8x8
848<8D8L8T8\8d8l8t8|8
848<8D8L8X8x8
848I8w8~8
848T8\8d8l8t8|8
848V8
849H9\9
858j8|8
859`9s9
859I9y9
859o9
859Y9
86;];s;
868L8
868P8{8
869=9U9
869I9
869m:
869P9l9
869q9
878E8Q8V8k8s8x8
878F8g8w8
878G8 9
879L9b9
879N9k9
8-8?8D8[8b8v8|8
8-83888L8Y8e8v8
888@8H8P8X8`8p8
888@8L8l8x8
888\8d8l8t8|8
888D8d8l8t8|8
888D8d8p8
888H8T8\8|8
888p8
888T8d8p8x8
8'8F8g8
8-8J8
8'9@9S9
89}4w
898A8Q8X8e8m8
898D8c8
899{9
899R9g9
899s:
899s9z9
8-9E9
8-9H9Y9
8'9L9b9
8-9R9
8a9}9
8a9k9
8A9K9e9
8b:x:
8B8e8
8B8i8
8B9]9
8c8|8
8C8M8a8n8
8C9a9m9y9
8C9i9t9
8C9p9
8D8|8
8D8}8
8D8i8
8D8L8T8\8d8p8
8D8Z8
8D9K9d9
8D9O9Z9
8E8K8Q8i8
8E9O9~9
8F$tA
8F,t,j
8F8i8
8f8u8}8
8F9m9
8F9V9
8G$t5
8g8y8
8g9q9
8giP9giP9giP9giP9
8H8O8
8H8t8
8H8W8
8H8x8
8H8X8b8l8v8
8H9c9m9
8h9w9&:
8I8k8
8I8P8]8i8z8
8i8s8
8J(Wj
8j:H;V;~;
8J;;=?>V>
8J8O8
8j8p8
8J9h9
8k:'<
8K:U:j:
8K8R8d8y8
8K8Z8
8K9q9
8L8Q8c8j8
8L8R8X8p8
8L8S8`8p8
8l8v8
8L8V8}8
8L9Q9w:
8L9r9
8l9t9|9
8l9v9{9
8M<t1
8M8h8w8
8n8{8
8N8m8
8N9~9
8O$u,
8O,tU3
8O8T8_8
8O9}9
8Q8[8p8
8Q8l8
8Q9q9
8R8h8
8R9\9%:/:T:n:
8S t 
8S8g8
8S8q8
8S8q8}8
8S8z8
8S9l9
8S9w9
8S9X9
8T8^8
8-uJ@
8V8>9Z9a9v9
8V9j9
8W8|8
8W8h8|8
8W9Y:
8X8g8
8X8j8
8X8t8~8
8Y9s9
8Z8d8y8
8Z8q8
8Z9s9x9v:
9 :(:,:0:8:<:@:D:l:t:x:|:
9 :(:0:8:@:L:T:
9 :*:?:T:i:
9 :':<:Q:n:
9 :1:M:d:
9 :L:R:g:m:
9 :o:t:6;A;L;
9 :P:s:
9 9$9(9,9094989<9@9D9H9L9P9T9X9\9`9d9h9l9p9t9x9|9
9 9$9(9,9T9\9`9d9l9p9t9x9
9 9$9L9T9X9\9d9h9l9p9
9 9&959<9E9K9Q9Z9a9
9 9(9,90949\9d9h9l9t9x9|9
9 9(9,90949<9@9D9H9L9T9l9|9
9 9(9,94989@9D9L9P9X9\9d9h9p9t9|9
9 9(9\9d9l9t9|9
9 9(90989@9H9P9X9`9p9x9
9 9(90989H9l9t9|9
9 9(949<9\9x9
9 9(949T9\9d9l9x9
9 9(949T9\9h9
9 9(949T9`9
9 9(989\9d9l9t9|9
9 9*949>9X9t9
9 9,9K9
9 9,9L9T9\9h9
9 9,9L9T9`9
9 9,9L9X9x9
9 9:9F9e9q9
9 9@9\9l9x9
9 9@9H9P9X9`9h9p9|9
9 9@9L9l9x9
9 9<9e9o9
9 9=9
9 909@9H9X9\9l9p9
9 969N9g9
9 9'9>9U9p9
9 9D9L9T9\9d9l9t9|9
9!:<:B:O:V:]:o:
9!:A:
9!:C:m:
9!:I;
9!:j:
9!;R;W<d<y<
9!9.9:9K9
9!9:9Q9j9
9!9;9
9!919A9Q9a9q9
9!929
9!939;9C9
9!949Q9k9q9
9!959I9U9a9m9
9!969e9w9
9!9-999E9d9
9!9-999E9m9
9!999J9P9U9a9s9z9
9!9A9b9f9l9p9v9z9
9!9f9
9!9G9
9!9H9X9^9f9{9
9!9J9a9
9!9q9
9!9t9
9":2:V:f:
9":6:\:p:X;j;
9":8:c:}:
9":b:
9"9(9.949:9@9F9L9R9X9_9c9g9k9o9s9w9{9
9"9(9B9H9b9h9
9"9,969@9G9c9h9{9
9"9@9H9I:
9"9@9H9I:k:
9"9\9
9"9+9D9_9{9
9"9A9`9
9#:A:M:Y:e:
9#:A:q:
9#:m:
9#:Y:6={=
9#;|;
9#9\9
9#949
9#9A9M9Y9e9
9#9F9
9#9m;{;
9$:+:D:r:y:
9$:4:@:H:|:
9$:I:V:Z:^:b:f:j:n:r:v:y<
9$`\B+
9$9)9@9G9^9p9w9
9$9)989=9L9Q9`9e9y9
9$9,949@9`9h9t9
9$9,949<9D9L9T9\9d9l9t9|9
9$9,949<9D9L9T9\9d9l9x9
9$9,949<9D9L9T9\9d9p9
9$9,949<9D9L9T9\9h9
9$9,949<9D9L9T9`9
9$9,949<9D9L9X9`9
9$9,949<9H9h9p9x9
9$9,989@9X9h9p9x9
9$9,989\9d9
9$9,989\9d9l9t9|9
9$9,989X9d9
9$9,9D9\9h9
9$9:9@9X9n9t9
9$9@9M9}9
9$90989X9t9
9$909P9X9d9
9$909T9\9d9l9t9|9
9$949@9`9h9t9
9$979U9
9$999N9k9
9$9B9
9$9C9a9m9y9
9$9D9P9p9|9
9$9D9P9p9x9
9$9n9
9$9p9
9%:*:
9%:*:B:G:[:h:t:
9%:c:
9%:D:h:y:
9%:H:Z:
9%;<;];
9%9)9=9O9b9l9
9%9*9>9K9W9h9
9%9<9~9I:
9%919P9
9%969z9
9%989
9%9M9
9%9S9q9}9
9&:?:R:
9&:H:
9&:P:
9&:w:
9&;v;
9&<=<
9&9?9X9^9y9
9&9`9j9|9
9&9|9
9&9E9d9
9&Zmm
9(:2:K:\:m:
9(:i:
9(:I:[:s:
9(9:9
9(9[9
9(90989@9H9T9t9
9(90989@9L9l9x9
9(90989D9d9p9
9(909P9l9|9
9(949<9p9
9(949T9\9h9
9(989\9d9l9t9|9
9(989>9F9[9g9{9
9(989H9X9h9x9
9(9-9<9A9P9U9a9q9v9
9(9-9D9U9[9`9l9v9
9(9B9H9b9h9
9(9C9Y9c9v9
9(9H9P9\9|9
9(9H9P9X9`9h9p9x9
9(9H9T9t9
9(9H9X9|9
9(9L9T9\9d9l9t9|9
9(9P9i9u9
9(9Y9h9
9):.:n:
9):3:K:a:
9):A:
9):r:
9)9/9A9H9]9c9u9{9
9)9:9f9
9)9[9
9)909=9I9Z9
9)959T9
9)989X9
9)9j9
9)9k9r9
9)9p9}9
9*:|:
9*:3:
9*:K:
9*9?9H9O9
9*969B9N9Z9f9r9~9
9*979c9
9*979Y9E:X:
9*9A9j9
9*9A9l9x9
9*9A9Z9q9
9*9B9|9
9*9q9
9,:d:
9,:D:V:
9,:F:
9,:G:
9,:o:,;`;
9,:P:
9,:Z:_:
9,929:9O9[9u9
9,929A9L9_9e9z9
9,949@9`9h9t9
9,949<9D9L9T9\9d9l9t9|9
9,949<9H9h9p9|9
9,969K9`9z9
9,969Q9[9t9{9
9,9A9^9
9,9H9P9a9
9,9S9
9.:@:e:
9.:c:
9.:k:
9.9?9[9r9
9.9^9
9.9>9z9
9.94999M9Z9f9w9
9.979V9x9
9.9h9
9.9R9b9
9.9v9
9/:A:V:
9/:G:N:c:x:
9/:O:j:t:
9/9:9@9J9a9f9
9/9;9G9`9y9
9/999N9c9
9/9h9s:
9':.:
9::?:
9::?:m:
9::Q:
9:9A9W9h9
9:9j9
9':a:
9':L:V:f:v:
9;:B:[:
9;:D:
9;:m:
9;:W:^:w:
9;9A9T9
9;9A9V9\9t9
9;9D9S9
9;9I9`9|9
9?9{9
9?9D9
9?9J9
9@9\9t9
9@9g9v9
9@9j9
9[::;~;
9[9b9t9
9\$,r3
9] vP
9] vV
9] vY
9]$|]
9]$|`
9]$|W
9]9p9
9]9r9
9]9x9
9^ ua
9^$~C
9^(v%
9^,|,
9_ u%
9_$voh\-D
9_(s3Sj
9_,~6
9_,vE
9_:l:
9_4vd
9_9u9
9_m#a
9`:!;
9`9j9
9{:`<
9} |E
9} |I
9} |K
9}:(;7;
9~ v6
9~ vt
9~$|{
9~$|=
9~$v\
9~$v}
9~$v9
9~(v?
9~,|C
9~,|F
9~,~@
9~:U;Z;
9+:?:o:
9+:<:v:
9+:0:o:
9+:2:D:Y:v:
9+:4:~:
9+:5:O:&;
9+:5:O:d:
9+:Q:
9+909H9U9a9o9
9+9H9O9h9
9+9j9
9+9o9
9+9S9
9<:F:
9<9\9
9<9A9K9R9Y9`9g9n9u9|9
9<9D9`9h9p9x9
9<9D9H9L9T9X9\9`9
9<9D9L9T9
9<9D9L9T9\9d9l9t9|9
9<9D9L9T9\9h9p9
9<9H9h9
9<9I9]9b9r9w9
9<9t9
9=:}:
9=:J:{:
9=:q:
9=:V:l:
9=:W:
9=9G9`9y9
9>:T:`:v:
9>9O9
90:@:L:l:t:
90:h:
90:T:
90:X:
909\9
909=9I9Z9
909C9P9|9
909m9w9
909T9
90M0V0
91:6:w:
91:G:
91:i:
91:s:K;
91;A;Q;a;q;
91989E9Q9b9
919c9
919E9Q9]9i9
919H9m9
919I9R9l9q9
919K9Q9d9
919v9
919z9
92:{:
92:<:Q:f:
92:d:
92:r:
92;D>
929}9
929<9Q9f9
929D9]9m9x9
929I9R9\9r9v9
929N9[9g9x9
929V9
93:#;
93:::O:d:
93:@:a:
93:_:j:w:
93:=:
93:m:
93:N:X:
93:Q:
93:Q:]:i:u:
93:Q:e:q:}:
93:U:q:
93:X:b:
93;=;R;g;
939{9
939c9w9}9
939Q9]9i9u9
94:l:
94:X:
94;a;T<a<}<
94=F={=
949<9D9L9T9\9d9l9t9|9
949<9H9h9p9x9
94999[9h9
949D9P9X9
949D9P9X9x9
949N9i9
95:x:
959,:6:O:`:c<n<|<
959G9h9
959i9
96:n:
96:Q:b:
96:Q:s:
961c151d2e87f2686a955a9be24d316f1362bf21 3.9.1
969;9M9T9k9
969@9`9
969@9U9i9
969a9}9
969J9h9z9
969M9d9{9
969O:n:u:
97:M:
97:Z:}:
979p90:
98:?:T:i:z:
98:[:~:
98:<:@:D:
98:B:O:]:o:
98;E;
989@9H9P9X9`9h9t9
989@9H9T9t9|9
989@9L9l9t9|9
989]9j9
989D9d9l9t9
989D9d9l9t9|9
989E:|:
989K9d9z9
989p9
98uch
98uHj
98uOj
9-9:9^9
99:W:e:y:
9'9]9
9'9<9Q9n9
9-9>9c9
9-949J9\9g9m9w9
9-969<9
9'989j9
999D9c9
999F9X9c9
999q9
999v9
9'9B9L9\9|9
9'9E9O9{9
9'9k9
9'9p9
9'9Q9
99tgh
9A t&
9A:a:
9A9v9
9B:U:c:
9B;T;.<@<
9B;Z;q;
9B9Z9i9
9c:6;f<C=
9C:e:v:}<n=x=
9C:M:_:}:
9c:r:
9C9d:
9C9e9
9C9M9f9w9
9c9u:
9C9W9}9
9C9X9t9
9CDui
9D$,u_
9D$@r+
9D$<u
9D9L9P9T9\9`9d9h9
9D9y9Y:r:
9E:L:a:v:
9F$tD
9F(t&
9F:M:`:~:
9F9!:V<`<
9F9T9b9
9Fev[
9FXt&
9G:P:Y:d:j:p:y:
9g:q:
9G:Y:-;
9g9t9
9H:m:t:
9H:P:]:
9h|ND
9H9c9}9
9H9s9
9H9t9
9h9t9
9H9V9
9H9X9d9l9
9HLvB
9I:a:f:
9I:f:
9I9O9U9m9
9J$~^
9J9l9
9J9y9
9K(v%
9K,|,
9K,~2
9K\~2
9K<~2
9k9p9
9K9P9U9
9Kl~2
9KL~2
9L$,r
9L:a:s:{:
9L:o:
9L;Y;#<d<p<|<
9l9|9
9M;W;o;
9M>]GM
9N:~:
9N8tE8N
9N9_9.:5:a:r:
9N9~95:
9N9U9m9
9N9X9
9o<a=
9p u"
9P:k:
9P:u:
9P9X9`9h9p9|9
9pDt~h
9pHt*
9q u#
9q u(
9Q:s:
9q:v:
9Q@tgh
9Q9]:s:
9Q9^9j9{9
9R:w:
9R9~9
9s$~&Vj
9S:q:
9S;Z;a;
9s9x9
9S9y9
9S9z9
9T$ u
9t$(u
9t$(v^
9T$@u
9T;l;s;
9T9x9
9u(v'
9u(v&
9u(v\
9u(v7+M
9u(vW
9u,|-
9u,|.
9u,|^
9u,|>
9u,|c
9u4wn
9u4wt
9U9k9
9V:]:
9v:+;k;
9V:i:
9v:z:~:
9V9j9s9n:
9V9x9
9W9^9w9
9W9{9
9w9~9
9W9a9
9W9i9z9
9wLtQ
9X u+
9x:o=
9x@t{
9x`ti
9X9^9d9|9
9x9~9
9X9n9
9XDt[hT6G
9Y u%9_ u
9Y u:
9Y:`:y:
9Y:g:
9yDugh
9yHwG
9z:9;q;
9Z:w:
9Z9k9
9zLvr
A != nullptr && B != nullptr
A ;A$u
A + (M * K) <= A_end
A + (M * lda - (lda - K)) <= A_end
A 0-D bool tensor. If given, this will scale gradients by the inverse of frequency of the indices (words) in the mini-batch. Default  is ``False``
A 0-D scalar tensor. If specified, the entries at `padding_idx` do not contribute to the gradient; therefore, the embedding vector at `padding_idx` is not updated during training, i.e. it remains as a fixed pad.
A 0-D tensor containing a single value corresponding to the number diagonals above or the main diagonal to exclude or include.Default value is 0 if it's not specified.
A 1-D input tensor that is to be processed.
A 1-D INT64 tensor containing the the count of each element of 'uniques' in the input 'x'
A 1-D INT64 tensor of the same size as 'x' containing the indices for each value in 'x' in the output 'uniques'
A 1-D tensor of the same type as 'x' containing all the unique values in 'x' sorted in the same order that they occur in the input 'x'
A 1-D values of (height, width).
A 1-D values of (leftBorder, topBorder, rightBorder, bottomBorder).
A 2D Matrix that represents the distance between each pair of the two collections of inputs.
A collection of intercepts.
A collection of weights of the model(s).
A Conv/ConvTranspose node has both 'auto_pad' and 'pads' attributes
A dimension cannot be less than -1, got 
A dso with name 
A float.
A high-performing neural network activation function.The GELU nonlinearity is
A list of 2 (or 4 if bidirectional) activation functions for update, reset, and hidden gates. The activation functions must be one of the activation functions specified above. Optional: See the equations for default if not specified.
A list of 3 (or 6 if bidirectional) activation functions for input, output, forget, cell, and hidden. The activation functions must be one of the activation functions specified above. Optional: See the equations for default if not specified.
A list of floats.
A list of integers, along which to reduce. The default is to caculate along axes [0,2,3] for calculating mean and variance along each channel. Two variables with the same C-coordinate are associated with the same mean and variance.
A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor.
A list of integers, along which to reduce. The default is to reduce over all the dimensions of the input tensor. Accepted range is [-r, r-1] where r = rank(data).
A list of integers. By default, reverse the dimensions, otherwise permute the axes according to the values given.
A list of ints.
A list of labels.
A list of strings. One and only one of 'keys_*'s should be set.
A list of strings. One and only one of 'value_*'s should be set.
A node with a function body within a subgraph within another function body is currently not supported in ORT
A p#Q
A shape tensor must be a vector tensor.
A string indicating the desired element type of the output tensor, one of 'TO_FLOAT', 'TO_STRING', 'TO_INT64'.
A string to use when an input integer value is not found in the map.<br>One and only one of the 'default_*' attributes must be defined.
A string vocabulary array.<br>One and only one of the vocabularies must be defined.
A string.
A target node must be set.
A value that needs replacing.
A$+A 
A$G+A 
A(;A(
A,+A(
A,+A(j
A,+Q0+A(
A;M t<
A\+AX
a_scale
A_scale
A_zero_point
a_zero_point
A`+A\j
A|+Ax
A|+AxjtY
A|+AxVW
A0;A4uM
A0R0[0a0
A4@9E4
A8;A<
A8;A<tB
A8;A<tT
A8+A4
A8+A4P
A88QP
ACLExecutionProvider
Acosh
AcquireSRWLockExclusive
AcquireSRWLockShared
across_channels
activation
activation and leaky_relu_alpha.
Activation functions:
activation.
activation_
activation_alpha
activation_beta
activation_func_names.size() == static_cast<size_t>(num_directions_) * 2
activation_func_names.size() == static_cast<size_t>(num_directions_) * 3
activation_gamma
activation_params
activation_params count mismatch
activation_size
ActivationDescCount
ActivationDescs
activations
activations, avoiding computation if the input is invalid (as in, the
activations_.size() == static_cast<size_t>(num_directions)
adapterLuidHighPart
adapterLuidLowPart
add_B_tensor_proto
Added in transpose optimizer
AddFoldedRange recurses too much.
Adding default CPU execution provider.
AddInitializedTensor already has tensor with name 
addition
Additional elements added to the side with higher coordinate indices in the output. Each padding value in "output_padding" must be less than the corresponding stride/dilation dimension. By default, this attribute is a zero vector. Note that this attribute doesn't directly affect the computed output values. It only controls the selection of the computed values, so changing this attribute only adds or removes output elements. If "output_shape" is explicitly provided, "output_padding" does not contribute additional size to "output_shape" but participates in the computation of the needed padding amount. This is also called adjs or adjustment in some frameworks.
address family not supported
address in use
address not available
Adlam
Af98u
Affine
affine
Affine takes one input data (Tensor<T>) and produces one output data
aggregate_function
Ah\QH
Ah`QH
Ah+Ad
ai.onnx
ai.onnx.ml
ai.onnx.preview.training
ai.onnx.training
Al+Ah
align_corners
AlignRegionsToCorners
All implicit inputs should have OrtValue instances by now. 
All inputs must have the same shape
All inputs to Concat must have same rank
All inputs to 'Range' op must be of the same type
All nodes have been placed on [
All scan outputs MUST be tensors
All Tensor and Sequence types
All Tensor types
All Tensor, Sequence, and optional types
all types
Allocated memory at 
Allocation of memory pattern buffer for 
Allocation of tensor types requires a shape.
allocator
allocator != nullptr
Allocator already registered for 
allocator_ != nullptr
allocator_for_caching.get() != nullptr
allocator_ptr_
AllocPlan(ml_value_idx).program_counter.Ends().back() == program_counter
ALLOW_RELEASED_ONNX_OPSET_ONLY
Allowed values are 'half_pixel' and 'output_half_pixel'. Use the value 'half_pixel' to pixel shift the input coordinates by -0.5 (the recommended behavior). Use the value 'output_half_pixel' to omit the pixel shift for the input (use this for a backward-compatible behavior).
allowed_activations.find(activations_[direction]) != allowed_activations.end()
allowed_directions.find(direction_) != allowed_directions.end()
allowzero
Alpha
alpha
alpha == 1.0f && (beta == 0.0f || beta == 1.0f)
alpha_ > 0.0f
already connected
An allocator for this device has already been registered for sharing.
An attribute specifying the number of scan_inputs M. 
An axes tensor must be a scalar or a 1-D tensor.
An axes tensor must be a vector tensor.
An axis tensor must be a scalar tensor.
An input tensor to hash.
An integer to use when an input string value is not found in the map.<br>One and only one of the 'default_*' attributes must be defined.
An integer vocabulary array.<br>One and only one of the vocabularies must be defined.
An integer.
An optional list of K flags, one for each scan_output. The i-th element of the list specifies whether the i-th scan_output should be constructed by appending or prepending a new value in each iteration: 0 indicates appending and 1 indicates prepending. If omitted, all scan_output tensors will be produced by appending a value in each iteration.
An optional list of K flags. The i-th element of the list specifies the axis for the i-th scan_output. The scan outputs are accumulated along the specified axis. If omitted, 0 will be used as the scan axis for every scan_output.
An optional list of K flags. The i-th element of the list specifies the axis for the i-th scan_output. The scan outputs are accumulated along the specified axis. If omitted, 0 will be used as the scan axis for every scan_output. Negative value for an axis means counting dimensions from the back. Accepted range is [-r, r-1].
An optional list of M flags. The i-th element of the list specifies the axis to be scanned (the sequence axis) for the i-th scan_input. If omitted, 0 will be used as the scan axis for every scan_input.
An optional list of M flags. The i-th element of the list specifies the axis to be scanned (the sequence axis) for the i-th scan_input. If omitted, 0 will be used as the scan axis for every scan_input. Negative value for an axis means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
An optional list of M flags. The i-th element of the list specifies the direction to be scanned for the i-th scan_input tensor: 0 indicates forward direction and 1 indicates reverse direction. If omitted, all scan_input tensors will be scanned in the forward direction.
an optional list of strings attribute that contains a list of separators - regular expressions to match separators Two consecutive segments in X connected by a separator would be divided into two tokens. For example, if the input is "Hello World!" and this attribute contains only one space character, the corresponding output would be ["Hello", "World!"]. To achieve character-level tokenization, one should set the 'separators' to [""], which contains an empty string.
An optional string. Token's regular expression in basic POSIX format (pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap09.html#tag_09_03). If set, tokenizer may produce tokens matching the specified pattern. Note that one and only of 'tokenexp' and 'separators' should be set.
An OrtValue for this name has already been added.
An split tensor must be a vector tensor.
aN42J
Anatolian_Hieroglyphs
and the running statistics in training mode (training_mode=True).
Another operand has a dim value of 
Ap;Ahst
Ap+Al
api-ms-win-core-com-l1-1-0.dll
api-ms-win-core-debug-l1-1-0.dll
api-ms-win-core-errorhandling-l1-1-0.dll
api-ms-win-core-fibers-l1-1-0.dll
api-ms-win-core-file-l1-1-0.dll
api-ms-win-core-file-l1-2-0.dll
api-ms-win-core-handle-l1-1-0.dll
api-ms-win-core-heap-l1-1-0.dll
api-ms-win-core-heap-l2-1-0.dll
api-ms-win-core-interlocked-l1-1-0.dll
api-ms-win-core-libraryloader-l1-2-0.dll
api-ms-win-core-libraryloader-l1-2-1.dll
api-ms-win-core-localization-l1-2-0.dll
api-ms-win-core-memory-l1-1-0.dll
api-ms-win-core-path-l1-1-0.dll
api-ms-win-core-processenvironment-l1-1-0.dll
api-ms-win-core-processthreads-l1-1-0.dll
api-ms-win-core-processthreads-l1-1-1.dll
api-ms-win-core-processthreads-l1-1-3.dll
api-ms-win-core-processtopology-obsolete-l1-1-0.dll
api-ms-win-core-profile-l1-1-0.dll
api-ms-win-core-rtlsupport-l1-1-0.dll
api-ms-win-core-string-l1-1-0.dll
api-ms-win-core-synch-l1-1-0.dll
api-ms-win-core-synch-l1-2-0.dll
api-ms-win-core-sysinfo-l1-1-0.dll
api-ms-win-core-sysinfo-l1-2-0.dll
api-ms-win-core-util-l1-1-0.dll
api-ms-win-crt-locale-l1-1-0.dll
api-ms-win-crt-math-l1-1-0.dll
api-ms-win-crt-private-l1-1-0.dll
api-ms-win-crt-runtime-l1-1-0.dll
api-ms-win-crt-string-l1-1-0.dll
api-ms-win-eventing-provider-l1-1-0.dll
apply softmax to elements for dimensions softmax_axis or higher
APQRV
APQVW
AQQQh
AQQQhh
AQQQhL
AQQQht
AQQQhT
AQQQhX
AQQQj
AQQQQh
AQQQQSQh$
AQQQQSQhh
AQQQQVQh
AQQQQVQh<uH
AQQQQVQhh
AQQQSWQh\
AQQQV
AQQQVh
AQQQVh@
AQQQVQhT
AQQQWh
AQQQWQh
AQQQWSQh\
Arabic
arena_extend_strategy
arg_num < arg_counts.size()
ArgMax
ArgMin
Argument is not a tensor
argument list too long
Argument mismatch when removing edge.
argument out of domain
Argument type mismatch when adding edge.
Armenian
ArmNNExecutionProvider
array
Array of sequence lengths.  len(seq_lengths) should equal batch size N.
ArrayFeatureExtractor
as.,k{n?,
AScaleTensor
Asinh
asymmetric
At least one element in the sequence is of a type different from others.
At least one output should be requested.
At least two inputs are needed, and each input must be (tensor, scale, zero_point) tuple!
At most one dimension can be -1.
At;Al|
AT+AP
aT<,(?
Atanh
ATensor
Attempt to retrieve final output before it was set.
Attempt to use DefaultLogger but none has been registered.
Attempting to broadcast an axis by a dimension other than 1. 
Attempting to get an input that does not exist.
Attempting to get an output that does not exist.
Attempting to get index by a name which does not exist:
Attention
Attention cannot have past sequence and extra add qk
Attention layer weight shape error! Expected: {
Attention mechanism memory sequence lengths must have shape {
Attention mechanism memory sequence lengths value must in (0, 
Attention mechanism memory shape error! Expected: {
Attention memory layer weight shape error! Expected:{
Attention query layer weight shape error! Expected:{
Attention v weight shape error! Expected:{
AttentionFusion
Attibute name and type don't match
AttnLSTM
attr->has_tp()
Attribute 
Attribute '
Attribute (name: 
Attribute `broadcast=1` needs to be passed to enable broadcasting.
Attribute axes has incorrect length
Attribute blocksize is not set.
Attribute border needs to be specified with four border elements, got 
attribute case_change_action has invalid value
attribute case_change_action is not set
Attribute dilations has incorrect size
Attribute dtype should be of integer type and specify a type.
Attribute expected to have a one-dim sparse tensor
Attribute expected to have a one-dim tensor
Attribute expected to have tensor or sparse tensor type
attribute is_case_sensitive is not set
Attribute kernel_shape has incorrect size
Attribute kernel_shape has incorrect size.
Attribute kernel_shape must be specified
Attribute kernel_shape must be specified.
attribute mark is not set
attribute mincharnum is not set
attribute mincharnum must have a positive value
Attribute name and type don't match for '
attribute pad_value is not set
Attribute pads has incorrect length
Attribute pads has incorrect size
Attribute pads has incorrect size.
Attribute perm of Transpose has an invalid value. Value 
Attribute pooled_shape has incorrect length
Attribute pooled_shape must be specified
Attribute 'scales' is required.
Attribute 'scales' must have floats type.
Attribute shape is not set.
Attribute specification type mismatch.
Attribute strides has incorrect size
Attribute strides has incorrect size.
Attribute to is not set.
Attribute 'type' should be a TypeProto and it should specify a type.
Attribute value for pads is required
Attribute 'value' of Constant node must exist with 'Tensor' data.
Attribute 'value_float' expect a float.
Attribute 'value_floats' expect a list of floats.
Attribute 'value_int' expect an integer.
Attribute 'value_ints' expect a list of integers.
Attribute 'value_string' expect a string.
Attribute 'value_strings' expect a list of strings.
Attribute: 
auto_pad
auto_pad == AutoPadType::NOTSET
auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that `output_shape[i] = ceil(input_shape[i] / strides[i])` for each axis `i`. The padding is split between the two sides equally or almost equally (depending on whether it is even or odd). In case the padding is an odd number, the extra padding is added at the end for SAME_UPPER and at the beginning for SAME_LOWER.
auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that `output_shape[i] = input_shape[i] * strides[i]` for each axis `i`. The padding is split between the two sides equally or almost equally (depending on whether it is even or odd). In case the padding is an odd number, the extra padding is added at the end for SAME_UPPER and at the beginning for SAME_LOWER.
auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that the output size match the input.In case of odd number add the extra padding at the end for SAME_UPPER and at the beginning for SAME_LOWER. VALID mean no padding. DEPRECATION NOTE: auto_pad is only intended to support legacy uses, and for framework authors, one is explicitly encouraged to use explicit padding specified in the pads attribute.
auto_pad must be either NOTSET, SAME_UPPER, SAME_LOWER or VALID. Where default value is NOTSET, which means explicit padding is used. SAME_UPPER or SAME_LOWER mean pad the input so that the output spatial size match the input.In case of odd number add the extra padding at the end for SAME_UPPER and at the beginning for SAME_LOWER. VALID mean no padding.
Available memory of 
AVERAGE
AveragePool
Avestan
Ax+Atj
aX2is
axes as an input and attribute cannot be specified at the same time.
'axes' attribute must not contain any duplicates
'axes' has a duplicate axis
'axes' has an axis outside of the tensor dimension count
'axes' has an out of range axis
'axes' has duplicates
Axes input is null
Axes that `starts` and `ends` apply to. It's optional. If not present, will be treated as [0, 1, ..., len(`starts`) - 1].
axes_right_stride >= 0 && static_cast<uint64_t>(axes_right_stride) < std::numeric_limits<size_t>::max()
axes_tensor != nullptr
axes_tensor->Shape().NumDimensions() == 0 || axes_tensor->Shape().NumDimensions() == 1
axes_tensor->Shape().NumDimensions() == 1
axis 
axis <= X_NumDims && axis >= -X_NumDims
axis == 1 || axis == largest
axis >= -tensor_rank && axis <= tensor_rank - 1
Axis has less than the requested k elements.
'axis' must be in [
axis must be in [-r, r-1]
'axis' must be in [-rank(indices), rank(indices)-1]
'axis' must be in [-rank(indices)-1, rank(indices)]
axis must be in [-rank, rank)
axis must be in [-rank, rank-1].
axis must be in [-rank, rank-1]. input rank was 
Axis must be within range [
axis tensor can be int32 or int64 only
Axis tensor must be provided to the CumSum op
Axis tensor should be 0D or 1D
Axis tensor should be of type `int32_t` or `int64_t`
axis_tensor->Shape().IsScalar()
Axis1D = Constant()
AxisCount
AxisDirection
aYe9m
AZeroPointTensor
B + (N * ldb - (ldb - K)) <= B_end
B QPR
B!LJ7
B(90t:
B(9J,u
b[|=x
b_scale
B_scale
b_scale_shape.NumDimensions() == 0 || (b_scale_shape.NumDimensions() == 1 && (b_scale_shape[0] == 1 || b_scale_shape[0] == helper.N()))
b_scale_shape.NumDimensions() == b_zp_shape.NumDimensions() && (b_scale_shape.NumDimensions() == 0 || (b_scale_shape[0] == b_zp_shape[0]))
b_zero_point
B_zero_point
b_zp_shape.NumDimensions() == 0 || (b_zp_shape.NumDimensions() == 1 && (b_zp_shape[0] == 1 || b_zp_shape[0] == helper.N()))
B|$\H
B|+Bx
B0[0~0
B09J4u
B2D = Flatten <axis=0> (B)
b33f88f7-c464-43e3-8692-97ac832bb14a
b33fd0fa-cd7b-4b10-ae5a-df64cabfe1f8
bad address
bad allocation
Bad args: nsubmatch=
bad array new length
Bad call to ParseState::ParsePerlFlags
bad cast
bad conversion
bad dimensions
bad exception
bad file descriptor
Bad final char: 
bad function call
Bad hex digit 
bad locale name
bad message
Bad node spec for node. Name: 
Bad optional access
Bad reference count 
bad repetition operator
bad variant access
Balinese
Bamum
Base values for classification, added to final class score; the size must be the same as the classes or can be left unassigned (assumed 0)
base_values
Bassa_Vah
Batak
Batch dimension should match for MatMul;
batch_axis
batch_axis != time_axis
batch_axis < 2
batch_dims
batch_indices
batch_indices shape input tensor has wrong dimension
batch_size is < 1
BatchDimensionCount
BatchIndicesTensor
BatchNormalization
BatchNormalizationAddFusion
BatchNormalizationMulFusion
Batchwise recurrent operations (layout == 1) are not supported. If you need support create a github issue with justification.
Begin execution
Bengali
Bernoulli
beta is expected to have 1 dimension, got 
beta is expected to have 1 dimensions, got 
beta is expected to have size of 
Beta scale must be a scalar or 1D tensor of size 1
Beta should be of shape (hidden_size). 
beta should have 1 dimension, dimension size known, and same hidden size as word_embedding.
Beta zero point must be a scalar or 1D tensor of size 1
beta_ > 0.0f
beta_quant
beta_scale
beta_zero_point
Beta1
Beta2
bfloat16
BH;AH
Bhaiksuki
BhPVWQQ
Bias applied to each channel, same size as C.
Bias Gelu.
bias is expected to have 1 dimension, got 
Bias size (
Bias tensor.
BiasDropout
BiasDropoutFusion
Biased = Add (Scaled, B2D)
Biased = Identity (Scaled)
BiasGelu
BiasGeluFusion
BiasSoftmax
BiasSoftmaxFusion
BiasTensor
bicubic
bidirectional
BifurcationDetector
bilinear
BILINEAR
Binarizer
binary
BinForSize(bin_size * 2 - 1) == BinFromIndex(b)
BinForSize(bin_size * 2) != BinFromIndex(b)
BinForSize(bin_size + 255) == BinFromIndex(b)
BinForSize(bin_size) == BinFromIndex(b)
BinFromIndex(c->bin_num)->free_chunks.erase(h) > 0
BitShift
Blocks of [blocksize, blocksize] are moved.
BlockSize
blocksize
Blocksize must be positive
bn_B_tensor_proto
bn_mean_tensor_proto
bn_scale_tensor_proto
bn_var_tensor_proto
Bool to determine if hidden state is zeroes or passed along for timesteps past the given sequence_length.
boolean
Boolean whether to mark the beginning/end character with start of text character (0x02)/end of text character (0x03).
Boolean. Indicates whether upper or lower part of matrix is retained. Default is true.
Boolean. Whether the identification of stop words in X is case-sensitive. Default is false
Bopomofo
border
'Border' attribute must be present and must contain exactly 4 values - (left_border, top_border, right_border, bottom_border)
Both `data` and `indices` input tensors in GatherND op need to have rank larger than 0.
Both attributes isinf_only and isnan_only cannot be set. Unset both to check for both conditions.
both data and indices tensor need to have rank larger than zero.
boxes
boxes and scores should have same num_batches.
boxes and scores should have same spatial_dimension.
boxes must be a 3D tensor.
boxes_tensor
Bp+Bl
BQRh,sH
Brahmi
Braille
BRANCH_EQ
BRANCH_GT
BRANCH_GTE
BRANCH_LEQ
BRANCH_LT
broadcast
broadcast bias across input for dimensions broadcast_axis to softmax_axis-1
Broadcast Output range [
broadcast_axis
BroadcastLooper requires two tensors as input.
broken pipe
BRRQVh
BRRQVh|
BScaleTensor
BTensor
Buffer containing the initializer must be owned by the user.
buffer size is too small for string element
buffers_.find(location) == buffers_.end()
buffers_.size() == buffer_sizes_.size()
Buginese
Buhid
bumped the operator version but 
by either re-generating the model with latest exporter/converter 
bytemap range 
BZeroPointTensor
C + (M * ldc - (ldc - N)) <= C_end
C = ((A - A_zero_point) * (B - B_zero_point)) * (A_scale * B_scale)/C_scale + C_zero_point
C = (A_scale * (A - A_zero_point) + B_scale * (B - B_zero_point))/C_scale + C_zero_point
C(;C(u@
C,;C,
C,+C(
C;_,|
C@+C<
C\+CX
C\p#Q
C\QWR
C_scale
c_shape != nullptr
c_shape is required if c_data is provided
C_zero_point
C`Php#Q
C|+Cx
C|p#Q
C++/WinRT version:2.0.201201.7
C<+C8
c->in_use() && (c->bin_num == kInvalidBinNum)
C0n0u0
C0o0z0
C0Php#Q
C0PRj
c2->prev == h1
C4+C0
C8;C<t
C8+C4
C8F+C4
C8PQW
caDyE
CallContext:[%hs] 
called_ == 1
cAMDt
Can broadcast 0 by 0 or 1. 
Can not digest separators: 
Can not digest tokenexp: 
Can not find the execution provider 
Can not find the node 
Can not get shape initializer data!
Can not multiply A and B as inner dimension does not match. inner_A: 
Can not use strings in pre-allocated memory. Use CreateSparseTensorAsOrtValue() to allocate memory inside and copy
Can only add a new input at the end of the current ones.
Canadian_Aboriginal
Candidate for fallback CPU execution: 
candidate_output.Shape().Size() == output_shape.Size()
candidate_output_dims[iter] == 1
Cannot apply CumSum operator on a scalar
cannot compare iterators of different containers
Cannot concatenate scalars
cannot find allocator
Cannot find missing input: 
Cannot find NodeArgs for [
cannot get value
Cannot infer type and shape for function
Cannot parse data from external tensors. Please 
Cannot parse the diagonal elements along dims 
Cannot replace concat node with initializer:
Cannot reshape initializer 
Cannot scale 0 by any factor to generate a non-zero value. 
Cannot slice scalars
Cannot split using values in 'split' attribute. Axis=
cannot use at() with 
Cannot use 'edge' mode to pad dimension with a value of 0. Input shape:
cannot use erase() with 
cannot use key() for non-object iterators
Cannot use 'reflect' mode to pad dimension with a value of 0. Input shape:
Cannot use SearchOnePass for unanchored matches.
Cannot use the same name as both a subgraph initializer and subgraph input: 
Cannot use user supplied initializer with name: (
Can't 
can't constant fold 
Can't find node with index 
Can't happen
Can't merge shape info. Both source and target dimension have values but they differ. Source=
Can't reduce on dim with value of 0 if 'keepdims' is false. Invalid output shape would be produced. input_shape:
Can't remove node 
Can't slice a non-tensor OrtValue. Type was 
Can't use func with null ptr
Carian
Carries out batch normalization as described in the paper
Case not handled in ComputeSimple: 
case_change_action
cast != nullptr
Cast Input from int64 to int32
Cast mask from int64 to int32
cast node to cast from float16 to float32 on cpu
cast output of layer norm
cast scale of layer norm
Cast_Scale
cast_to
CastElimination
CastFloat16Transformer
CastLike
CastMap
CategoryMapper
cats_int64s
cats_strings
Caucasian_Albanian
Caught exception while destructing CustomOpsLoader with message: 
Caught exception while loading custom ops with message: 
CblasNoTrans Unexpected CBLAS_TRANSPOSE for TransB of 
CblasTrans Unexpected CBLAS_TRANSPOSE for TransB of 
CD;0|<
CD;CHt
Cd;Cht
CDist
CdPRj
CDWVj
ceil_mode
Cell clip threshold. Clipping bounds the elements of a tensor in the range of [-threshold, +threshold] and is applied to the input of activations. No clip if not specified.
CellMemInitTensor
center_point_box
center_point_box only support 0 or 1
CF;t$
Ch+C`
Ch4_E
Chakma
ChannelCount
channels_last
Char embedding size does not match char_embedding_size attribute.
Char embedding size does not match conv kernal size 2.
char_embedding_size
CHECK failed: !is_closed_: 
CHECK failed: (backup_bytes_) == (0): 
CHECK failed: (buffer_used_) == (buffer_size_): 
CHECK failed: (count) <= (buffer_used_): 
CHECK failed: (count) >= (0): 
CHECK failed: (index) < (current_size_): 
CHECK failed: (index) >= (0): 
CHECK failed: (min_bytes) <= (std::numeric_limits<size_t>::max() - SerialArena::kBlockHeaderSize): 
CHECK failed: (static_cast<int64>(new_size)) <= (static_cast<int64>((std::numeric_limits<size_t>::max() - kRepHeaderSize) / sizeof(old_rep->elements[0]))): 
CHECK failed: backup_bytes_ == 0 && buffer_.get() != NULL: 
CheckNodesInPathK returns false
CheckNodesInPathQ returns false
CheckNodesInPathV return false
CheckSliceParameters return false
CheckSliceParameters return false for slice2
CheckSliceParameters returns false for last_slice
CheckSliceParameters returns false for mask_slice
CheckSliceParameters returns false for slice1
checksum
Cherokee
Child node if expression is false
Child node if expression is false.
Child node if expression is true
Child node if expression is true.
Chorasmian
Chosen support vectors
Chttp://www.microsoft.com/pkiops/crl/MicWinProPCA2011_2011-10-19.crl0a
chxED
CL;CL_^
Class labels if using integer labels.<br>One and only one of the 'classlabels_*' attributes must be defined.
Class labels if using string labels.<br>One and only one of the 'classlabels_*' attributes must be defined.
Class labels when using integer labels. One and only one 'classlabels' attribute must be defined.
Class labels when using string labels. One and only one 'classlabels' attribute must be defined.
class_ids
class_nodeids
class_treeids
class_weights
classes.size() == 2 || classes.size() == 1
classes_strings
classlabels_int64s
classlabels_ints
classlabels_strings
classlabels_strings_.empty() ^ classlabels_int64s_.empty()
classlabels_strings_.size() > 0 || classlabels_ints_.size() > 0
clip_ > 0.f
ClipThreshold
close() failed: 
CloseHandle
CoalesceWalker::ShortVisit called
CoCreateFreeThreadedMarshaler
code != static_cast<int>(common::OK)
Coefficient of ELU default to 1.0.
Coefficient of ELU.
Coefficient of leakage default to 0.01.
Coefficient of leakage.
Coefficient of SELU default to 1.0507.
Coefficient of SELU default to 1.05070102214813232421875 (i.e., float32 approximation of 1.0507009873554804934193349852946).
Coefficient of SELU default to 1.6732.
Coefficient of SELU default to 1.67326319217681884765625 (i.e., float32 approximation of 1.6732632423543772848170429916717).
coefficients
coefficients_.size() > 0
com.microsoft
com.microsoft.
com.microsoft.dml
com.microsoft.experimental
com.microsoft.nchwc
com.microsoft.QLinearAdd
com.microsoft.QLinearAveragePool
com.microsoft.QLinearConcat
com.microsoft.QLinearGlobalAveragePool
com.microsoft.QLinearLeakyRelu
com.microsoft.QLinearMul
com.microsoft.QLinearReduceMean
com.microsoft.QLinearSigmoid
combase.dll
Common
CommonSubexpressionElimination
CompanyName
CompareStringEx
Compiled kernel hashes must be provided
compiled_kernel_hashes != nullptr
Compiler::Copy called!
complex128
complex64
ComplexMul
ComplexMulConj
Compress
Compute_
compute_info_->create_state_func(&context, &func_state_) == 0
Computed size: 
ComputePad: pad type not supported.
Computes an one-layer GRU. This operator is usually supported via some custom
Computes an one-layer LSTM. This operator is usually supported via some
Computes an one-layer simple RNN. This operator is usually supported
Computes the indices of the {name} elements of the input tensor's element along the
Concat
concat first input value is not -1
Concat of 
concat_after_gather does not have expected number of inputs or output edges
concat_after_gather input 2 does not have expected value
concat_result
ConcatFromSequence
Concretely, given the (fused) inputs X (TxNxD), the previous hidden
condition
ConditionTensor
Config key is empty or longer than maximum length 128
Config value is longer than maximum length 1024
Config with key [
Conflicting free dimension overrides.
connection aborted
connection already in progress
connection refused
connection reset
const_ignore_index
const_one
const_one_casted
const_one_float
const_transpose_optimizer
const_zero
const_zero_casted
const_zero_float
const_zero_target_typed
Constant
constant
Constant initializer NodeArg shape should not be null. NodeArg: 
constant_value
ConstantFill
ConstantFolding
ConstantOfShape
Constrain bias type to 32-bit integer tensor.
Constrain filter type to 8-bit integer tensor.
Constrain index tensor to int64
Constrain indice type to int32 or int64
Constrain indices to integer types
Constrain input a and its zero point data type to 8-bit integer tensor.
Constrain input A and its zero point types to 8 bit tensors.
Constrain input A data type to 8-bit integer tensor.
Constrain input A data types as 16-bit integer tensor
Constrain input A, b_scale and output Y data type as float tensor.
Constrain input a_scale, b_scale and output Y data type as float tensor.
Constrain input and output  types to float tensors.
Constrain input and output float tensors types.
Constrain input and output integer tensors types
Constrain input and output to all tensor types.
Constrain input and output types (except mean and inv_std_var) to float tensors.
Constrain input and output types to 8 bit signed and unsigned tensors.
Constrain input and output types to 8 bit tensors.
Constrain input and output types to all numeric tensors and bool tensors.
Constrain input and output types to all numeric tensors.
Constrain input and output types to all numerical tensor types.
Constrain input and output types to all tensor and sequence types.
Constrain input and output types to all tensor types (including bfloat).
Constrain input and output types to all tensor types.
Constrain input and output types to all tensor, sequence, and optional types.
Constrain input and output types to all tensors.
Constrain input and output types to any tensor type.
Constrain input and output types to float and 8 bit tensors.
Constrain input and output types to float and float16 tensors.
Constrain input and output types to float or half tensors.
Constrain input and output types to float tensors
Constrain input and output types to float tensors.
Constrain input and output types to float/int tensors.
Constrain input and output types to float32 tensors.
Constrain input and output types to floating-point tensors.
Constrain input and output types to high-precision and 8 bit numeric tensors.
Constrain input and output types to high-precision numeric tensors.
Constrain input and output types to int8 tensors.
Constrain input and output types to integer tensors.
Constrain input and output types to numeric tensors.
Constrain input and output types to signed numeric tensors.
Constrain input and output types to singed/unsigned int8 tensors.
Constrain input and output types.
Constrain input b and its zero point data type to 8-bit integer tensor.
Constrain input B and its zero point types to 8 bit tensors.
Constrain input B data type to 8-bit integer tensor.
Constrain input B data types as 16-bit integer tensor
Constrain input C to 32 bit integer tensors.
Constrain input 'ratio' types to float tensors.
Constrain input type to 8-bit integer tensor.
Constrain input type to unsigned or signed 32-bit integer tensor, or string tensor. It should be utf-8 encoded if using unicode.
Constrain input types and output Y type to float tensors.
Constrain input types to 8 bit signed and unsigned tensors.
Constrain input types to all tensor types.
Constrain input types to any tensor type.
Constrain input types to common numeric type tensors.
Constrain input types to float tensors.
Constrain input types.
Constrain input types. Casting from complex is not supported.
Constrain input types. Casting from strings and complex are not supported.
Constrain input types. Strings and complex are not supported.
Constrain input w and its zero point data type to 8-bit integer tensor.
Constrain input x and its zero point data type to 8-bit integer tensor.
Constrain input X and output types to float/int tensors.
Constrain input 'X' and output 'Y' to all tensor types.
Constrain input Y types to float/int tensors.
Constrain input, weight, and output types to floating-point tensors.
Constrain input0 and output types to float tensors
Constrain key_padding_mask to bool tensors.
Constrain mask index to integer types
Constrain mean and inv_std_var to be float tensors.
Constrain mean and inv_std_var to float tensors.
Constrain mean and variance types to float tensors.
Constrain mean and variance types to float tensors. It allows all float type for U.
Constrain output data type to 32-bit integer tensor.T2 must be tensor(uint32) when T1 is tensor(uint8),or must be tensor(int32) when T1 is tensor(int8).
Constrain output mask types to boolean tensors.
Constrain output 'mask' types to boolean tensors.
Constrain output to int64 tensor, which should be a scalar though.
Constrain output to int64 tensor.
Constrain output to integral tensor. It must be a scalar(tensor of empty shape).
Constrain output type to 8-bit integer tensor.
Constrain output type to all tensor or sequence types.
Constrain output type to float32 or 8 bit tensors.
Constrain output type to unsigned and signed 32-bit integer tensor.
Constrain output types to 32 bit tensors.
Constrain output types to all numeric tensors and bool tensors.
Constrain output types to all tensor types.
Constrain output types to any tensor type.
Constrain output types to be numerics.
Constrain output types to bool, int32, int64, float16, float, double tensors.
Constrain output types to boolean tensors.
Constrain output types to float tensors.
Constrain output types to integral tensors.
Constrain output types. Casting to complex is not supported.
Constrain output types. Casting to strings and complex are not supported.
Constrain output types. Strings and complex are not supported.
Constrain output y and its zero point data type to 8-bit integer tensor.
Constrain output Y data type as 32-bit integer tensor.
Constrain output y data type to 32-bit integer tensor.
Constrain output Y data types as 32-bit integer tensor.T3 must be tensor(uint32) when both T1 and T2 are tensor(uint16),or must be tensor(int32) when either T1 or T2 is tensor(int16).
Constrain output zero point types to 8 bit tensors.
Constrain position to integral tensor. It must be a scalar(tensor of empty shape).
Constrain repeat's type to int64 tensors.
Constrain roi type to float or double.
Constrain scale and bias types to float tensors.
Constrain scale types to any float tensor type.
Constrain scale types to float tensors.
Constrain scores input and output types to float tensors.
Constrain seq_lens to integer tensor.
Constrain seq_lens to integral tensors.
Constrain split size to integral tensor.
Constrain target to integer types
Constrain the output to a boolean tensor.
Constrain tiles and axis's type to int64 tensors.
Constrain to all fixed size tensor and sequence types. If the dtype attribute is not provided this must be a valid output type.
Constrain to all tensor types.
Constrain to any tensor type.
Constrain to any tensor type. If the dtype attribute is not provided this must be a valid output type.
Constrain to boolean tensors.
Constrain to integer types
Constrain to integer types.
Constrain to tensor(float).
Constrain to tensor(int32).
Constrain types to float tensors.
Constrain types to int tensors.
Constrain weights types to 8 bit tensors.
Constrain 'x' and 'x_zero_point' to 8-bit integer tensors.
Constrain 'x' to float or int32 tensor.
Constrain 'x' to float tensor.
Constrain 'x', 'y_scale' to float tensors.
Constrain 'x_zero_point' and 'x' to 8-bit/32-bit integer tensor.
Constrain 'y', 'x_scale' to float tensors.
Constrain 'y_zero_point' and 'y' to 8-bit integer tensor.
Constrain 'y_zero_point' and 'y' to 8-bit integer tensors.
Constrain 'y_zero_point' and 'y' to 8-bit unsigned integer tensor.
Constrains input and output to only numeric types.
Constrains input to boolean tensor.
Constrains input to float tensors.
Constrains input to integral tensors.
Constrains input to only numeric types.
Constrains input type to all tensor and sequence types.
Constrains input type to optional tensor and optional sequence types.
Constrains input types to all numeric tensors.
Constrains input/output to boolean tensors.
Constrains output to a boolean tensor.
Constrains output to boolean tensor.
Constrains output type to all optional tensor or optional sequence types.
Constrains to boolean tensors.
consumed_inputs
Container for generated shape data cannot be nullptr when enable_data_propagation option is set.
context does not contain text
Conv filter size does not match embedding_size attribute.
Conv kernal size 1 does not match conv_window_size attribute .
conv_B_tensor_proto
conv_W_tensor_proto
conv_window_size
ConvActivationFusion
ConvAddFusion
ConvAddFusion_Add_B_
ConvAddFusion_B_
ConvBNFusion
ConvBnFusion_BN_B_
ConvBnFusion_W_
Conversion Error
ConvInteger
ConvMulFusion
ConvMulFusion_Mul_B_
ConvMulFusion_W_
ConvTranspose
ConvTransposeWithDynamicPads
COO indices must be 2-D, got: 
COO k index: 
COO m index: 
coordinate_transform_mode:[
coordinate_transformation_mode
Coptic
Copy from/to host memory
copy_info.size() == num_feeds
CoreMLExecutionProvider
corrupted protobuf data: tensor shape size(
CoTaskMemAlloc
Could not finalize session options while constructing the inference session. Error Message: 
Could not find a CPU kernel and hence 
Could not find an implementation for 
Could not find chunk in bin
Could not find OrtValue with idx '
Could not find OrtValue with name '
Could not find Region for 
Could not find Region for: 
Could not infer data type from input tensor with data type 
Could not parse model successfully while constructing the inference session
Could not write a profile because no model was loaded.
count == 1
count_include_pad
counter.current_offset == last
counts
Couple the input and forget gates if 1, default 0.
Couple the input and forget gates if 1.
CoupleInputForget
Cp_^[
CP+CL
CPp#Q
CPUExecutionProvider
Create_State_
Created a new Cast node to interchange Cast and Transpose nodes
Created a new Transpose node to interchange Cast and Transpose nodes
CreateDirectoryA
CreateDirectoryW
CreateDXGIFactory2
CreateEventW
CreateFeedsFetchesManager must be called prior to execution of graph.
CreateFile2
CreateMutexExW
CreateSemaphoreExW
Creating 
Creating and using per session threadpools since use_per_session_threads_ is true
Creating BFCArena for 
Crop and image to the specified spatial dimensions. If scale is given,
crop_size
crop_size shape input tensor has wrong dimension
CropAndResize
cross device link
CrossChannel
CShpnH
CSSSSWQhh
Ct+Cl
CTensor
CTp#Q
cubic
'Cubic' mode only support 2-D inputs ('Bicubic') or 4-D inputs with the corresponding outermost 2 scale values being 1 in the 
cubic_coeff_a
CUDA and/or ROCM execution provider is either not enabled or not available.
CUDAExecutionProvider
cudaMalloc
CudaPinned
CumSum
Cuneiform
cur + size <= end
cur_index == &*indices_data.cend()
cur_input == end_input || cur_input->first >= 0
cur_iteration_ < num_iterations_
cur_out == end_out
cur_tokens
cur1 == end1
current <= buffer_size_
current_mean = ReduceMean(X, axis=all_except_channel_index)
current_var =  ReduceVar(X, axis=all_except_channel_index)
Currently do not support dims higher than 2 dimensions: 
Currently supporting only 2-D matrices
custom implementation such as CuDNN.
custom join thread function not set
custom join thread function not set for inter op thread pool
custom join thread function not set for intra op thread pool
custom op registered at runtime
custom_create_thread_fn returned invalid handle.
custom_logger != nullptr
CxhXHE
Cxp#Q
CXp#Q
CY;\$
Cypriot
Cyrillic
D D R R z | 
D!gVM
D$ ;D$$
D$ ;D$P
D$ ;M
D$ |<
D$ |U
D$ |V
D$ 9\$ |
D$ 9D$
D$ 9D$$
D$ 9D$@|7
D$ 9F
D$ 9W
D$ j@P
D$ p0
D$ P8
D$ PD
D$ Pj
D$ PQ
D$ PW
D$ QQ
D$ SVW
D$ SVW3
D$ W;
D$#VQ
D$$;D$
D$$;D$@
D$$;D$H
D$$;D$H|
D$$;D$P
D$$;D$X
D$$;L$
D$$;T$
D$$+|$4
D$$+D$ @
D$$03
D$$9F
D$$H#
D$$j@P
D$$p.
D$$pD
D$$Pj
D$$PQ
D$$PW
D$$RP
D$$SP
D$$VW
D$$xP
D$$YY9D$
D$(;D$P
D$(;D$X
D$(;L$
D$(+D$
D$(9F
D$(9K
D$(9L$
D$(Af
D$(PQ
D$(QQ
D$(QVPW
D$(S3
D$(SV
D$(SVW
D$(SVW3
D$(W;
D$(xs
D$(YY
D$,;D$0|
D$,;D$H
D$,;D$P
D$,_^[
D$,|I
D$,9V
D$,PQ
D$,PW
D$@;A
D$@;L$
D$@9L$
D$@PW
D$@SVW
D$@Y;
D$\;D$`
D$\LkE
D$\VW3
D$`|]
D$`9}0
D$`9s
D$`SV
D$|;C
D$<;D$(
D$<;D$`
D$<;F
D$<;L$0
D$<_^[
D$<PQ
D$<PS
D$<QQ
D$0;D$P
D$0;F
D$09K
D$09p
D$0PR
D$0PW
D$0SV
D$0SVW
D$0YY
D$4,TC
D$4;B
D$4;D$P
D$4;F
D$4;G
D$4PQ
D$4PRPj
D$4PV
D$4PW
D$4QRP
D$8;A
D$8;D$<
D$8;D$p
D$8;F
D$8;Wtu
D$8;Wxu
D$8;Y
D$8|b
D$8|T
D$8|X
D$8Php
D$8PQ
D$D;D$h
D$D;F
D$D|W
D$DPQ
D$H;D$h
D$H;D$p
D$H;E
D$H+D$
D$H9E
D$H9r
D$HSVW
D$HxV
D$L;F
D$LVP
D$LYY
D$P(kE
D$P)T$(
D$p;B
D$P;E
D$p;w
D$P|[
D$P+D$
D$P9E
D$P9T$D
D$PPVRj
D$PSVW
D$T;F
D$X|]
D$X+D$8
D$xjo
D$XSV
d/P{;
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\controlflow\defs.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\controlflow\old.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\generator\defs.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\generator\old.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\logical\defs.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\logical\old.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\math\defs.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\math\old.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\nn\defs.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\nn\old.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\object_detection\defs.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\object_detection\old.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\optional\defs.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\quantization\defs.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\quantization\old.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\reduction\defs.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\reduction\old.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\rnn\defs.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\rnn\old.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\sequence\defs.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\tensor\defs.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\tensor\old.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\traditionalml\defs.cc
D:\a\_work\1\s\engine\lotus\cmake\external\onnx\onnx\defs\traditionalml\old.cc
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google/protobuf/parse_context.h
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google/protobuf/repeated_field.h
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\arena.cc
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\io\zero_copy_stream.cc
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\io\zero_copy_stream_impl.cc
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\io\zero_copy_stream_impl_lite.cc
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\message_lite.cc
D:\a\_work\1\s\engine\lotus\cmake\external\protobuf\src\google\protobuf\repeated_field.cc
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2/walker-inl.h
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\bitstate.cc
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\compile.cc
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\dfa.cc
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\nfa.cc
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\onepass.cc
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\parse.cc
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\re2.cc
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\regexp.cc
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\simplify.cc
D:\a\_work\1\s\engine\lotus\cmake\external\re2\re2\tostring.cc
D:\a\_work\1\s\engine\lotus\cmake\external\wil\include\wil/wrl.h
D:\a\_work\1\s\engine\lotus\cmake\external\wil\include\wil\resource.h
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/common/const_pointer_container.h
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/common/logging/logging.h
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/framework/data_types.h
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/framework/data_types_internal.h
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/framework/op_kernel_context.h
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/framework/ort_value.h
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/framework/tensor.h
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/graph/graph.h
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/optimizer/graph_transformer.h
D:\a\_work\1\s\engine\lotus\include\onnxruntime\core/platform/EigenNonBlockingThreadPool.h
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops/cpu/activations.h
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops/cpu/crop_and_resize.h
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\attnlstm\bahdanau_attention.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\attnlstm\deep_cpu_attn_lstm.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\attnlstm\deep_cpu_attn_lstm.h
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\attention.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\attention_base.h
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\attention_cpu_base.h
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\bias_gelu.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\bifurcation_detector.h
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\embed_layer_norm.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\ngram_repeat_block.h
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\bert\qembed_layer_norm.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\cdist.h
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\cpu_contrib_kernels.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\crop.h
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\element_wise_ops.h
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\expand_dims.h
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\fused_conv.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\fused_gemm.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\grid_sample.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\image_scaler.h
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\layer_norm.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\math\sparse_dense_matmul.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\matmul_integer16.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\maxpool_with_mask.h
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\murmur_hash3.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\qlinear_binary_op.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\qlinear_concat.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\qlinear_global_average_pool.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\qlinear_lookup_table.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\qlinear_pool.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\quantization\attention_quant.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\quantization\dynamic_quantize_lstm.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\quantization\dynamic_quantize_matmul.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\quantization\nhwc_max_pool.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\quantization\quant_gemm.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\skip_layer_norm.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\tokenizer.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\contrib_ops\cpu\word_conv_embedding.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core/common/safeint.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/bfc_arena.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/execution_frame.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/execution_providers.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/feeds_fetches_manager.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/func_kernel.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/mem_pattern_planner.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/mldata_type_utils.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/node_index_info.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/op_kernel_context_internal.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/ort_value_tensor_slicer.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/sequential_execution_plan.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/tensorprotoutils.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/framework/TensorSeq.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/graph/function_impl.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/graph/model_load_utils.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/optimizer/attention_fusion_helper.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/optimizer/initializer.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/optimizer/selectors_actions/helpers.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/platform/path_lib.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/common.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/activation/activations.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/controlflow/scan_utils.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/element_wise_ranged_transform.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/generator/constant_of_shape_base.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/generator/random.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/math/clip.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/math/element_wise_ops.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/math/gemm_helper.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/math/matmul_helper.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/ml/cast_map.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/ml/category_mapper.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/ml/dictvectorizer.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/ml/feature_vectorizer.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/ml/label_encoder.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/ml/ml_common.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/ml/normalizer.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/batch_norm.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/conv_attributes.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/conv_transpose_attributes.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/dropout_op.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/flatten.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/instance_norm.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/lp_norm.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/lrn.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/pool_attributes.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/pool_base.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/roi_pool.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/shrink.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/nn/unpool.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/reduction/reduction_ops.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/rnn/deep_cpu_gru.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/rnn/rnn.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/rnn/rnn_helpers.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/copy.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/identity_op.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/mean_variance_normalization.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/reshape.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/reverse_sequence.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/slice.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/space_depth_ops.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/split.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/squeeze.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/transpose.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/unsqueeze.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/upsample.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/cpu/tensor/utils.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/dml/OperatorAuthorHelper/MLOperatorAuthorHelper.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/dml/OperatorAuthorHelper/OperatorHelper.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/providers/dml/OperatorAuthorHelper/SchemaInferenceOverrider.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core/session/inference_session.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\common\helper.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\common\logging\logging.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\common\path.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\common\profiler.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\common\status.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\common\threadpool.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\flatbuffers\flatbuffers_utils.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\allocation_planner.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\allocator.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\allocatormgr.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\bfc_arena.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\config_options.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\data_transfer.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\data_transfer_manager.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\data_types.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\endian_utils.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\ex_lib_loader.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\execution_frame.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\execution_provider.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\fallback_cpu_capability.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\feeds_fetches_manager.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\fuse_nodes_funcs.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\graph_partitioner.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\kernel_def_builder.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\kernel_registry.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\mldata_type_utils.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\node_index_info.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\op_kernel.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\op_kernel_info.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\op_node_proto_helper.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\ort_value_tensor_slicer.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\parallel_executor.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\prepacked_weights.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\prepacked_weights_container.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\sequential_executor.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\session_state.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\session_state_utils.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\sparse_tensor.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\sparse_utils.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\tensor.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\tensor_shape.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\tensor_type_and_shape.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\tensorprotoutils.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\framework\utils.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\contrib_ops\contrib_defs.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\contrib_ops\nhwc_schema_defs.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\contrib_ops\quantization_defs.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\dml_ops\dml_defs.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\function.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\graph.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\graph_flatbuffers_utils.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\graph_utils.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\graph_viewer.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\model.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\graph\schema_registry.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\attention_fusion.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\bias_dropout_fusion.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\bias_gelu_fusion.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\bias_softmax_fusion.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\common_subexpression_elimination.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\constant_folding.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\conv_activation_fusion.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\conv_add_fusion.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\conv_bn_fusion.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\conv_mul_fusion.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\dynamic_quantize_matmul_fusion.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\embed_layer_norm_fusion.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\fast_gelu_fusion.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\free_dim_override_transformer.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\gelu_approximation.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\gelu_fusion.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\gemm_activation_fusion.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\gemm_sum_fusion.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\graph_transformer.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\graph_transformer_mgr.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\graph_transformer_utils.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\initializer.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\insert_cast_transformer.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\layer_norm_fusion.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\matmul_add_fusion.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\matmul_integer_to_float.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\matmul_scale_fusion.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\matmul_transpose_fusion.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\nhwc_transformer.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\optimizer_execution_frame.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\qdq_transformer\qdq_propagation.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\qdq_transformer\qdq_s8_to_u8.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\relu_clip_fusion.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\reshape_fusion.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\rule_based_graph_transformer.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\selectors_actions\actions.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\selectors_actions\helpers.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\selectors_actions\selector_action_transformer.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\skip_layer_norm_fusion.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\transformer_memcpy.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\transpose_optimizer\api_impl.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\transpose_optimizer\ort_transpose_optimizer.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\optimizer\unsqueeze_elimination.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\platform\windows\env.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\activation\activations.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\if.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\loop.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\scan_8.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\scan_9.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\controlflow\scan_utils.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\cpu_execution_provider.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\generator\constant_of_shape.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\generator\random.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\clip.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\cumsum.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\det.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum_utils\einsum_auxiliary_ops.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum_utils\einsum_compute_preprocessor.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\einsum_utils\einsum_typed_compute_processor.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\element_wise_ops.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\gemm_base.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\gemm_helper.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\hardmax.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\matmul.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\matmul_integer.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\quantize_linear_matmul.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\softmax.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\math\top_k.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\cast_map.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\feature_vectorizer.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\imputer.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\linearclassifier.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\linearregressor.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\ml_common.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\onehotencoder.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\scaler.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\svmclassifier.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\svmclassifier.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\svmregressor.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\tree_ensemble_aggregator.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\tree_ensemble_common.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\ml\zipmap.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\conv.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\conv_integer.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\conv_transpose.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\instance_norm.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\lrn.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\pool.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\qlinearconv.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\roi_pool.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\string_normalizer.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\tfidfvectorizer.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\nn\Unpool.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\object_detection\non_max_suppression.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\object_detection\non_max_suppression.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\object_detection\roialign.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\optional\optional_ops.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\optional\optional_ops.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\reduction\reduction_ops.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\deep_cpu_gru.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\deep_cpu_lstm.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\lstm_base.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\lstm_base.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\rnn.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\rnn\rnn_helpers.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\sequence\concat_from_sequence.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\sequence\sequence_ops.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\cast_op.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\compress.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\concat.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\concatbase.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\dynamicquantizelinear.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gather.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gather_elements.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gather_elements.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gather_nd.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\gatherbase.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\isinf.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\nonzero_op.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\onehot.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\pad.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\padbase.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\quantize_linear.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\reshape_helper.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\reverse_sequence.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\scatter.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\scatter_nd.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\slice.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\space_depth_ops.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\split.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\tile.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\transpose.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\trilu.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\trilu.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\unsqueeze.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\cpu\tensor\upsample.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\dml_provider_factory.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\AbiCustomRegistry.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\BucketizedBufferAllocator.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\CommandAllocatorRing.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\CommandQueue.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\DescriptorPool.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\DmlCommandRecorder.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\DmlCommon.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\ExecutionContext.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\ExecutionProvider.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\External/DirectMLHelpers/ApiHelpers.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\External/DirectMLHelpers/ApiTraits.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\External/DirectMLHelpers/GeneratedSchemaHelpers.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\External/DirectMLHelpers/SchemaHelpers.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\FusedGraphKernel.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GpuEvent.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GraphDescBuilder.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GraphPartitioner.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GraphTransformer.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\MLOperatorAuthorImpl.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\MLOperatorAuthorImpl.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperator.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorActivation.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorAffine.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorBatchNormalization.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorCast.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorConcat.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorConstantOfShape.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorConvInteger.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorConvolution.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorCopy.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorCrop.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorCumSum.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorDepthToSpace.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorDynamicQuantizeLinear.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorEinSum.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorElementWise.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorExpand.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorEyeLike.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorGather.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorGemm.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorInstanceNormalization.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorLocalResponseNormalization.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorLpNormalization.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMatMul.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMatMulInteger.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMaxUnpool.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMeanVarianceNormalization.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorMemcpy.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorNeg.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorOneHot.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorPadding.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorPooling.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorQLinearAdd.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorQLinearConv.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorQLinearMatMul.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorRange.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorRecurrentNeuralNetwork.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorReduce.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorResize.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorReverseSequence.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorRoiAlign.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorRoiPooling.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorScatter.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorSlice.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorSpaceToDepth.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorSplit.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorTile.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorTopk.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorTranspose.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\DmlOperatorValueScale2D.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\OperatorRegistration.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\Operators\OperatorUtility.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\PooledUploadHeap.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\ReadbackHeap.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\TensorDesc.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\GraphTransformers\GraphTransformerHelpers.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\OperatorAuthorHelper\MLOperatorAuthorHelper.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\OperatorAuthorHelper\OperatorHelper.cpp
D:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\OperatorAuthorHelper\OperatorHelper.h
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\custom_ops.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\environment.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\inference_session.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\inference_session_utils.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\IOBinding.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\onnxruntime_c_api.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\ort_env.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\session\provider_bridge_ort.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\util\math_cpu.cc
D:\a\_work\1\s\engine\lotus\onnxruntime\core\util\thread_utils.cc
D:\a\_work\1\s\engine\lotus\winml\adapter\winml_adapter_dml.cpp
D:\a\_work\1\s\engine\lotus\winml\adapter\winml_adapter_session.cpp
D0<1F1[1p1
D0H0L0T0d0p0t0
D0k0r0
D3B4V5+7W;U<f==?
d3d12.dll
D3H3L3
DA{0d]
Data for input  
Data of TensorProto ( tensor name: 
data overflow
Data size mismatch. Tensor: 
data tensor must have rank >= 1
data type 
Data type for starts and ends inputs' is not supported in this build. Got 
data type is different from updates type
data type is not supported
Data type mismatch
Data type of the input tensor MUST be same as that of the input sequence. Sequence data type (
Data types of the inputs must match for MatMul
data_0
data_1.Shape() == shape
DATA_BATCH
data_n.Shape() == shape
data_rank * 2 == pads.size()
data_rank > 0
data_scale
data_transfer registered is nullptr.
data_type
data_zero_point
DataTypeUtils::FromDataTypeString - Received invalid data type string 
DCR (default) for depth-column-row order re-arrangement. Use CRD for column-row-depth order.
DD$<PRQ
DeadState in RunStateOnByte
DebugBreak
DecodePointer
DecoderAttention
decomposed_QLinearSigmoid_DequantizeLinear_
decomposed_QLinearSigmoid_input_
decomposed_QLinearSigmoid_output_
decomposed_QLinearSigmoid_QuantizeLinear_
decomposed_QLinearSigmoid_Sigmoid_
default 1; Pooled output Y's height.
default 1; Pooled output Y's width.
Default logger already set. 
default_float
default_int64
default_logger_id must be provided if instance_type is InstanceType::Default
default_string
Defines behaviour if 'axes' is empty. Default behaviour with 'false' is to reduce all axes. When axes is empty and this attribute is set to true, input tensor will not be reduced,and the output tensor would be equivalent to input tensor.
Defines how to aggregate leaf values within a target. <br>One of 'AVERAGE,' 'SUM,' 'MIN,' 'MAX.'
DeleteCriticalSection
DeleteFile() failed - path: 
DeleteFileW
delta
delta in Range operator can not be zero!
delta in Range operator should be scalar like tensor, yet got shape:
Denote x_resized as the coordinate of axis x in the resized tensor, x_original as the coordinate of axis x in the original tensor, length_original as the length of the original tensor in axis x, length_resized as the length of the resized tensor in axis x, roi_x = (start_x, end_x) of the axis x in input "roi", scale = length_resized / length_original, <br/>
DENSE
dense shape must 2-D. Got: 
depth
Depth is negative.
DepthToSpace
DepthToSpace op: only 'DCR' and 'CRD' modes are supported
DepthToSpace requires input depth to be a multiple of (block_size * blok_size)
DequantizeLinear
DequantizeLinear with type int32 should have no zero point or all zero points should be 0
deque<T> too long
Describes the axis of the inputs when coerced to 2D; defaults to one because the 0th axis most likely describes the batch_size
Describes the axis of the inputs when coerced to 2D; defaults to one because the 0th axis most likely describes the batch_size. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
Deseret
Deserialize tensor 
DeserializeTensorProto() takes either pre-allocated buffer or an allocator!
destination address required
Destination must have a CPU allocator set
Destination should be empty
detect_negative
detect_positive
Devanagari
Deviation = Sub (XU, Mean2D)
device or resource busy
Device:[
DeviceType:
DFA out of memory: 
dh\?D
dh8iE
DhX`H
DictVectorizer
Did not find an arena based allocator registered for device-id 
Did not find session options in the model file to be used while running the model
Dilation not supported for AutoPadType::SAME_UPPER or AutoPadType::SAME_LOWER.
Dilation value along each spatial axis of filter.
Dilation value along each spatial axis of filter. If not present, the dilation defaults to 1 along each spatial axis.
dilation value along each spatial axis of the filter.
dilation value along each spatial axis of the filter. If not present, the dilation defaults is 1 along each spatial axis.
dilation value along each spatial axis of the filter. If not present, the dilation defaults to 1 along each axis.
dilation value along each spatial axis of the filter. If not present, the dilation defaults to 1 along each spatial axis.
dilations
Dilations
Dilations dimensions should match kernel shape
dilations.size() == kernel_shape.size()
dim_iter == rank
dim_param value with no name. Invalid ORT format model.
dim_size > 0
dim0_offset < dim0_size
dimension <= num_dims
Dimension could not be inferred: incompatible shapes
dimension for each axis in the list of axes, it uses this information to
Dimension mismatch in unification between 
Dimension of input 
Dimension on which to do the sort.
Dimension on which to do the sort. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(input).
Dimension value inferred (
dimension. If the value passed to start or end is larger than the `n` (the
Dimension: 
DimensionCount
dims.size() == extents.size() && dims.size() >= steps.size()
dims.size() == extents_.size()
dims.size() == starts.size()
dims.size() == starts.size() && dims.size() == extents_.size() && dims.size() >= steps.size()
dims.size() == steps.size()
dims.size()=
dims[d_i] < d_max
dimstart <= dimend && dimend <= values_.size()
direction
Direction
Direction of moving bits. It can be either "RIGHT" (for right shift) or "LEFT" (for left shift).
directions
directions.size() == num_entries
DirectML.dll
directory not empty
discarded
Distribution
DistributionEnqueue
Div and Shape does not have edge
Div and Shape1 does not have edge
div_inputs.size() == 2
Dives_Akuru
Divide by zero
DivMulFusion
DML CPU
Dml::ComputationCapacityFromPartition
Dml::ExecutionProviderImpl::CopyTensors
Dml::GraphTransformer::ApplyImpl
DML_OPERATOR_ACTIVATION_CELU
DML_OPERATOR_ACTIVATION_ELU
DML_OPERATOR_ACTIVATION_HARD_SIGMOID
DML_OPERATOR_ACTIVATION_HARDMAX
DML_OPERATOR_ACTIVATION_IDENTITY
DML_OPERATOR_ACTIVATION_LEAKY_RELU
DML_OPERATOR_ACTIVATION_LINEAR
DML_OPERATOR_ACTIVATION_LOG_SOFTMAX
DML_OPERATOR_ACTIVATION_PARAMETERIZED_RELU
DML_OPERATOR_ACTIVATION_PARAMETRIC_SOFTPLUS
DML_OPERATOR_ACTIVATION_RELU
DML_OPERATOR_ACTIVATION_RELU_GRAD
DML_OPERATOR_ACTIVATION_SCALED_ELU
DML_OPERATOR_ACTIVATION_SCALED_TANH
DML_OPERATOR_ACTIVATION_SHRINK
DML_OPERATOR_ACTIVATION_SIGMOID
DML_OPERATOR_ACTIVATION_SOFTMAX
DML_OPERATOR_ACTIVATION_SOFTPLUS
DML_OPERATOR_ACTIVATION_SOFTSIGN
DML_OPERATOR_ACTIVATION_TANH
DML_OPERATOR_ACTIVATION_THRESHOLDED_RELU
DML_OPERATOR_ADAM_OPTIMIZER
DML_OPERATOR_ARGMAX
DML_OPERATOR_ARGMIN
DML_OPERATOR_AVERAGE_POOLING
DML_OPERATOR_AVERAGE_POOLING_GRAD
DML_OPERATOR_BATCH_NORMALIZATION
DML_OPERATOR_BATCH_NORMALIZATION_GRAD
DML_OPERATOR_BATCH_NORMALIZATION_TRAINING
DML_OPERATOR_BATCH_NORMALIZATION_TRAINING_GRAD
DML_OPERATOR_CAST
DML_OPERATOR_CONVOLUTION
DML_OPERATOR_CONVOLUTION_INTEGER
DML_OPERATOR_CUMULATIVE_PRODUCT
DML_OPERATOR_CUMULATIVE_SUMMATION
DML_OPERATOR_DEPTH_TO_SPACE
DML_OPERATOR_DEPTH_TO_SPACE1
DML_OPERATOR_DIAGONAL_MATRIX
DML_OPERATOR_DYNAMIC_QUANTIZE_LINEAR
DML_OPERATOR_ELEMENT_WISE_ABS
DML_OPERATOR_ELEMENT_WISE_ACOS
DML_OPERATOR_ELEMENT_WISE_ACOSH
DML_OPERATOR_ELEMENT_WISE_ADD
DML_OPERATOR_ELEMENT_WISE_ADD1
DML_OPERATOR_ELEMENT_WISE_ASIN
DML_OPERATOR_ELEMENT_WISE_ASINH
DML_OPERATOR_ELEMENT_WISE_ATAN
DML_OPERATOR_ELEMENT_WISE_ATAN_YX
DML_OPERATOR_ELEMENT_WISE_ATANH
DML_OPERATOR_ELEMENT_WISE_BIT_AND
DML_OPERATOR_ELEMENT_WISE_BIT_COUNT
DML_OPERATOR_ELEMENT_WISE_BIT_NOT
DML_OPERATOR_ELEMENT_WISE_BIT_OR
DML_OPERATOR_ELEMENT_WISE_BIT_SHIFT_LEFT
DML_OPERATOR_ELEMENT_WISE_BIT_SHIFT_RIGHT
DML_OPERATOR_ELEMENT_WISE_BIT_XOR
DML_OPERATOR_ELEMENT_WISE_CEIL
DML_OPERATOR_ELEMENT_WISE_CLIP
DML_OPERATOR_ELEMENT_WISE_CLIP_GRAD
DML_OPERATOR_ELEMENT_WISE_CLIP_GRAD1
DML_OPERATOR_ELEMENT_WISE_CLIP1
DML_OPERATOR_ELEMENT_WISE_CONSTANT_POW
DML_OPERATOR_ELEMENT_WISE_COS
DML_OPERATOR_ELEMENT_WISE_COSH
DML_OPERATOR_ELEMENT_WISE_DEQUANTIZE_LINEAR
DML_OPERATOR_ELEMENT_WISE_DIFFERENCE_SQUARE
DML_OPERATOR_ELEMENT_WISE_DIVIDE
DML_OPERATOR_ELEMENT_WISE_ERF
DML_OPERATOR_ELEMENT_WISE_EXP
DML_OPERATOR_ELEMENT_WISE_FLOOR
DML_OPERATOR_ELEMENT_WISE_IDENTITY
DML_OPERATOR_ELEMENT_WISE_IF
DML_OPERATOR_ELEMENT_WISE_IS_INFINITY
DML_OPERATOR_ELEMENT_WISE_IS_NAN
DML_OPERATOR_ELEMENT_WISE_LOG
DML_OPERATOR_ELEMENT_WISE_LOGICAL_AND
DML_OPERATOR_ELEMENT_WISE_LOGICAL_EQUALS
DML_OPERATOR_ELEMENT_WISE_LOGICAL_GREATER_THAN
DML_OPERATOR_ELEMENT_WISE_LOGICAL_GREATER_THAN_OR_EQUAL
DML_OPERATOR_ELEMENT_WISE_LOGICAL_LESS_THAN
DML_OPERATOR_ELEMENT_WISE_LOGICAL_LESS_THAN_OR_EQUAL
DML_OPERATOR_ELEMENT_WISE_LOGICAL_NOT
DML_OPERATOR_ELEMENT_WISE_LOGICAL_OR
DML_OPERATOR_ELEMENT_WISE_LOGICAL_XOR
DML_OPERATOR_ELEMENT_WISE_MAX
DML_OPERATOR_ELEMENT_WISE_MEAN
DML_OPERATOR_ELEMENT_WISE_MIN
DML_OPERATOR_ELEMENT_WISE_MODULUS_FLOOR
DML_OPERATOR_ELEMENT_WISE_MODULUS_TRUNCATE
DML_OPERATOR_ELEMENT_WISE_MULTIPLY
DML_OPERATOR_ELEMENT_WISE_NEGATE
DML_OPERATOR_ELEMENT_WISE_POW
DML_OPERATOR_ELEMENT_WISE_QUANTIZE_LINEAR
DML_OPERATOR_ELEMENT_WISE_QUANTIZED_LINEAR_ADD
DML_OPERATOR_ELEMENT_WISE_RECIP
DML_OPERATOR_ELEMENT_WISE_ROUND
DML_OPERATOR_ELEMENT_WISE_SIGN
DML_OPERATOR_ELEMENT_WISE_SIN
DML_OPERATOR_ELEMENT_WISE_SINH
DML_OPERATOR_ELEMENT_WISE_SQRT
DML_OPERATOR_ELEMENT_WISE_SUBTRACT
DML_OPERATOR_ELEMENT_WISE_TAN
DML_OPERATOR_ELEMENT_WISE_TANH
DML_OPERATOR_ELEMENT_WISE_THRESHOLD
DML_OPERATOR_FILL_VALUE_CONSTANT
DML_OPERATOR_FILL_VALUE_SEQUENCE
DML_OPERATOR_GATHER
DML_OPERATOR_GATHER_ELEMENTS
DML_OPERATOR_GATHER_ND
DML_OPERATOR_GATHER_ND1
DML_OPERATOR_GEMM
DML_OPERATOR_GRU
DML_OPERATOR_JOIN
DML_OPERATOR_LOCAL_RESPONSE_NORMALIZATION
DML_OPERATOR_LOCAL_RESPONSE_NORMALIZATION_GRAD
DML_OPERATOR_LP_NORMALIZATION
DML_OPERATOR_LP_POOLING
DML_OPERATOR_LSTM
DML_OPERATOR_MATRIX_MULTIPLY_INTEGER
DML_OPERATOR_MAX_POOLING
DML_OPERATOR_MAX_POOLING_GRAD
DML_OPERATOR_MAX_POOLING1
DML_OPERATOR_MAX_POOLING2
DML_OPERATOR_MAX_UNPOOLING
DML_OPERATOR_MEAN_VARIANCE_NORMALIZATION
DML_OPERATOR_MEAN_VARIANCE_NORMALIZATION1
DML_OPERATOR_NONZERO_COORDINATES
DML_OPERATOR_ONE_HOT
DML_OPERATOR_PADDING
DML_OPERATOR_PADDING1
DML_OPERATOR_QUANTIZED_LINEAR_CONVOLUTION
DML_OPERATOR_QUANTIZED_LINEAR_MATRIX_MULTIPLY
DML_OPERATOR_RANDOM_GENERATOR
DML_OPERATOR_REDUCE
DML_OPERATOR_RESAMPLE
DML_OPERATOR_RESAMPLE_GRAD
DML_OPERATOR_RESAMPLE1
DML_OPERATOR_REVERSE_SUBSEQUENCES
DML_OPERATOR_RNN
DML_OPERATOR_ROI_ALIGN
DML_OPERATOR_ROI_ALIGN_GRAD
DML_OPERATOR_ROI_ALIGN1
DML_OPERATOR_ROI_POOLING
DML_OPERATOR_SCATTER
DML_OPERATOR_SCATTER_ND
DML_OPERATOR_SLICE
DML_OPERATOR_SLICE_GRAD
DML_OPERATOR_SLICE1
DML_OPERATOR_SPACE_TO_DEPTH
DML_OPERATOR_SPACE_TO_DEPTH1
DML_OPERATOR_SPLIT
DML_OPERATOR_TILE
DML_OPERATOR_TOP_K
DML_OPERATOR_TOP_K1
DML_OPERATOR_UPSAMPLE_2D
DML_OPERATOR_VALUE_SCALE_2D
DMLCreateDevice1
DmlExecutionProvider
DmlFusedNode_
DmlFusedNodeDomain
DnnlExecutionProvider
DO`;O`t
DoCoalesce failed: r1->op() is 
DoCoalesce failed: r2->op() is 
does not have the graph for key 
Dogra
Domain already set in registry
domain_version != -1
domainToVersionMap
Done saving initialized tensors
Done saving OrtValue mappings.
double
double_data
drop_states
Dropout
dst.DataType() == src.DataType()
dst_implicit_input_idx < (int)node->ImplicitInputDefs().size()
dst_strides.size() == src_strides.size() && src_strides.size() == copy_shape.size() && !copy_shape.empty()
dT)iU
DtEVW
dtype
dtype_ != nullptr
dtype_attribute->second.has_i()
dup_replacements.find(&arg) == dup_replacements.end()
Duplicate constant node sparse initializer name: '
Duplicate initializer (dense or ConstantNode): '
Duplicate initializer (dense, sparse or ConstantNode): '
Duplicate ngram detected, size: 
Duplicate of FusedMatMul. Going forward FusedMatMul should be used. This OP will be supported for backward compatibility.
Duplicate sparse_tensor_initializer: '
Duplicate stopwords not allowed
Duplicate type constraint name
duplicated allocator
duplicated allocator: 
duplicated ort_value index:
Duployan
dxgi.dll
dxUwP
DynamicQuantizeLinear
DynamicQuantizeLSTM
DynamicQuantizeLSTM : 
DynamicQuantizeMatMul
DynamicQuantizeMatMulFusion
DynamicSlice
DyScB
Dz-hX%F
E PQ3
E RjpZ
E VVQ
E$RRQQ
E$VWj
e%6E)
E(QQPQPQ
E,SVW
e@M6=
E_Xsquared
E<9ED
E<wv:
e0j0 1Q1\1
'Ea<L
Each element of the sequence should be either tensor or map.
ed6{/
edge_it->GetDstArgIndex() == 0 && edge_it->GetSrcArgIndex() == 0 && edge_it->GetNode().Index() == down_node.Index()
EF`;X
EF`;x$}i
EF`;X4
EF`;xD
Egyptian_Hieroglyphs
eh`DE
ehl]D
Ehttp://crl.microsoft.com/pki/crl/products/MicRooCerAut_2010-06-23.crl0Z
Ehttp://www.microsoft.com/pkiops/certs/MicWinProPCA2011_2011-10-19.crt0
Einsum
Einsum expression string.
Einsum op: An implementation for the input type 
Einsum op: Could not copy the intermediate output's buffer into the op's output buffer. Error: 
Einsum op: Exception during MatMul operation: 
Einsum op: Input dimensions must be equal along an axis to be reduced across all inputs
Einsum op: Input shapes do not align
Einsum op: The candidate output cannot be reshaped into the op's output
Einsum op: The candidate output does not match the actual output's shape
Einsum op: There must be atleast one input
Einsum op: Transpose failed: 
Einsum op: Unsupported data type for Diagonal 
Einsum operands could not be broadcast together. Please check input shapes/equation provided.Input shape of operand 
Einsum subscripts does not contain enough subscript labels and there is no ellipsis for input 
Einsum subscripts string contains too many subscript labels when compared to the rank of the input 
Either both scale and offset can be of feature size (
Either one of the separators OR tokenexp attributes required but none is set
Either scales or sizes MUST be provided as input.
Either the key tensor or the value tensor has NumDimensions > 1
Elbasan
elem_proto != nullptr
elem_type
elem_type_ != nullptr
element index is out of bounds
Element type of input 
Element type of input was unknown
Element type of inputs are expected to be the same.
Element type of optional input 
Element type of sequence input 
Element type of tensor or sparse tensor input was unknown
Element_size of: 
elements, but feeds has 
EliminateDropout
EliminateIdentity
EliminateSlice
Ellipsis must indicate a fixed number of dimensions across all inputs
Ellipsis represents incompatible dimensions.
else_branch
Elu_Result
Elymaic
EM\:Y2
embedding_size
embedding_sum
EmbedLayerNormalization
EmbedLayerNormFusion
Empty dimensions for input tensor
Empty graph proto from deserialization of ORT format model
Empty input dimensions.
Empty scale in attributes
Empty stopwords not allowed
Empty value of imputed values.
Enable broadcasting
enable_profiling
enable_profiling option in the model file must be an integer
enabled_
EnableOrtCustomOps: Custom operators in onnxruntime-extensions are not enabled
EncodePointer
Encountered unknown exception in Initialize()
Encountered unknown exception in Load()
Encountered unknown exception in Run()
end >= starts_.back()
end of a dimension with unknown size, it is recommended to pass in `INT_MAX`.
end of input
end_idx >= start_idx && end_idx <= total_items
Ending indices (exclusive) of corresponding axis in axes`
EndPadding
endpos: 
Ends must be a 1-D array
Engine failed to create a model!
ENGINE_ERROR
EnterCriticalSection
entiu
entry != initialized_tensors_to_allocate.end() && entry->second->data_type() != ONNX_NAMESPACE::TensorProto_DataType_STRING
entry != kernel_create_info_map.cend()
entry != kernel_create_info_map_.cend()
entry != node_to_subgraph_ss.second.cend()
entry != nullptr
entry != regions_.end()
Entry exists in node 
entry.program_counter.HasValidEntries()
en-US
Env is null
env_ptr == p_instance_
Environment dependent string that denotes the locale according to which output strings needs to be upper/lowercased.Default en_US or platform specific equivalent as decided by the implementation.
EP_FAIL
epsilon
Epsilon
epsilon_ >= 0
Equal
equal const not matched.
equation
Equations (Default: f=Sigmoid, g=Tanh):
Equations (Default: f=Sigmoid, g=Tanh, h=Tanh):
Equations (Default: f=Tanh):
ERROR
Error compiling '
Error context: 
Error during EndProfiling(): 
Error mapping feeds: 
Error mapping output names: 
Error merging shape info for output. '
Error parsing '
Error parsing function body:
Error parsing node:
Error parsing TensorProto (expected a tensor shape).
Error parsing TensorProto (expected a tensor type).
Error parsing TensorProto shape (expected numeric dimension).
Error reverse compiling '
Error unexpected extra input in node:
Error: Duplicate definition-site for (
errorCategory
errorCode
errorMessage
Esession_state_ != nullptr
Ethiopic
euclidean
EvaluationStart
EvaluationStop
EventRegister
EventSetInformation
EventUnregister
EventWriteTransfer
ew96W
EX_squared
EX`;Jx
Example 1:
Example 2:
Example 3:
Example 4:
Exception
Exception caught: 
Exception during initialization: 
Exception during loading: 
Exception running nodes starting at 
exclude_outside
exclude_outside can be set to 1 only when mode is CUBIC. Current mode is set to 
exclusive
exec_plan_index
exec_plan_ptr
executable format error
Execution frame was null
Execution providers must be registered before the session is initialized.
Execution providers must be registered before the session is initialized. 
Execution type '
execution_mode
execution_mode is not valid
execution_mode option in the model file must be an integer
ExecutionProviderEvent
executionProviderIds
Existing entry in compiled kernel hashes for 
Existing registration with name 
existing_entries.find(attribute_name) == existing_entries.cend()
existing->second == &tensor
Exiting due to terminate flag being set to true.
Expand
ExpandBroadcastLooper should only have a shape for the second input.
ExpandDims
ExpandDims echo operator.
expanded
expanded_target
expanded_target_int64
ExpandElimination
Expect mask data type is uint8 or float
Expect to have present state output when past state input is given
expected a registered ONNX type
Expected a single float value!
Expected a single int64 value!
Expected a single string value!
Expected AllocateFinalOutput to have been called to before we increment the iterator
Expected AllocateFinalOutput to have been called to before we read the OrtValue from the iterator.
Expected character 
Expected 'replace_value_int64' attribute since 'imputed_values_int64' is specified
Expected 'replaced_value_float' attribute since 'imputed_value_floats' is specified
Expected shape from model of 
Expected value:
Expecting 2xValues == indices
Expecting a non-empty tokenexp
Expecting activation to be one of Affine, Relu, LeakyRelu, ThresholdedRelu, Tanh, ScaledTanh, Sigmoid, HardSigmoid, Elu, Softsign, Softplus. Got 
Expecting all elements to be tensors. Got: 
Expecting COO 2-D indices shape
Expecting data type to be set as string
Expecting fully sparse tensors to have indices shape {0}
Expecting fully sparse tensors to have value shape {0}
Expecting index blocks: 
Expecting indices to be equal the number of values or be twice as many
Expecting indices to have 2-D shape . Got: 
Expecting inner index size: 
Expecting inner indices to be same as nnz. Got: 
Expecting one index. Got: 
Expecting to contain one index, got: 
Expecting to have at lest 3-D shape. Got:
Expecting two indices. Got: 
Exponent
ExponentTensor
Extended allocation by 
Extending BFCArena for 
extents.size()=
External data type cannot be UNDEFINED or STRING.
External data type must not be UNDEFINED or STRING.
Extra unparsed input unexpected.
extra_add
extra_shape
extrapolation_value
EyeLike
EyeLike : Input tensor dimension is not 2
F F ~ ~ 
F f99_
F$;F$tR
F$+F 
F$W;F(t
F(+F$
F(+F$j
F,+F(
F,SWj
F;t$0
F;t$4
F;u$|
F\SWj
F|+Fx
F|G+Fx
f}O\w5
F<$"<
F<+F8j
F<SWj
F0;F4tE
F09F,t8
f1.3R
F4;F8t4
F4@9E0
F4+F0j
F8+F4
F8G+F4
F9q<u
Factor used in computing the running mean and variance.e.g., running_mean = running_mean * momentum + mean * (1 - momentum), default is 0.9f.
Factor used in computing the running mean and variance.e.g., running_mean = running_mean * momentum + mean * (1 - momentum).
Faild to find path to qkv_matmul
Faild to find path v
Faild to find path v to Split
Faild to match concat node for Gather paths
Faild to match gemm gather path
Faild to match gemm path
Faild to match path 1 for unidirectional mask
Faild to match path 2 for unidirectional mask
Faild to match path 3 for unidirectional mask
Faild to match path 4 for unidirectional mask
Faild to match the path (Div-->Where-->Add) for unidirectional mask
Failed in match input mask subgraph
Failed in match Transpose attribute perm. Expected: 0, 2, 1, 3
Failed in match v_matmul and v_add input shape
Failed in match v_transpose attribute perm. Expected: 0, 2, 1, 3
Failed memory size calculation
Failed since multiple edges matched:
Failed to add kernel for 
Failed to allocate memory for requested buffer of size 
Failed to analyze start state.
Failed to construct locale with name:
Failed to convert dense initializer to sparse
Failed to convert mask to int32
Failed to copy tensor to 
Failed to create output tensor for 
Failed to create output tensor for If output 
Failed to create output tensor for output #
Failed to create the inter-op thread pool for the parallel executor, setting ExecutionMode to SEQUENTIAL
Failed to find a free memory block despite calling Extend. rounded_bytes=
Failed to find allocator for device 
Failed to find initializer for name: 
Failed to find initializer to reshape with name 
Failed to find input name in the mapping: 
Failed to find kernel def hash (
Failed to find kernel for 
Failed to find mask path
Failed to find path 1 of position shape.
Failed to find path 2 of position shape.
Failed to find path for k
Failed to find path for mask
Failed to find path for past_k
Failed to find path for present_k
Failed to find path for present_v and past_v
Failed to find path for q
Failed to find reshape shape path 1
Failed to find reshape shape path 2
Failed to find shape path
Failed to find Softmax node
Failed to find symbol in library, error code: 
Failed to get allocator for optimizer
failed to get first output!
Failed to get initializer tensor.
Failed to get position embedding weights.
Failed to get size of TensorProto
Failed to load model because protobuf parsing failed.
Failed to load model with error: 
Failed to load Q, K and V bias tensors, or data type is not float or float16.
Failed to load Q, K and V weights, or data type is not float or float16.
Failed to match position embedding subgraph.
Failed to match position subgraph.
Failed to match Shape node. 
Failed to match v_concat
Failed to obtain detect_negative
Failed to obtain detect_positive
Failed to parse model file!
Failed to parse model stream!
Failed to parse path root: 
Failed to remove node.
Failed to serialize model!
Failed to set node op schema.
Failed to unload DSO: 
Failed to write value with snprintf().
FailFast
false
false literal
fast_gelu_output
fast_shape.size() == 2
fast_shape.size() == 3
fast_shape[0] * fast_shape[2] == output.Shape().Size()
fast_shape[0] == output.Shape().Size()
fast_shape[1] == output.Shape().Size()
FastGelu
FastGeluFusion
FATAL
Fatal error: 
Fatal error: 0 count processors from GetLogicalProcessorInformation
Fatal error: 0 count processors from GetSystemInfo
fbs_attr cannot be null
fbs_node_arg_names cannot be null
FD iD
FD lD
FD(hD
FD(oD
FD@jD
FD@nD
FD`gD
FD`hD
Fd+F`
fD>0&
FD0kD
FD4gD
FD8rD
FDhpD
FDliD
FDplD
FDXmD
FDxoD
FDXqD
Feature id for each node.
FeatureVectorizer
feed_locations.size() == copy_info.size()
feeds.size() == feed_mlvalue_idxs.size()
feeds_fetches_manager_
feeds_fetches_manager_ && info_
fetch_alloc_info.size() == copy_info.size()
Fetches vector passed to GetOutputs contains 
fetches.empty() || fetches.size() == fetch_mlvalue_idxs_.size()
fetches.size() == node->OutputDefs().size()
fff?Qj
fff?QVh
fff?QVh0hI
fg:SM
FH+FDj
FHjhj
fhtxG
fhXCE
fhXLD
Field '
file exists
File is too large.
File not found!
file too large
file_path == nullptr
FileDescription
filename too long
FileVersion
filter number not equal to input channel number.
filter_info_ == nullptr
FilterScaleTensor
FilterTensor
FilterZeroPointTensor
final_output_mlvalue_
final_state_and_scan_outputs
FindClose
FindFirstFileW
FindNextFileW
First dimension (num_rois) of batch_indices and rois don't match
First input does not have rank 2
first input tensor has wrong dimension
First input tensor must have rank 3
First set of probability coefficients.
First, offset by this.<br>Can be length of features in an [N,F] tensor or length 1, in which case it applies to all features, regardless of dimension count.
Fl;Fpu
Fl;Vpt
Flag indicating whether the regression is a one-class SVM or not.
Flatten
FLOAT
float
float_data
float16
FLOATFLOATSGRAPHGRAPHSINTINTSSPARSE_TENSORSPARSE_TENSORSSTRINGSTRINGSTENSORTENSORSTYPE_PROTOTYPE_PROTOSUNDEFINED
floats
FLOATS
Floor
floor
FlsAlloc
FlsFree
FlsGetValue
FlsSetValue
FLSWj
Flush-to-zero and denormal-as-zero are 
fmod attribute must be true for floating point types
fmod must have value either 0 or 1
Fop_name
For each node, define what to do in the presence of a missing value: if a value is missing (NaN), use the 'true' or 'false' branch based on the value in this array.<br>This attribute may be left undefined, and the defalt value is false (0) for all nodes.
For each node, define what to do in the presence of a NaN: use the 'true' (if the attribute value is 1) or 'false' (if the attribute value is 0) branch based on the value in this array.<br>This attribute may be left undefined and the defalt value is false (0) for all nodes.
For example, the following tensor shapes are supported (with broadcast=1):
For internal use.
For map type num_values MUST be 2
for node: 
For ort_value with index: 
For previous (depreciated) non-spatial cases, implementors are suggested
forgot to update the version range in DomainToVersionRange 
Format() == SparseFormat::kBlockSparse
Format() == SparseFormat::kCoo
Format() == SparseFormat::kCsrc
Format() == SparseFormat::kUndefined
format_data_.size() == 1U
format_data_.size() == 2U
FormatMessageA
FormatMessageW
forward
Found '.' not part of an ellipsis in input: 
Found '.' not part of an ellipsis in the output subscript provided
Found a '.' not part of an ellipsis in input: 
Found a '.' not part of an ellipsis in the output subscript provided
found duplicated provider 
Found kernel for Op with name (
Found session/run/environment configuration in the model file to be used while running the model
found_in_outer_scope_location_map
Four modes: round_prefer_floor (default, as known as round half down), round_prefer_ceil (as known as round half up), floor, ceil. Only used by nearest interpolation. It indicates how to get "nearest" pixel in input tensor from x_original, so this attribute is valid only if "mode" is "nearest".
foward
Fp+Fl
frame != nullptr
FreeDimensionOverrideTransformer
FreeLibrary
FreeLibrary failed with error 
frPy=?=
ft&9q
func info for node: 
Function
function
Function body initialization failed for Function '
Function body initialization failed for node '
function not supported
Fused
fused 
fused Add and Gelu
fused Add-Dropout-(Add) for 
Fused an attention node for GPT.
Fused an attention node.
Fused Attention subgraphs 
fused Conv 
fused EmbedLayerNorm subgraphs 
fused Gelu subgraphs 
fused Gemm 
Fused Gemm with Sum
Fused Gemm with Transpose
fused GPT2Gelu subgraphs 
fused LayerNorm subgraphs 
fused Matmul and Add 
Fused MatMul and Scale
fused MatMul and Transpose 
fused op (
Fused reshape node: 
fused SkipLayerNorm subgraphs 
fused_
fused_activation
fused_activation_domain
fused_activation_since_version
fused_alpha
fused_beta
fused_function_subgraph
fused_gamma
fused_ratio
FusedActivation
FusedAdd
FusedAddTensor
FusedBatchNormalization
FusedConv
FusedConvTranspose
FusedGemm
FusedInstanceNormalization
FusedMatMul
FusedMeanVarianceNormalization
FusedNode's nodeArgList does not contain one of the nodeArg
FusedSum
FuseReluClip
FuseReluClip_
fusion_style == IExecutionProvider::FusionStyle::Function
fv_z~$
FVVVWQh
Fx;N|
g "i(
G _[]
G _^[
G$;G(u
G$9G$t
G(;G,u
G,+G(
G,+G(j
G;|$$
G;|$4
G;|$L
G;~$r
G@;C@u\
G@+G<
G@p#Q
G@PRC
G_^[]
G`p#Q
G|+Gx
G|+GxR
G|F+Gx
G<+G8
G<+G8j
G0+G,
G4;G8
G4+G0
G8;G<
G8_^[]
G8+G4
G88WP
G8F+G4
G8PRQS
gamma
Gamma
gamma is expected to have 1 dimension, got 
gamma is expected to have 1 dimensions, got 
gamma is expected to have size of 
Gamma scale must be a scalar or 1D tensor of size 1
Gamma should be of shape (hidden_size). 
gamma should have 2 dimension, dimension size known, and same hidden size as word_embedding.
Gamma zero point must be a scalar or 1D tensor of size 1
gamma_quant
gamma_scale
gamma_zero_point
gates
Gather
gather axis value not expected
gather indices not matched.
gather input 1 value is not expected
Gather node in path 2 is not linked to another subgraph.
Gather Tind type not supported in this build.
GatherElements
GatherElements op: Cannot operate on scalar input
GatherElements op: Data type of input 'data' should match the data type of the output
GatherElements op: 'indices' shape should have values within bounds of 'data' shape. Invalid value in indices shape is: 
GatherElements op: Rank of input 'data' needs to be equal to rank of input 'indices'
GatherElements op: Value in indices must be within bounds [
GatherND
GatherNDBase PrepareForCompute: Input count mismatch
Gaussian Error Linear Unit.
GD$hWP
GD;CDuL
GD+G@
GDp#Q
Gdp#Q
gDz>uR
Gelu approximation
GeluApproximation
GeluFusion
Gemm bias is not constant
Gemm bias is not constant initializer
Gemm bias shape is not expected
Gemm bias shape not expected
Gemm does not have 3 inputs
Gemm weight is not constant initializer
Gemm weight shape is not expected
GEMM: Dimension mismatch, W: 
Gemm: Invalid bias shape for broadcast
gemm_input_edge.src_arg_index < 2
GemmActivationFusion
GemmSumFusion
GemmTransposeFusion
GENERAL ERROR
generated at runtime
generic
Georgian
GetCPInfo
GetCurrentProcess
GetCurrentProcessId
GetCurrentProcessorNumber
GetCurrentThread
GetCurrentThreadId
GetEnvironmentVariableA
GetFileAttributesA
GetFileAttributesW
GetFileLength: File is too large
GetFileSizeEx
GetFileSizeEx 
GetFinalPathNameByHandle() failed: 
GetFinalPathNameByHandleW
GetFullPathNameW
GetFusedActivationAttr(info, activation_).IsOK()
GetLastError
GetLocaleInfoEx
GetLogicalProcessorInformation
GetModuleFileNameA
GetModuleFileNameW
GetModuleHandleExW
GetModuleHandleW
GetNativeSystemInfo
GetProcAddress
GetProcessHeap
GetProvider
GetSessionGetInputDevice
GetStringTypeW
GetSystemInfo
GetSystemTimeAsFileTime
GetSystemTimePreciseAsFileTime
GG2> 
GH;CHuT
gh`~G
GH+GD
Gh8!G
GHp#Q
Ghp#Q
gi^t8
Given `data` tensor of rank r >= 1, and `indices` tensor of rank q >= 1, gather
Given model could not be parsed while creating inference session. Error message: 
GivenTensorFill
GL;CLud
Gl_^[
GL+GH
Glagolitic
global
global_bias
global_weight
GlobalAveragePool
GlobalLpPool
GlobalMaxPool
GLp#Q
Got invalid dimensions for input: 
Got nullptr for sequence input.
Got nullptr from GetKernel for node: 
Got weights of size: 
Gothic
GP;FPt
GP+GL
Gp+Gl
Gp+O`+Gl
GPT2Gelu
G-r+o
GradientTensor
-grams
Grantha
Graph
graph
GRAPH
Graph attribute inferencing failed: 
Graph attribute inferencing returned type information for 
Graph attribute value was null. Invalid ORT format model.
Graph ctor should have created NodeArg for initializer. Missing:
Graph has 
Graph in 'body' attribute of Loop should have 
Graph initializer names must appear after the actual inputs: 
Graph is null. Invalid ORT format model.
Graph must be in single static assignment (SSA) form, however '
Graph state to be loaded into must be empty.
Graph to run if condition is false. Has N outputs: values you wish to be live-out to the enclosing scope. The number of outputs must match the number of outputs in the then_branch.
Graph to run if condition is true. Has N outputs: values you wish to be live-out to the enclosing scope. The number of outputs must match the number of outputs in the else_branch.
Graph transformers must be registered before the session is initialized.
graph.RemoveNode(gemm_node.Index())
graph.RemoveNode(sum_node.Index())
graph_->GetNode(idx) != nullptr
graph_index
graph_inputs_excluding_initializers_.empty() && graph_inputs_including_initializers_.empty() && value_info_.empty() && graph_outputs_.empty()
graph_optimization_level
graph_optimization_level is not valid
graph_optimization_level option in the model file must be an integer
graph_proto != nullptr
graph_proto cannot be null
graph_proto_ is not in sync with name_to_initial_tensor_.
GraphProto attribute inferencing is not enabled in this InferenceContextImpl instance.
GRAPHS
graphs
GraphTransformerHelpers::RegisterGraphTransformers
Greater
GreaterOrEqual
Greek
Grid batch size 
grid_dims[0] == N
grid_dims[3] == 2
GridSample
group
group count is <= 0
GroupCount
GrowStack() failed: 
GRU operator does not support double yet
GRUUnit
GRUUnit computes the activations of a standard GRU,
gsl::narrow_cast<int64_t>(input_axes_.size()) == num_scan_inputs_
gsl::narrow_cast<int64_t>(input_shape.Size()) == size
gsl::narrow_cast<int64_t>(output_axes_.size()) == num_scan_outputs
gsl::narrow_cast<int64_t>(tensor_shape.NumDimensions()) >= slice_dimension
gsl::narrow_cast<int64_t>(X_shape.NumDimensions()) >= axis
Gujarati
Gunjala_Gondi
Gurmukhi
GWWWh
GWWWj
GWWWS
GYY;~
GYYf;
h != kInvalidChunkHandle
h < chunks_.size()
H C;]
H|+Hx
half_pixel
Hangul
Hanifi_Rohingya
Hanunoo
Hardmax
Hardmax inputs N, D and N * D must be < 
HardSigmoid
hardsigmoid
HardSwish
has output size 
has_key_padding_mask
has_layer_state
has_starts && has_ends && attr_starts_.size() == attr_ends_.size()
HasDataType(dense_proto)
HasExclusiveProduct
HasExclusiveSum
Hatran
Having memory pattern enabled is not supported while using the DML Execution Provider. 
HC;^$|
'hd]G
HeapAlloc
HeapFree
Hebrew
height_scale
helper.HaveTwoTensorInputs()
hhxaD
hidden
Hidden layer sizes of Q, K, V paths in Attention
hidden_prev
hidden_size
hidden_size != num_heads * head_size
hidden_size should be divisiable by num_heads.
hidden_size should be divisiable by num_heads:
HiddenInitTensor
hipMalloc
Hiragana
Hl;Hp
host unreachable
However, the number of key is 
hResult
hrg,r
https://arxiv.org/abs/1502.03167. Depending on the mode it is being run,
https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html
Hx+Ht
i < input_shape.NumDimensions()
i < tensors_.size()
I(lA!%
I.e. the output shape should be [C][0] or [N][C][0] if input shape was [N][C].
I?Qdd*
I0`0p0
I0G1-0+
i1n1P2
id >= 0 && static_cast<size_t>(id) < ort_value_info_.size()
Identifier expected but not found.
identifier removed
Identity
IdentityTo
IExecutionProvider constructor must be called with true for use_metadef_id_creator
IExecutionProvider::Compile with fused Node and dll path is not implemented by 
IExecutionProvider::Compile with fused Node is not implemented by 
IExecutionProvider::Compile with FusedNodeAndGraph is not implemented by 
If `axes` are omitted, they are set to `[0, ..., ndim-1]`.
If 0, normalize the mean only.  Default is 1.
If 1, mean and variance are computed across channels. Default is 0.
If align_corners=1, the extrema (-1 and 1) are considered as referring to the center points of the input's corner pixels. If align_corners=0, they are instead considered as referring to the corner points of the input's corner pixels, making the sampling more resolution agnostic.
if coordinate_transformation_mode is "align_corners", <br/>
if coordinate_transformation_mode is "asymmetric", <br/>
if coordinate_transformation_mode is "half_pixel", <br/>
if coordinate_transformation_mode is "pytorch_half_pixel", <br/>
if coordinate_transformation_mode is "tf_crop_and_resize", <br/>
if coordinate_transformation_mode is "tf_half_pixel_for_nn", <br/>
If keepdims equal 0, then the resulting tensor have the reduced dimension pruned.
If necessary the right-hand-side argument will be broadcasted to match the
If node has 
'If' node has 
If scale is not provided, crop the borders as provided.
If set to 1 will perform the sums in reverse direction.
If set to 1 will return exclusive sum in which the top element is not included. In other terms, if set to 1, the j-th output element would be the sum of the first (j-1) elements. Otherwise, it would be the sum of the first j elements.
If set to 1, the weight of sampling locations outside the tensor will be set to 0 and the weight will be renormalized so that their sum is 1.0. The default value is 0.
If set to nonzero, run spatial batch normalization in test mode, default is 0.
If set to true then it indicates dropout is being used for training. It is an optional value hence unless specified explicitly, it is false. If it is false, ratio is ignored and the operation mimics inference mode where nothing will be dropped from the input data and if mask is requested as output it will contain all ones.
If set to true, it indicates BatchNormalization is being used for training, and outputs 1, 2, 3, and 4 would be populated.
If set, defines the broadcast dimensions.
If set, defines the broadcast dimensions. See doc for details.
If shape was concrete we shouldn't be using a custom allocator
If the tokenizer receives empty input of [0] then the output is [0] if empty input
If the value of map_form is 'SPARSE,' this attribute indicates the total length of the output tensor.
If tokenizer removes the entire content of [C]-input, it will produce [[]].
If true and category is not present, will return all zeros; if false and a category if not found, the operator will fail.
If true, check only for Inf, -Inf.
If true, check only for NaN.
If true, compute the mean and variance across all spatial elements If false, compute the mean and variance across per feature.Default is 1.
If true, compute the mean and variance across per activation. If false, compute the mean and variance across per feature over each mini-batch.
If value is 1, output type is uint32_t, else int32_t. Default value is 1.
ignore_index
Ignoring unsupported session option in ORT config: 
IijP7
Ijw3mG
illegal byte sequence
illegal input path:
ilogb
ilogbf
ImageScaler
Imperial_Aramaic
impl_->max_gram_length_ >= impl_->min_gram_length_
impl_->max_skip_count_ >= 0
impl_->min_gram_length_ > 0
impl_->weighting_criteria_ != kNone
impl_->weights_.size() == impl_->ngram_indexes_.size()
implementation such as CuDNN.
imputed_value_floats
imputed_value_int64s
imputed_values_float_.empty() ^ imputed_values_int64_.empty()
Imputer
in a sequence-length aware fashion.
in initializers. 
in onnx/defs/schema.h).
in the inclusive range [
in[idx]->IsTensor()
inappropriate io control operation
IncludePadding
Incompatible dimensions
Incompatible dimensions for matrix multiplication
Incompatible matrix dimensions for matMul
Inconsistent shape in loop output for output. 
Incorrect arena extend strategy.
Incorrect or missing attribute value for starts and ends
Incorrect or missing input value for starts and ends
index < data_.size()
index >= 0 && static_cast<size_t>(index) < inputs.size()
index >= 0 && static_cast<size_t>(index) < outputs.size()
index is out of bounds
index out of range
Index out of range
Index size: 
Index tensor shape should be same as that of the input data tensor to unpool.
IndexDimensions
IndexedSubGraph contains values not present in the Graph
Indicate up to which input dimensions (exclusive) should be flattened to the outer dimension of the output. The value for axis must be in the range [0, R], where R is the rank of the input tensor. When axis = 0, the shape of the output tensor is (1, (d_0 X d_1 ... d_n), where the shape of the input tensor is (d_0, d_1, ... d_n). 
Indicate up to which input dimensions (exclusive) should be flattened to the outer dimension of the output. The value for axis must be in the range [-r, r], where r is the rank of the input tensor. Negative value means counting dimensions from the back. When axis = 0, the shape of the output tensor is (1, (d_0 X d_1 ... d_n), where the shape of the input tensor is (d_0, d_1, ... d_n). 
Indicates the transform to apply to the regression output vector.<br>One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT'
Indicates the transform to apply to the score. <br> One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT.'
Indicates the transform to apply to the score. <br>One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT'
Indicates the transform to apply to the score. <br>One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT.'
Indicates the transform to apply to the scores vector.<br>One of 'NONE,' 'SOFTMAX,' 'LOGISTIC,' 'SOFTMAX_ZERO,' or 'PROBIT'
Indicates whether to do OvR or multinomial (0=OvR is the default).
Indicates whether to only output as many values as are in the input (dense), or position the input based on using the key of the map as the index of the output (sparse).<br>One of 'DENSE', 'SPARSE'.
indices
Indices
Indices and updates must have the same rank
Indices dim=
indices element out of data bounds, idx=
Indices must have the same rank as Input. Indices rank=
Indices shape must have dim[0] == 2
indices tensor data type not supported
indices tensor must has rank larger than 0
Indices tensor must have rank >= 1
Indices type is not supported.
Indices vs updates dimensions differs at position=
indices_shape[1] > 0 && static_cast<size_t>(indices_shape[1]) == dims.size()
IndicesDimensionCount
IndicesTensor
ineIu
InferenceSession is null. Invalid ORT format model.
Inferred elem type differs from existing elem type: (
Inferred shape and existing shape differ in dimension 
Inferred shape and existing shape differ in rank: (
InfinityMode
info == nullptr
info.GetAttr("alpha", &alpha_).IsOK()
info.GetAttr("beta", &beta_).IsOK()
info.GetAttr("blocksize", &blocksize_).IsOK()
info.GetAttr("direction", &direction).IsOK()
info.GetAttr("direction", &direction_).IsOK()
info.GetAttr("hidden_size", &hidden_size_).IsOK()
info.GetAttr("hidden_size", &int64_value).IsOK() && int64_value > 0
info.GetAttr("keepdims", &keepdims).IsOK()
info.GetAttr("linear_before_reset", &int64_value).IsOK()
info.GetAttr("num_heads", &num_heads).IsOK() && num_heads > 0
info.GetAttr("scale", &scale_).IsOK()
info.GetAttr("storage_order", &storage_order).IsOK()
info.GetAttr<float>("alpha", &alpha_).IsOK()
info.GetAttr<float>("beta", &beta_).IsOK()
info.GetAttr<float>("high", &high_).IsOK()
info.GetAttr<float>("low", &low_).IsOK()
info.GetAttr<float>("mean", &mean_).IsOK()
info.GetAttr<float>("scale", &scale_).IsOK()
info.GetAttr<float>("spatial_scale", &spatial_scale_).IsOK()
info.GetAttr<int64_t>("across_channels", &across_channels_).IsOK()
info.GetAttr<int64_t>("axis", &axis_).IsOK()
info.GetAttr<int64_t>("batch_axis", &batch_axis).IsOK()
info.GetAttr<int64_t>("count_include_pad", &temp).IsOK()
info.GetAttr<int64_t>("default_int64", &default_int_).IsOK()
info.GetAttr<int64_t>("dtype", &dtype).IsOK()
info.GetAttr<int64_t>("max_map", &max_map_).IsOK()
info.GetAttr<int64_t>("max_ngram_size", &max_ngram_size_).IsOK()
info.GetAttr<int64_t>("min_ngram_size", &min_ngram_size_).IsOK()
info.GetAttr<int64_t>("ngram_size", &ngram_size_).IsOK()
info.GetAttr<int64_t>("normalize_variance", &normalize_variance_).IsOK()
info.GetAttr<int64_t>("num_scan_inputs", &num_scan_inputs_).IsOK()
info.GetAttr<int64_t>("p", &p_).IsOK()
info.GetAttr<int64_t>("sample_size", &num_samples_).IsOK()
info.GetAttr<int64_t>("size", &size).IsOK()
info.GetAttr<int64_t>("targets", &num_targets_).IsOK()
info.GetAttr<int64_t>("time_axis", &time_axis).IsOK()
info.GetAttr<int64_t>("transA", &temp).IsOK()
info.GetAttr<int64_t>("transB", &temp).IsOK()
info.GetAttr<int64_t>("upper", &temp).IsOK()
info.GetAttr<ONNX_NAMESPACE::GraphProto>("body", &proto).IsOK()
info.GetAttr<ONNX_NAMESPACE::GraphProto>("else_branch", &proto).IsOK()
info.GetAttr<ONNX_NAMESPACE::GraphProto>("then_branch", &proto).IsOK()
info.GetAttr<std::string>("auto_pad", &auto_padding).IsOK()
info.GetAttr<std::string>("cast_to", &attr).IsOK()
info.GetAttr<std::string>("default_string", &default_string_).IsOK()
info.GetAttr<std::string>("equation", &equation_).IsOK()
info.GetAttr<std::string>("map_form", &attr).IsOK()
info.GetAttr<std::string>("metric", &metric).IsOK()
info.GetAttr<std::string>("mode", &mode).IsOK()
info.GetAttr<std::string>("norm", &norm).IsOK()
info.GetAttrs("activations", activations_).IsOK()
info.GetAttrs("axes", axes_).IsOK()
info.GetAttrs(std::is_same<AttrType, std::string>::value ? "string_vocabulary" : "int64_vocabulary", vocabulary_).IsOK()
info.GetAttrs<float>("bias", bias_).IsOK()
info.GetAttrs<float>("coefficients", coefficients_).IsOK()
info.GetAttrs<float>("kernel_params", kernel_params).IsOK()
info.GetAttrs<float>("rho", rho_).IsOK()
info.GetAttrs<float>("scales", scales_).IsOK()
info.GetAttrs<int64_t>("cats_int64s", int_categories).IsOK()
info.GetAttrs<int64_t>("kernel_shape", kernel_shape).IsOK()
info.GetAttrs<int64_t>("kernel_shape", kernel_shape_).IsOK()
info.GetAttrs<int64_t>("pooled_shape", pooled_shape).IsOK()
info.GetAttrs<int64_t>("shape", shape).IsOK()
info.GetAttrs<std::string>("cats_strings", string_categories).IsOK()
info.GetAttrs<std::string>("classes_strings", string_classes).IsOK()
info.GetAttrs<std::string>("classlabels_strings", classlabels_strings_).IsOK() || info.GetAttrs<int64_t>("classlabels_ints", classlabels_ints_).IsOK()
info.GetAttrs<TKey>(_key_field_name, keys).IsOK()
info.GetAttrs<TValue>(_value_field_name, values).IsOK()
info_ == nullptr
Inherited
initial_c
initial_chunk_size_bytes
initial_growth_chunk_size_bytes
initial_h
initial_state_and_scan_inputs
InitializeCriticalSectionAndSpinCount
InitializeCriticalSectionEx
Initialized tensor with unexpected type: 
Initializer 
Initializer tensor is missing. Invalid ORT format model.
Initializer with same name exists. Name:
initializer_node_arg != nullptr
InitializeSListHead
InitializeSRWLock
Initializing session.
InitOnceBeginInitialize
InitOnceComplete
Inner and Outer indices must either be both zero or non-zero
inner_num == src.Values().Shape().Size()
input
Input
Input 
input != nullptr
Input 0 and 1 shall have same shape
Input 0 and 7 (mask) shall have same shape
Input 0 is expected to have 1 or more dimensions, got 
Input 1 dimension 0 should have same length as dimension 2 of input 0
Input 1 dimension 0 should have same length as the last dimension of input 0
Input 1 dimension 1 should be 3 times of hidden dimension
Input 1 is expected to have 1 dimensions, got 
Input and output types can be of any tensor type.
Input and target dimension value mismatch.
input and zero_point pair is expected to have be same type.
input and zero_point pair is expected to have same type.
input array doesn't equal tensor size
input array is too short
Input axes has incorrect length
Input axes has invalid data
Input axis is invalid: 
Input B must have shape {
Input 'bias' dimension 0 should have same length as dimension 1 of input 'weights'
Input 'bias' is expected to have 1 dimension, got 
Input can be of any tensor type.
Input cannot be split evenly on selected axis. Input shape=
Input channels C is not equal to kernel channels * group.
Input channels is not divisible by group.
Input contains invalid utf8 chars
input count mismatch
input count mismatch, expected 1 input - the tensor to be processed
input count mismatch, expected 2 inputs - the tensor to be processed and a tensor containing k value
Input count of Tile OP mismatch, the first one is empty
Input count of Tile OP mismatch, the second one is empty
Input data tensor from the previous layer.
Input data tensor from the previous operator; 4-D feature map of shape (N, C, H, W), where N is the batch size, C is the number of channels, and H and W are the height and the width of the data.
Input data to be scaled
Input data type does not match the expected data type
Input data type does not match the expected data type. Current data type is 
Input data type is not int32 or int64
Input data with index: 
Input 'depth' must be a scalar or rank 1 tensor.
Input 'depth' must have exactly one element.
Input dim is zero but required output dim is non-zero. 
Input dimension cannot be less than 3.
Input dimensions are either [C] or [N][C] allowed
Input dimensions are either[C > 0] or [1][C > 0] allowed
input edges
Input element type of 
Input 'extra_add_qk' dimension 0 should be same as batch_size, got 
Input 'extra_add_qk' dimension 1 should be same as number of heads, got 
Input 'extra_add_qk' dimension 2 should be same as sequence_length, got 
Input 'extra_add_qk' dimension 3 should be same as sequence_length, got 
Input 'extra_add_qk' is expected to have 4 dimensions, got 
Input features_per_batch[
Input id is not valid. 
input index out of range
input index: 
Input initial_c must have shape {
Input initial_h must have shape {
Input 'input' is expected to have 3 dimensions, got 
Input is ether string UTF-8 or int32/int64
input is expected to have 3 dimensions, got 
Input is expected to have dim value in all dimensions.
Input is expected to have four dimensions corresponding to [N,C,H,W]
Input is expected to have four dimensions corresponding to [N,C,H,W], got 
Input is not of one of the supported map types.
Input is not of one of the supported sequence types.
Input is not of type sequence or map.
Input 'mask_index' is expected to have 1, 2, 3 or 4 dimensions, got 
Input must be an optional-type value containing an element with type information.
Input must be of COO format
Input must be of CSR format
'input' must have rank >= 2
input name cannot be empty
Input of int64 must have output of string 
Input of reshape_before_gemm is not the input of subgraph
Input of string must have output of int64
Input of tensor(int64) must have output of tensor(string)
Input of tensor(string) must have output of tensor(int64)
Input offset, 4-D tensor of shape (N, H_out, W_out, 2), where H_out and W_out are the height and width of grid and output, Grid specifies the sampling pixel locations normalized by the input spatial dimensions. Therefore, it should have most values in the range of [-1, 1]. If grid has values outside the range of [-1, 1], the corresponding outputs will be handled as defined by padding_mode.
Input P must have shape {
Input 'past' is expected to have 5 dimension, got 
Input R must have shape {
Input rank for starts and ends should be the same: (
Input rank must be >= 2.
Input scale is not float for input def @
Input scale is not float for quantized input @
input scale must be a scalar or 1D tensor of size 1
Input 'scales' must have float element type.
Input Sequence and Tensor are expected to have the same elem type. Sequence=
Input Sequence and Tensor are expected to have type info. Current type is null.
Input sequence_lens must have shape {
Input shape dimensions mismatch:
Input shape had more than 2 dimension. Dims=
Input shape is unknown or not 2D, or data type unknown
Input shape must have either [C] or [1,C] dimensions where C > 0
Input shape must have either [C] or [B,C] dimensions with B > 0.
Input shape needs to be at least a single dimension.
input shape: 
Input 'sizes' must have int64 element type.
Input 'split' can not be empty.
Input steps has incorrect length
Input string contains invalid utf8 chars
Input string contains invalid utf8 chars: 
input tensor
Input tensor
Input tensor A. The shape of A should be (M, K) if transA is 0, or (K, M) if transA is non-zero.
input tensor and indices tensor must has rank larger than 0. 
Input tensor B. The shape of B should be (K, N) if transB is 0, or (N, K) if transB is non-zero.
Input tensor C. The shape of C should be unidirectional broadcastable to (M, N).
Input tensor can be of arbitrary type.
Input tensor has no dimensions
Input tensor must be 2-dimensional
Input tensor must be 4-dimensional
Input tensor must have at least 2 dimensions
Input tensor must have atleast 2 dimensions
Input tensor must have rank 1 or 2
Input tensor must have rank 2
Input tensor of rank 2 or higher.
Input tensor of shape [N,C,H,W]
Input tensor should have a rank of at least 2
Input tensor to Unique op should be 1D
Input tensor X must have atleast 2 dimensions.
Input tensor.
Input tensor. Every matrix in the batch must be invertible.
Input tensors of wrong rank (0).
Input tensors to check.
Input to 'Range' op should be scalars (Tensor with only one element and shape empty)
Input to set must exist.
Input type for input at index 
Input type for input at index 0 is null. Type info is expected.
Input type is not float tensor but keys_floats is set
Input type is not int64 tensor but keys_int64s is set
Input type is not string tensor but key_strings is set
Input type is null. Input must have Type information.
Input type is null. Type information is expected for the input.
Input type was null
Input 'values' must be rank 1 tensor.
Input 'values' must have exactly two elements.
Input W must have shape {
Input was expected to have either tensor, sequence, or optional type. Got 
Input was expected to have optional type. Got 
Input was expected to have sequence type. Got 
Input was expected to have tensor or sparse tensor type. Got 
Input 'weights' is expected to have 2 dimensions, got 
Input with name: 
Input X must have 3 dimensions only. Actual:
Input x_scale must be a scalar or 1D tensor of size 1
input x_zero_point must be a scalar or 1D tensor of size 1 if given
input y_scale must be a scalar or 1D tensor of size 1
input y_zero_point must be a scalar or 1D tensor of size 1 if given
input zero point must be a scalar or 1D tensor of size 1.
Input/Output is a string tensor
Input: 
input_0
input_1
input_1.DataType() == input_2.DataType()
input_arg->Type() != nullptr
input_as_shape
input_copy_needed != DeviceCopyCheck::Unknown && output_copy_needed != DeviceCopyCheck::Unknown
input_count >= 0 && static_cast<size_t>(input_count) == input_dimensions_.size()
input_count >= 1
input_count_x3 >= 6 && input_count_x3 % 3 == 0
input_def_count >= 8 && (input_def_count - 2) % 3 == 0
input_dims.size() >= 2
input_dims[rank - 2] == input_dims[rank - 1]
input_forget
input_gather_element
input_gather_element_transform
input_ids
input_ids and position_ids shall have same shape
Input_ids and segment id should have the same shape. 
input_ids is expected to have 2 dimensions, got 
input_ids shall be 2 dimensions
input_ids_dims.size() == 2
input_indices.size() == expected_values.size() && input_indices.size() > 0
input_mean
input_node.InputDefs().size() == 2 && scale_and_index->second < 2
input_num_bytes % 4 == 0
input_offset >= 0 && output_offset >= 0
input_ptr
input_rank == permutation.size()
input_rank == reference_rank
input_scale
input_sequence
input_shape.Size() > 0 || input_shape[0] == 0
input_shape.Size() > 0 || N == 0
input_shape[i] == 1
input_shape_1_override.size() == 3 && input_shape_2_override.size() == 3
input_shape_1_override[0] == input_shape_2_override[0]
input_shape_1_override[2] == input_shape_2_override[1]
input_size < std::numeric_limits<std::ptrdiff_t>::max()
input_tensor != nullptr && indices_tensor != nullptr
input_tensor_ptr != nullptr
input_var
'input_var'.
input_zero_point
input->Exists()
InputBroadcaster can only start at span boundary!
InputCount
inputCount >= 1
InputDimensionCount
inputdimensions
inputdimensions attribute must be provided
InputFirstMomentTensor
InputGradientTensor
InputParametersTensor
InputPixelOffset
InputPixelOffsets
Inputs
inputs
Inputs 0 shall be 3 dimensions
Inputs 4 shall be 5 dimensions
inputs are expected to have tensor type and output type should not be null.
inputs are expected to have tensor type.
inputs by their magnitude, rather than gates inputs by their sign as in ReLUs.
Inputs have ellipses in them but the provided output subscript does not contain an ellipsis
Input's height (
Inputs 'mask_index' with 1D data shall have length of batch_size or 2 * batch_size
Inputs 'mask_index' with 2D data shall have shape batch_size x (past_sequence_length + sequence_length)
Inputs 'mask_index' with 3D data shall have shape batch_size x sequence_length x (past_sequence_length + sequence_length)
Inputs 'mask_index' with 4D data shall have is_unidirectional_ set to false
Inputs 'mask_index' with 4D data shall have shape batch_size x 1 x max_sequence_length x max_sequence_length)
Inputs 'past' dimension 0 shall have length of 2
Inputs 'past' dimension 1 shall have same length as dimension 0 of input 0
Inputs 'past' dimension 2 shall have length of 
Inputs 'past' dimension 2 shall have length of num_heads
Input's shape must be 4-D
Input's shape should be 1D or 2D
Input's width (
InputScaleTensor
InputSecondMomentTensor
InputStateTensor
InputTensor
InputTensors
InputWindowOffsets
InputWindowSizes
InputWindowStrides
InputZeroPointTensor
Inscriptional_Pahlavi
Inscriptional_Parthian
Insert and concatenate on a new axis or not, default 0 means do not insert new axis.
InsertCastTransformer works on the assumption that `dtype` attribute holds an integer.
inserted
InsertedCast_
InstanceNormalization
Insufficient dimensions to slice on 
int16
int32
int32_data
int64
Int64 tensor
int64_data
int64_vocabulary
Integer indicate the format of the box data. The default is 0. 0 - the box data is supplied as [y1, x1, y2, x2] where (y1, x1) and (y2, x2) are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval [0, 1]) or absolute. Mostly used for TF models. 1 - the box data is supplied as [x_center, y_center, width, height]. Mostly used for Pytorch models.
Integer overflow
Integer representing the embedding vector size for each char.If not provide, use the char embedding size of embedding vector.
Integer representing the embedding vector size for each word.If not provide, use the fileter size of conv weight
Integer value expected, but not found.
inter_op_num_threads
inter_op_num_threads option in the model file must be an integer
intercepts
InterlockedFlushSList
internal error
Internal error in BatchNormalizationMulFusion. BatchNormalization_B_tensor_proto is NULL
Internal error. The preallocated buffer is too small. Requires 
InternalName
InternalTestingExecutionProvider
inter-op
-inter-op
InterpolationMode
interrupted
intra_op_num_threads
intra_op_num_threads option in the model file must be an integer
intra-op
-intra-op
inv_std_var
Invalid activation function of 
Invalid allocation kind: 
invalid allocator.
Invalid arg_num of 
invalid argument
Invalid argument for depth; it's not a scalar.
Invalid argument for values; either it's rank is more than 1 or it has more than 2 elements
Invalid argument: input has empty dimensions.
Invalid argument: X input has empty dimensions.
Invalid assumption of output element size
Invalid attribute perm {
Invalid axes attribute, axes attribute (if present) should have the same size as starts/ends attributes
Invalid batch_axis of 
Invalid bias shape
invalid BOM; must be 0xEF 0xBB 0xBF if given
Invalid CAST_TO value of 
invalid character class
invalid character class range
invalid comment; expecting '/' or '*' after '/'
invalid comment; missing closing '*/'
Invalid data type 
Invalid data type for GRU operator of 
Invalid data type for LSTM operator of 
Invalid data type for split tensor 
Invalid data type of 
Invalid DataTypeImpl TypeProto definition
Invalid destination node arg slot specified when adding edge.
Invalid destination node arg slot specified when removing edge.
Invalid dim0_offset of 
Invalid dimension of 
Invalid dimension value: 
Invalid 'direction' argument of '
Invalid direction value of '
Invalid dtype of 
Invalid 'end'. Value is larger than 'start'.
Invalid entries in sequence_lens. Max sequence length was 
invalid escape sequence
Invalid ExecutionOrder
invalid expand shape
Invalid fd was supplied: 
Invalid Feed Input Name:
Invalid free dimension override.
Invalid GRU hidden gate activation function: 
Invalid GRU reset gate activation function: 
invalid hash bucket count
invalid index 
invalid index found, index = 
Invalid index requested for map type.
Invalid index: 
invalid indice found, indice = 
Invalid input B: 
Invalid input B: 0th dimension != 
Invalid input B: number of dimensions is not 1: 
Invalid input B: NumDimensions() != 
Invalid input data: number of dimensions is less than 3: 
Invalid input index for node 
Invalid input mean: 
Invalid input mean: 0th dimension != 
Invalid input mean: NumDimensions() != 
Invalid input scale: 
Invalid input scale: 0th dimension != 
Invalid input scale: number of dimensions is not 1: 
Invalid input scale: NumDimensions() != 
Invalid input shape. Only N can be zero. Got:
Invalid input shape: 
Invalid input type of value: 
Invalid input type:
Invalid input var: 
Invalid input var: 0th dimension != 
Invalid input var: NumDimensions() != 
Invalid key found: 
invalid literal
invalid location range
Invalid LSTM merge activation function of 
invalid map<K, T> key
Invalid 'mode' attribute value
Invalid mode of value 
Invalid model. Node input '
invalid named capture group
Invalid node indexes specified when adding edge.
Invalid node indexes specified when removing edge.
Invalid normalize value of 
Invalid number of outputs for BN training
invalid number; expected '+', '-', or digit after exponent
invalid number; expected digit after '-'
invalid number; expected digit after '.'
invalid number; expected digit after exponent sign
Invalid ORT format model.
Invalid Output Name:
Invalid PACK_MAP value of 
Invalid 'pads' attribute value
invalid perl operator
Invalid position of 0
Invalid position of 0.
Invalid program_counter entries at index 
Invalid rank for 
Invalid rank for input: 
Invalid RE2: 
invalid repetition size
Invalid roi input index.
Invalid run log severity level. Not a valid onnxruntime::logging::Severity value: 
Invalid scan input:
invalid seek
Invalid sequence index (
Invalid sequence length: 
Invalid session log severity level. Not a valid onnxruntime::logging::Severity value: 
Invalid shape value: 
Invalid source node arg slot specified when adding edge.
Invalid source node arg slot specified when removing edge.
Invalid SparseTensor indices. INT16 indices must be in the raw data of indices tensor
Invalid SparseTensor indices. INT8 indices must be in the raw data of indices tensor
Invalid SparseTensor indices. Should be rank 0 or 1. Got:
Invalid SparseTensor indices. Should one of the following types: int8, int16, int32 or int64
Invalid 'start'. Value is smaller than previous 'end'.
Invalid start/ending offset [
invalid stod argument
invalid stof argument
invalid stol argument
invalid stoll argument
invalid stoull argument
invalid string position
invalid string: '\u' must be followed by 4 hex digits
invalid string: control character U+0000 (NUL) must be escaped to \u0000
invalid string: control character U+0001 (SOH) must be escaped to \u0001
invalid string: control character U+0002 (STX) must be escaped to \u0002
invalid string: control character U+0003 (ETX) must be escaped to \u0003
invalid string: control character U+0004 (EOT) must be escaped to \u0004
invalid string: control character U+0005 (ENQ) must be escaped to \u0005
invalid string: control character U+0006 (ACK) must be escaped to \u0006
invalid string: control character U+0007 (BEL) must be escaped to \u0007
invalid string: control character U+0008 (BS) must be escaped to \u0008 or \b
invalid string: control character U+0009 (HT) must be escaped to \u0009 or \t
invalid string: control character U+000A (LF) must be escaped to \u000A or \n
invalid string: control character U+000B (VT) must be escaped to \u000B
invalid string: control character U+000C (FF) must be escaped to \u000C or \f
invalid string: control character U+000D (CR) must be escaped to \u000D or \r
invalid string: control character U+000E (SO) must be escaped to \u000E
invalid string: control character U+000F (SI) must be escaped to \u000F
invalid string: control character U+0010 (DLE) must be escaped to \u0010
invalid string: control character U+0011 (DC1) must be escaped to \u0011
invalid string: control character U+0012 (DC2) must be escaped to \u0012
invalid string: control character U+0013 (DC3) must be escaped to \u0013
invalid string: control character U+0014 (DC4) must be escaped to \u0014
invalid string: control character U+0015 (NAK) must be escaped to \u0015
invalid string: control character U+0016 (SYN) must be escaped to \u0016
invalid string: control character U+0017 (ETB) must be escaped to \u0017
invalid string: control character U+0018 (CAN) must be escaped to \u0018
invalid string: control character U+0019 (EM) must be escaped to \u0019
invalid string: control character U+001A (SUB) must be escaped to \u001A
invalid string: control character U+001B (ESC) must be escaped to \u001B
invalid string: control character U+001C (FS) must be escaped to \u001C
invalid string: control character U+001D (GS) must be escaped to \u001D
invalid string: control character U+001E (RS) must be escaped to \u001E
invalid string: control character U+001F (US) must be escaped to \u001F
invalid string: forbidden character after backslash
invalid string: ill-formed UTF-8 byte
invalid string: missing closing quote
invalid string: surrogate U+D800..U+DBFF must be followed by U+DC00..U+DFFF
invalid string: surrogate U+DC00..U+DFFF must follow U+D800..U+DBFF
Invalid Target shape product of 0
Invalid Target shape product of 0. Product cannot be 0 in combination with -1
Invalid tensor data type 
Invalid tensor shape slice argument.
Invalid TensorProto
Invalid time_axis of 
Invalid type
invalid unordered_map<K, T> key
Invalid usage. Input 1 is a shape with no data.
invalid UTF-8
Invalid value for attribute axis
Invalid value for attribute k
Invalid value in scan_input_axes for input 
Invalid value in scan_output_axes for output 
Invalid value in 'split' attribute. All values must be > 0
Invalid value in 'split' input. All values must be >= 0
Invalid value of attribute 'axis'. Accepted range=[
Invalid value of attribute 'axis'. Rank=
Invalid value(
Invalid value/s in sequence_lens. All values must be > 0 and < seq_length. seq_length=
Invalid values in '
invalid vector subscript
Invalid Y argument: index is out of range: Y[
Invalid Y argument: num_indices = 0
INVALID_ARGUMENT
INVALID_GRAPH
invalid_iterator
INVALID_PROTOBUF
Inverse
inverse_indices
InvStdDev
InvStdDev = Reshape (InvStdDev2D, ReducedShape)
InvStdDev2D = Reciprocal (StdDev)
io error
ios_base::badbit set
ios_base::eofbit set
ios_base::failbit set
iostream
iostream stream error
iou_threshold
iou_threshold must be in range [0, 1].
Irfft
irVersion
is a directory
is applied to the data tensor elementwise.
is applied to the tensor elementwise.
'is defined.
is not supported.
is_case_sensitive
is_concrete_shape_
is_model_proto_parsed
is_spatial_
is_test
IsAllFinite
IsBQuantParamSupported(b_offset->Shape(), b ? b->Shape() : b_shape_)
IsBQuantParamSupported(b_scale->Shape(), b ? b->Shape() : b_shape_)
IsBQuantParamSupported(b_zero_point->Shape(), b ? b->Shape() : b_shape_)
IsBQuantParamSupported(b_zp_tensor->Shape(), b_tensor ? b_tensor->Shape() : b_shape_)
IsDebuggerPresent
IsInf
isinf_only
ISink must be provided.
IsNaN
isnan_only
IsOptionalSeqTensor(type)
IsOptionalTensor(type)
IsProcessorFeaturePresent
isRedist
IsSameDataType(tensor)
IsScalarOr1ElementVector(a_offset)
IsScalarOr1ElementVector(a_scale)
IsScalarOr1ElementVector(a_zero_point)
IsScalarOr1ElementVector(a_zero_point_tensor)
IsScalarOr1ElementVector(a_zp)
IsScalarOr1ElementVector(k)
IsScalarOr1ElementVector(tensor_a_scale)
IsScalarOr1ElementVector(tensor_b_scale)
IsScalarOr1ElementVector(tensor_c_scale)
IsScalarOr1ElementVector(tensor_x_scale)
IsScalarOr1ElementVector(tensor_x_zero_point)
IsScalarOr1ElementVector(tensor_y_scale)
IsScalarOr1ElementVector(tensor_y_zero_point)
IsScalarOr1ElementVector(W_Zero_Point)
IsScalarOr1ElementVector(X_scale)
IsScalarOr1ElementVector(X_zero_point)
IsScalarOr1ElementVector(X_Zero_Point)
IsScalarOr1ElementVector(y_offset)
IsScalarOr1ElementVector(Y_scale)
IsScalarOr1ElementVector(y_scale)
IsScalarOr1ElementVector(Y_zero_point)
IsSparseTensor()
IsTensor()
IsTensorSequence()
IsValidQuantParam(W_scale, M)
IsValidQuantParam(W_zero_point, M)
it->i < (int64_t)predictions.size()
iteration_num_ < sequence_len_
iterator does not fit current value
iterator out of range
itr != node_args.end()
It's an extension of Gelu. It takes the sum of input A and bias input B as the input of Gelu activation. 
j h _G
j h@BG
j hh"G
j hXrF
j Y+M
j Z_^
j!hP`E
j"hx>E
j#hXrF
j$hPKF
j$Z_[
j%hx>E
j%hxDG
j&hPKF
j&hxdF
j&hXrF
J(+J$
j(hPKF
j(Y!>
j)hx>E
j*hhkF
J/deE
j/h8&G
j/hPKF
j?h0DF
j?hPXE
j?hxLE
J@QPR
j[h0DF
j[hXwG
j_hXrF
j`hXrF
j{hxLE
J|+Jx
J|hp#Q
j+h }F
j+hhkF
j<hXrF
j=hXrF
J>f;O
j>hXwG
j1hPKF
J1x1a2
j3hPKF
j7h0@E
j8hxdF
j8hxLE
jA^f;1
ja^f;1w
jAh<_E
jAh0DF
jahP3D
Javanese
jaZf;Q
jAZf;Q
jCh ;G
jCh _G
jCh0DF
jChXwG
JDhp#Q
jDhXwG
jEhXwG
jGhXwG
j'hX,F
j'hxdF
jhXPj
jJhx>E
jjjjj
JLhp#Q
jlhXrF
jLhXrF
jMh ;G
jMh\YE
jMh`GE
jMh0DF
jMhX?F
jNh@BG
jNhXWE
job_.size() = 
johXrF
joZRY
jp_h$
jpZjoY
jqh _G
jrhxdF
jrhXrF
jSh`GE
jshxdF
Json stored in the `ort_config` key cannot be parsed. Error message: 
jThHrG
jtYjt
jUh ;G
jUh@BG
juhxdF
jVh85F
jWh0DF
Jx;J|t8
jx^Pj
Jxhp#Q
jXhXCE
jYh0DF
jyhP D
jzhXWE
K +K$
k and v are not from same Split node
k argument [
K input must be a one-dimensional tensor of size 1.
K input must be of type int64.
k root is not layer norm
k should be a 1-D or 0-D tensor.
k tensor should be a 1D tensor of size 1
K,;K0t
k_matmul and k_add shape not matched
k_reshape const not matched
k_temp > 0
k_transpose has not perm attribute
k_transpose perm attribute not matched
K|- 6
K<WPQ
K0\0{0
k0D3[3
K0w4u5
K89K@
K8WPQ
Kaithi
Kannada
Katakana
Kayah_Li
Keep the reduced dimension or not, default 1 mean keep reduced dimension.
Keep the split dimension or not. Default 1, which means we keep split dimension. If input 'split' is specified, this attribute is ignored.
keepdims
Kernel
kernel != nullptr
Kernel create info hashes are null. Invalid ORT format model.
Kernel create info is null. Invalid ORT format model.
Kernel create info node indices are null. Invalid ORT format model.
kernel def can't be NULL
Kernel not found
kernel_info != nullptr
kernel_params
kernel_shape
kernel_shape is not compatible with W shape.
kernel_shape num_dims is not compatible with W num_dims.
kernel_shape num_dims is not compatible with X num_dims.
kernel_shape[dim] > 0
kernel_shape_[dim] > 0
kernel_type
KERNEL32.DLL
kernel32.dll
kernelbase.dll
key '
key and value cache dimensions value shall not be null
key and value cache shall be 4 dimensions
Key and value tensors have unequal number of elements.
Key type is not supported yet.
key_cache
key_padding_mask
key_type
keys_floats
keys_int64s
keys_strings
Kharoshthi
Khitan_Small_Script
Khmer
Khojki
KHQhp#Q
Khudawadi
known by the checker.
Kp`LE
kRegexpCapture cap() == 0
kv_weight
KX;K\t
L$ ;|$$
L$ ;N
L$$;C
L$$;G
L$$;K
L$$;L$0
L$$;w
L$$^3
L$$_^[3
L$$_^3
L$(;K
L$(;U$
L$(|;
L$(|3
L$(|Q
L$(9L$,
L$,;K
L$,^3
L$,_^[3
L$,_^3
L$,RP
L$,xO
L$@;L$(
L$@|N
L$@9L$$
L$\_^[3
L$\_^3
L$\QP
L$`;M
L$`9L$@
L$`9L$<
L$`9L$H
L$`9M
L$`WR
L$|;M(
L$|_^[3
L$|_^3
L$<;N
L$<^3
L$<_^[3
L$<_^3
L$<|N
L$=QP
L$0;F
L$0;K
L$0;M
L$0;O
L$0|Q
L$091}
L$091~
L$09M
L$09y
L$0RV
L$4 D$
L$4;N
L$4^3
L$4_^
L$4_^[3
L$48X
L$8;H
L$8;J
L$8;M0
L$D;N
L$D_^[3
L$d_^[3
L$D_^[3
L$d_^[3
L$D_^3
L$D|C
L$D9U
L$d9U
L$h9|
L$H9L$$
L$H9L$,
L$h9L$D
L$h9L$H
L$HWQ
L$HxK
L$L;T$
L$l_^[3
L$L_^[3
L$l_^[3
L$L_^[3
L$l_^[3
L$L_^[3
L$L_^3
L$L|I
L$L+L$
L$LRPV
L$lSP
L$lWRPu
L$P9L$ 
L$P9L$$
L$P9L$(
L$P9L$,
L$P9L$0
L$P9L$4
L$p9L$H
L$PxK
L$T_^[3
L$t_^[3
L$T_^[3
L$t_^[3
L$T_^[3
L$t_^[3
L$X9L$$
L$X9L$(
l?4?~?
l?4?~?4?~?4?~?4?~?
L@I#s
L>2?~>2?~>2?~>2?~>
L>2?~>p
l013Q4
L0Q0@6
l1t1|1
L2P2T2X2
Label encoder has only one input.
Label encoder has only one output.
LabelEncoder
labels
lambd
largest
largest <= 1
last >= first
Last dimension of `indices` input tensor in GatherND op must not be larger than the rank of `data` tensor
Last dimension of beta and input does not match
Last dimension of bias and input does not match
Last dimension of gamma and input does not match
Last dimension of grid: 
last dimension of indices must not be larger and rank of data tensor
last dimension of indices must not be larger than rank of input tensor
last_loop_red_size > 0
last_loop_size > 0
last_outputs[j + 1].IsTensor()
Latin
layer_norm_out
layernorm_out
LayerNormalization
LayerNormFusion
layout
layout_ == 0
LCMapStringEx
lda >= K && ldb >= K && ldc >= N
LeakyRelu
leakyrelu
LearningRate
LeaveCriticalSection
left operand cannot broadcast on dim 
Left shape: 
left.NumDimensions() == 2 || left.NumDimensions() == 1
left.Shape().Size() == left_shape_override.Size()
left_dim == right_dim
left_num_dims and right_num_dims must be >= 1
left_rank == right_rank
legacy optimization attribute.
LegalCopyright
len <= op_schema.inputs().size()
len >= 0 && static_cast<uint64_t>(len) < std::numeric_limits<size_t>::max()
length
length > buffer.size()
length of each output
length of each output. Values should be >= 0.
Length of permutation must match the rank of the input to be permutated
length overflow
lengths allocation failed
Lepcha
LessOrEqual
Level
Limbu
limit
limit in Range operator should be scalar like tensor, yet got shape:
LINEAR
Linear
linear
'Linear' mode only support:
Linear_A
Linear_B
linear_before_reset
LinearBeforeReset
LinearClassifier
LinearRegressor
list count 
List of 3 elements containing gamma, coef0, and degree, in that order. Zero if unused for the kernel.
List of categories, ints.<br>One and only one of the 'cats_*' attributes must be defined.
List of categories, strings.<br>One and only one of the 'cats_*' attributes must be defined.
list of floats. This attribute stores the weight of each n-gram in pool. The i-th element in weights is the weight of the i-th n-gram in pool. Its length equals to the size of ngram_indexes. By default, weights is an all-one tensor.This attribute is used when mode is "IDF" or "TFIDF" to scale the associated word counts.
List of int64 n-grams learned from the training set. Either this or pool_strings attributes must be present but not both. It's an 1-D tensor starting with the collections of all 1-grams and ending with the collections of n-grams. The i-th element in pool stores the n-gram that should be mapped to coordinate ngram_indexes[i] in the output vector.
list of int64s (type: AttributeProto::INTS). This list is parallel to the specified 'pool_*' attribute. The i-th element in ngram_indexes indicate the coordinate of the i-th n-gram in the output tensor.
List of integers indicate the padding element count at the beginning and end of each axis, for 2D it is the number of pixel. `paddings` rank should be double of the input's rank. `paddings` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`.
List of integers indicating the dimensions to be inserted. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(expanded).
List of integers indicating the dimensions to squeeze. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data).
List of integers indicating the number of padding elements to add or remove (if negative) at the beginning and end of each axis. For 2D it is the number of pixels. `pads` rank should be double of the input's rank. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`.
List of non-negative integers, indicate the dimensions to be inserted
List of non-negative integers, indicate the dimensions to squeeze.
List of stop words. If not set, no word would be removed from X.
List of strings n-grams learned from the training set. Either this or pool_int64s attributes must be present but not both. It's an 1-D tensor starting with the collections of all 1-grams and ending with the collections of n-grams. The i-th element in pool stores the n-gram that should be mapped to coordinate ngram_indexes[i] in the output vector.
List of tensors for 
list too long
LO_M$
load external data into raw data for tensor: 
Load model 
Load model from 
loadedFrom
loading_ort_format && serialized_session_state != nullptr
LoadLibrary failed with error 
LoadLibraryExA
LoadLibraryExW
LoadLibraryW
LoadNodeArgsFromOrtFormat: Node [
Local\SM0:%d:%d:%hs
locale
LocalFree
LocalSize
localtime_s(&local_tm, &in_time_t) == 0
location
location dimensions do not match shape size
log_prob
LogHr
LOGISTIC
LogSoftmax
LogStart must pair with LogEnd
Long tensor containing the indices to extract from embedding matrix.
LongformerAttention
Loop 'body' subgraph outputs should all be tensors but output 
Loop 'body' subgraph outputs should all be tensors or sequences but output 
Loop 'body' subgraph outputs should all be tensors or sequences or optionals, but output 
Loop 'body' subgraph scan outputs should all be tensors but output 
Loop had zero iterations and the shape of subgraph output 
'Loop' input 'cond' should be a scalar tensor. Got shape of 
'Loop' input 'M' should be a scalar tensor. Got shape of 
'Loop' node has 
Loop subgraph input 0 has unknown shape: 
Loop subgraph input 1 has unknown shape: 
loss_N1dd
loss_NCdd
loss_Ndd
loss_sum
loss_unweighted
LOWER
Lower boundary of the output values.
LPhp#Q
LpNormalization
LpPool
lstd::exception: %hs
LSTM operator does not support double yet
Lycian
Lydian
M$@Pj
M$PRV
M$SV3
M(PSW
m?I)y
M_ == 1 && N_ == 1 was false
M_ >= 0 && K_ > 0 && N_ >= 0
m_dims_with_pad - 2 != num_output_dims
M~D(]k
M0;M,|:
M0+M(
M0K0I
Mahajani
Main Graph instance should have populated all subgraphs when being resolved.
Makasar
Malayalam
Malformed repeat 
Mandaic
Manichaean
Map is missing type entry for its value
map(int64, double)
map(int64, float)
map(int64, string)
map(string, double)
map(string, float)
map(string, int64)
map/set too long
map_form
map_form_ != PACK_MAP::SPARSE || max_map_ > 0
map_type
MapFileIntoMemory is not implemented on Windows.
Marchen
Masaram_Gondi
Mask data type is not int32 or int64 or float32
Mask is neither unidirectional nor all ones
Mask shape is unknown or not 2D, or data type unknown
mask_index
mask_index_out
Mask_Int32
mask_mul const input not matched
mask_sub const input not matched
mask_unsqueeze_1 axes not matched. Expect: 1
mask_unsqueeze_2 axes not matched. Expect: 2
MaskCast
Match contains invalid utf8 chars: 
Matched 
MatchInputMaskSubgraph returns false
MatchPastSubgraph returns false
MatchUnidirMaskSubgraph returns NULL
MatMul
MatMul dimension mismatch
MatMul_With_Transpose
MatMulAddFusion
MatMulInteger
MatmulInteger : B zero point is not valid
MatmulInteger : b zero point is not valid
MatmulInteger : input1 A_scale must be a scalar or 1D tensor of size 1
MatmulInteger : input1 A_zero_point must be a scalar or 1D tensor of size 1 if given
MatmulInteger : input1 B_scale must be a scalar or 1D tensor of size 1
MatmulInteger : input1 B_zero_point must be a scalar or 1D tensor of size 1 if given
MatmulInteger : input1 C_scale must be a scalar or 1D tensor of size 1
MatmulInteger : input1 C_zero_point must be a scalar or 1D tensor of size 1 if given
MatmulInteger : input1 zero point must be a scalar or 1D tensor of size 1
MatMulInteger16
MatMulIntegerToFloat
MatMulIntegerToFloat : input a zero point must be a scalar or 1D tensor of size 1. Per-Channel is not supported yet.
MatMulIntegerToFloatFusion
MatMulScaleFusion
MatmulTransposeFusion
Matrix dimensions are not equal. Square matrix is expected
Matrix multiply results
Matrix multiply results from A * B
Matrix product that behaves like numpy.matmul: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.matmul.html
Matrix product that behaves like numpy.matmul: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.matmul.html.
max should be a scalar.
max_dead_bytes_per_chunk
max_gram_length
max_gram_length must be inbounds of ngram_counts: 
max_map
max_map must be > 0 if map_form is SPARSE
max_mem
max_ngram_size
max_ngram_size_ > 0
max_ngram_size_ >= min_ngram_size_
max_output_boxes_per_class
max_skip_count
max_skip_count is required
max_skip_count must be non-negative: 
max->Shape().IsScalar()
Maximum n-gram length. If this value is 3, 3-grams will be used to generate the output.
Maximum number of events reached, could not record profile event.
Maximum number of items (integers/strings) to be skipped when constructing an n-gram from X. If max_skip_count=1, min_gram_length=2, max_gram_length=3, this operator may generate 2-grams with skip_count=0 and skip_count=1, and 3-grams with skip_count=0 and skip_count=1
Maximum value, above which element is replaced by max
MaximumSamplesPerOutput
MaxPool
MaxpoolWithMask
MaxRoiPool
MaxUnpool
MaxUnpool op must have either two or three inputs.
mdk07
Mean = Reshape (Mean2D, ReducedShape)
Mean2D = ReduceMean <axes = [1]> (XU)
MeanOfSquare = ReduceMean <axes = [1]> (Square)
MeanTensor
MeanVarianceNormalization
Medefaidrin
Meetei_Mayek
Mem pattern should be disabled when using DML execution provider.
mem_steps <= max_memory_steps_ && mem_steps > 0
Memcpy
MemcpyFromHost
MemcpyToHost
MemcpyTransformer
Memory pattern planner is not enabled on this execution framework.
memory.enable_memory_arena_shrinkage
memory_seq_lens
Mende_Kikakui
Meroitic_Cursive
Meroitic_Hieroglyphs
message size
metadef_id_generator_
Method IncrementIndexAndComputeOffset assumes this value is strictly positive.
metric
Mhd_F
MhX@D
Microsoft
Microsoft Corporation
Microsoft Corporation1
Microsoft Corporation1&0$
Microsoft Corporation1)0'
Microsoft Corporation1.0,
Microsoft Corporation1200
Microsoft Time-Stamp PCA 2010
Microsoft Time-Stamp PCA 20100
Microsoft Time-Stamp Service
Microsoft Time-Stamp Service0
Microsoft Windows0
Microsoft.ML.ONNXRuntime
MIGraphXExecutionProvider
min should be a scalar.
min_ <= max_
min_gram_length
min_gram_length >= max_gram_length required: 
min_gram_length is required
min_gram_length must be inbounds of ngram_counts: 
min_ngram_size
min_ngram_size_ > 0
min->Shape().IsScalar()
mincharnum
mincharnum is too big for char level tokenezation
mincharnum_ > 0
Minimum n-gram length. If this value is 2 and max_gram_length is 3, output may contain counts of 2-grams and 3-grams.
Minimum number of characters allowed in the output. For example, if mincharnum is 2, tokens such as "A" and "B" would be ignored
Minimum value, under which element is replaced by min
MinimumSamplesPerOutput
MinMaxDataType
Mismatch between expected shape and shape from first output
Mismatch between Graph and IndexedSubGraph. Input not found:
Mismatch between Graph and IndexedSubGraph. Node not found: 
Mismatch between Graph and IndexedSubGraph. Output not found:
Mismatch between input data and B: size of B != input channel count 
Mismatch between input data and scale: size of scale != input channel count 
Mismatch between number of source and target dimensions. Source=
Mismatch between number of splits (
Mismatch between source and target type. Source=
Mismatch between the sum of 'split' (
Mismatched attribute type in '
Mismatched data types between input and output Tensors. 
Mismatched sparse tensor element type:
Mismatched tensor element type for output 
Mismatched tensor element type:
Mismatched type for output 
Mismatched type:
missing )
missing ]
Missing case in Compiler: 
Missing dimensions for initializer. Invalid ORT format model.
Missing dims for sparse initializer: 
Missing 'equation' attribute
Missing indicies for sparse initializer: 
Missing Input: 
Missing model IR version.
Missing Model. Invalid ORT format model.
Missing name for SparseTensor initializer. Invalid ORT format model.
Missing opset in the model. All ModelProtos MUST have at least one entry that specifies which version of the ONNX OperatorSet is being imported.
Missing or invalid starts and ends attribute
Missing raw data for initializer. Invalid ORT format model.
Missing session state for subgraph. Node:'
Missing string data for initializer. Invalid ORT format model.
Missing values for sparse initializer. Invalid ORT format model.
Missing/Invalid 'axes' attribute value
Missing/Invalid 'axis' attribute value
Misuse of LoopStateVariable. Attempt to move beyond end of sequence
ml_type != nullptr
MLDataType for: 
mlvalue.Fence() == nullptr
mode "
mode attribute is 
mode is required
mode: 
mode_str == "bilinear" || mode_str == "nearest" || mode_str == "bicubic"
Model file not found!
model format error!
model format error! Missing 'location'
model format error! Need a key for the external data info
model format error! Need a value for the external data info
Model must have opset imports. Invalid ORT format model.
Model was not loaded
Model was not loaded.
MODEL_LOADED
model_loading_array
model_loading_from_saved_proto
model_loading_proto
model_loading_uri
model_path must not be empty. Ensure that a path is provided when the model is created or loaded.
model_run
modelDomain
modelGraphName
modelMetaData
modelProducerName
modelProducerVersion
ModelProto corresponding to the model to be loaded has already been parsed. Invoke Load().
ModelProto corresponding to the model to be loaded has not been parsed yet. This API should be called in conjunction with a ctor that takes a model abstraction.
ModelProto does not have a graph.
ModelProto needs to be parsed to check for ORT config within it
momentum
Mongolian
More work items than threads
Move it out of graph inputs if there is no need to override it, 
Msg:[%ws] 
mul_B_tensor_proto
mul_inputs.size() == 2
MulInteger
Multani
multi_class
MultiByteToWideChar
Multinomial
Multiple entries for operator is not supported. OpType=
Multiple errors were found.
multiplication
Multiplicative spatial scale factor to translate ROI coordinates from their input scale to the scale used when pooling.
Multiplicative spatial scale factor to translate ROI coordinates from their input spatial scale to the scale used when pooling, i.e., spatial scale of the input feature map X relative to the input image. E.g.; default is 1.0f. 
MurmurHash3
Must be a scalar or 1D tensor or size 1.
must be overloaded.
Must contain BlockSparse format. Got: 
Must contain Coo format. Got: 
Must contain Csr format. Contains: 
Must have 1 or more inputs
Must have a single dimension
Must have a single dimension of 1
Must have a valid data type
Must have a valid input shape.
Must have the same shape
Must have the same size. Got src_size: 
Must have valid 'axis' attribute
Must provide classlabels_strings or classlabels_int64s but not both.
Must provide imputed_values_float_ or imputed_values_int64_ but not both.
Must use Function based fusion when exporting compiled nodes to dll.
mutually equal shape is specified by the argument "axis", and if it is not set,
Myanmar
n <= num_threads_+1
n >= 0
n >= 0 && static_cast<size_t>(n) < ort_value_info_.size()
n >= 0 && static_cast<size_t>(n) < plan_.allocation_plan.size()
n@/)@
n_supports
n_targets
n_targets_or_classes > 0
N|+Nx
n>019
N0L0J
N8;N<t
N8;N0
Nabataean
name:
name: 
Nandinagari
naxes > 0
NchwcTransformer
N-dimensional dense matrix B
N-dimensional matrix A
N-dimensional matrix B
nearest
NEAREST
nearest_mode
nearest_mode:[
Negative index values are not permitted. First entry in map has index value of 
Negative ngram_indexes values are not allowed
Negative values are not allowed in a shape specification
NegativeLogLikelihoodLoss
Nested parallelism not supported
network down
network reset
network unreachable
New shape
new_axis
new_axis must be either 0 or 1
new_gemm_input_defs.size() == 3
new_gemm_output_defs.size() == 1
new_key_cache
new_num_elts == old_num_elts
New_Tai_Lue
new_value_cache
n-gram counts out of bounds for 
ngram_counts
ngram_indexes
ngram_indexes must be non-empty with no negative values
ngram_size
ngram_size_ > 0
NGramRepeatBlock
NH;OHt*h
Nhttp://www.microsoft.com/pkiops/crl/Microsoft%20Time-Stamp%20PCA%202010(1).crl0l
NhwcMaxPool
NhwcTransformer
nJ)[/&6
njob_ = 
NLjt[
NnapiExecutionProvider
No allocator for this device has been registered for sharing.
no argument for repetition operator
No attribute with name:'
No attribute with name: 
No attribute with this name is defined.
no buffer space
no child process
no error
No Graph instance was found for attribute 
No graph was found in the protobuf.
No kernel shape is set.
no link
no lock available
No matching 'start' entry.
no message
no message available
No NodeArg found for name 
No Op registered for 
No opset import for domain '
No opset registered for domain 
no protocol option
No provider specified.
No ranges in char class
No requested allocator available
no space on device
no stream resources
no such device
no such device or address
no such file or directory
no such process
NO_MODEL
NO_SUCHFILE
Node 
Node (
Node [
Node id for each node. Ids may restart at zero for each tree, but it not required to.
Node id for each node. Node ids must restart at zero for each tree and increase sequentially.
node id that this weight is for.
Node index is out of range
Node is missing. Invalid ORT format model.
Node must only have one used output
Node placements
node.GetAttributeNameToMutableSubgraphMap().empty()
Node:
Node::LoadEdgesFromOrtFormat, edge is missing for 
Node::LoadFromOrtFormat, input_arg_counts is missing
node_arg
node_arg_ != nullptr
node_arg_name cannot be null
node_in_parent_graph->InputDefs().size() == function_body_graph.GetInputsIncludingInitializers().size()
node_in_parent_graph->OutputDefs().size() == function_body_graph.GetOutputs().size()
node_index < nodes_.size()
node_index_info and ort_value_idx_map are out of sync and cannot be used
node_index_info_
node_index_info_.GetMaxMLValueIdx() == ort_value_idx_map.MaxIdx()
node_offsets_index < node_offsets_size_
node->GetOutputEdgesCount() == 0
nodearg
NodeArg is missing. Invalid ORT format model.
NodeArg Name is missing. Invalid ORT format model.
NodeEdge is missing. Invalid ORT format model.
NodeProto (name: 
Nodes in a graph must be topologically sorted, however input '
nodes_.size() < static_cast<unsigned int>(std::numeric_limits<int>::max())
nodes_falsenodeids
nodes_falsenodeids.size() == nodes_featureids.size()
nodes_falsenodeids.size() == nodes_modes.size()
nodes_falsenodeids.size() == nodes_nodeids.size()
nodes_falsenodeids.size() == nodes_treeids.size()
nodes_falsenodeids.size() == nodes_truenodeids.size()
nodes_falsenodeids.size() == nodes_values.size()
nodes_featureids
nodes_hitrates
nodes_missing_value_tracks_true
nodes_modes
nodes_nodeids
nodes_treeids
nodes_truenodeids
nodes_values
Non concat axis dimensions must match: Axis 
Non per-tensor quantization is not supported now.
non_tensor_base != nullptr
Non-empty ngram_counts is required
Non-empty ngram_indexes is required
non-empty pool_int64s is required if pool_strings not provided
NonMaxSuppression
NonZero
Non-zero status code returned while running 
noop_with_empty_axes
NoopElimination
normalize_variance
normalized
Normalized = Div (Deviation, StdDev)
NormalizedT = Cast (Normalized)
Normalizer
NormalizeVariance
not a directory
not a socket
not a stream
Not able to find appropriate IDataTransfer to copy sparse data
Not all dimensions to be reduced have been reduced in the candidate output. Candidate output dims: 
not connected
Not eliminating output 
Not enough elements in dilations. Expected: 
Not enough elements in kernel shape. Expected: 
Not enough elements in pads. Expected: 
Not enough elements in strides. Expected: 
not enough memory
not enough space: expected 
Not expecting an allocator set
Not implemented
not implemented
not support normalize yet.
Not supported
not supported
Not supported with filtered graph.
NOT_IMPLEMENTED
NOT_SET
Notations:
Note that 'input_mean' and 'input_var' are expected to be the estimated
Notice that ReduceVar refers to the population variance, and it equals to
NOTSET
NotWhereFusion
Np;Vl
NPu53
nteltF=Authu
Null batch_indices_ptr
Null crop_size_ptr
Null entry in dimensions. Invalid ORT format model.
Null entry in metadata_props. Invalid ORT format model.
Null floats attribute. Invalid ORT format model.
Null graph attribute. Invalid ORT format model.
Null input ptr
Null input X ptr
Null ints attribute. Invalid ORT format model.
null literal
Null map type info. Invalid ORT format model.
Null rois_ptr
Null sequence type info. Invalid ORT format model.
NULL state in RunStateOnByte
Null string attribute. Invalid ORT format model.
Null string in strings attribute. Invalid ORT format model.
Null strings attribute. Invalid ORT format model.
Null tensor attribute. Invalid ORT format model.
Null tensor in tensors attribute. Invalid ORT format model.
Null tensor type info. Invalid ORT format model.
Null tensors attribute. Invalid ORT format model.
Null type info for 
Null value type info in fbs::MapType. Invalid ORT format model.
Null value type info in fbs::SequenceType. Invalid ORT format model.
nullj
nullptr != func_meta_def
nullptr != p.output_tensor
nullptr != tensor_type_base
nullptr != type_proto
nullptr == p_data
num_axes > 0
num_broadcasted_indices < num_of_ellipsis_dims_
num_categories_ > 0
num_classes is < 1
num_dims_with_pad - 1 != num_output_dims
num_dims_with_pad != num_output_dims
num_entries == int_categories.size()
num_explicit_inputs == static_cast<size_t>(target_input_idx)
num_features == feature_count_
num_heads
num_inputs >= 1
num_keys == num_values
num_samples is < 1
num_scan_inputs
num_subgraph_outputs - 1 == num_outputs
num_subgraph_outputs == static_cast<size_t>(num_outputs)
num_variadic_inputs == num_subgraph_inputs
number
number literal
Number of attention heads
Number of dimensions for batch indices should be exactly 1
Number of dimensions for crop size should be exactly 1
Number of dimensions for rois should be exactly 
number of elements in this dimension), it represents `n`. For slicing to the
Number of elements of attribute 'scales' must be same as rank of input 'X'
Number of elements of input 'scales' must be same as rank of input 'X'
Number of elements of input 'sizes' must be same as rank of input 'X'
Number of entries in '
Number of entries in 'scan_input_axes' was 
Number of entries in 'scan_output_axes' was 
number of groups input channels and output channels are divided into.
number of groups input channels and output channels are divided into. default is 1.
Number of input tensors does not match the operands in the equation.
Number of inputs (
Number of items must compose whole 
Number of neurons in the hidden layer
Number of neurons in the hidden layer.
Number of sampling points in the interpolation grid used to compute the output value of each pooled output bin. If > 0, then exactly sampling_ratio x sampling_ratio grid points are used. If == 0, then an adaptive number of grid points are used (computed as ceil(roi_width / output_width), and likewise for height). Default is 0.
Number of scan input axes specified (
Number of scan output axes specified (
Number of subscripts in the input equation does not match number of input tensors
Number of times to sample.
Number of top elements to retrieve
Number of values should be at least 1.
number overflow parsing '
NumCapturesWalker::ShortVisit called
NumReducedAxes = Neg (Axis1D)
NumReducedAxes = Sub (Rank, Axis1D)
NupharExecutionProvider
Nushu
Nyiakeng_Puachue_Hmong
O ;O t
O _^[]
O"*!U{
O$;C,tB
O(9O$t'+O$
O~x ~
O0M0K
O0QPQ
O2 4l6
o3m4w5a7
O8_^[
object
object key
object separator
Od+O`
of [N, 0] then [N, 0].
Offset
offset
offset % span_size_ == 0
offset + size <= size_t(span.size())
offset < 0
offset >= 0 && static_cast<size_t>(offset) < node_values_size_
Offsets
offsets buffer is not equal to tensor size
Ogham
Ol_Chiki
Old_Hungarian
Old_Italic
Old_North_Arabian
Old_Permic
Old_Persian
Old_Sogdian
Old_South_Arabian
Old_Turkic
OLEAUT32.dll
One (or two if bidirectional) activation function for input gate. The activation function must be one of the activation functions specified above. Optional: Default `Tanh` if not specified.
One and only one of the attributes 'value', 'value_*' or 'sparse_value' must be specified for a Constant node.
One and only one of the 'cats_*' attributes must be defined
One falsenode is pointing either to itself, either to another tree.
One float, indicates the value to be filled, default is 0
One float, indicates the value to be filled.
One of 'MAX,' 'L1,' 'L2'
One of the attributes 'value' or 'sparse_value' must be specified for a Constant node.
One sided attention windows length W, or half of total window length
one_class
OneHot
OneHot node must have three inputs.
OneHotEncoder
onesided
Only 1 batch dimension is allowed for MatMul
Only 4-D tensor is supported
Only bool
Only CPU allocators can be shared between multiple sessions for now.
Only CPU devices are supported for now.
Only one instance of LoggingManager created with InstanceType::Default can exist at any point in time.
Only one node should produce an output. Existing entry for 
Only one of keys_*'s can be set in label encoder.
Only one of scales or sizes must be provided as input.
Only one of the attributes 'value' or 'sparse_value' must be specified for a Constant node.
Only one of values_*'s can be set in label encoder.
Only one thread was configured for parallel execution. Hence will use sequential execution.
Only ONNX MLDataType can be registered
Only Optional type OrtValues containing Tensors and Sequence Tensors are acceptable
Only supports `int32_t` or `int64_t` inputs for split
Only supports `int32_t` or `int64_t` inputs for starts/ends/axes/steps
Only tensors are supported for external outputs for now.
Only tensors, tensor sequence, optional tensor, and optional tensor sequence types are supported
Only works on matrices with two dimensions.
ONNX Runtime
ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 
ONNX Runtime only *guarantees* support for models stamped with opset version 7 or above for opset domain 'ai.onnx'. Please upgrade your model to opset 7 or higher. For now, this opset 
ONNX Schema 
onnx.AttributeProto
onnx.FunctionProto
onnx.GraphProto
onnx.ModelProto
onnx.NodeProto
onnx.OperatorSetIdProto
onnx.SparseTensorProto
onnx.StringStringEntryProto
onnx.TensorAnnotation
onnx.TensorProto
onnx.TensorProto.Segment
onnx.TensorShapeProto
onnx.TensorShapeProto.Dimension
onnx.TrainingInfoProto
onnx.TypeProto
onnx.TypeProto.Map
onnx.TypeProto.Opaque
onnx.TypeProto.Optional
onnx.TypeProto.Sequence
onnx.TypeProto.SparseTensor
onnx.TypeProto.Tensor
onnx.ValueInfoProto
ONNX_NAMESPACE::TensorProto::DataType_IsValid(dtype_) && dtype_ != ONNX_NAMESPACE::TensorProto::UNDEFINED
ONNX_NAMESPACE::TensorProto::DataType_IsValid(output_dtype_) && output_dtype_ != ONNX_NAMESPACE::TensorProto::UNDEFINED
ONNX_NAMESPACE::TensorProto::DataType_IsValid(t_proto.data_type())
onnxruntime
onnxruntime.dll
onnxruntime.pdb
onnxruntime::`anonymous-namespace'::AssignNodesToEpsFromHashes
onnxruntime::`anonymous-namespace'::AssignNodesToEpsFromHashesImpl
onnxruntime::`anonymous-namespace'::Cast::Cast
onnxruntime::`anonymous-namespace'::CastToString
onnxruntime::`anonymous-namespace'::ConstantOfShape::Compute
onnxruntime::`anonymous-namespace'::CopyData
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<__int64>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<double>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<float>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<int>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<struct onnxruntime::BFloat16>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<struct onnxruntime::MLFloat16>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<unsigned __int64>::operator ()
onnxruntime::`anonymous-namespace'::ExtractScalarAsFloatDispatchTarget<unsigned int>::operator ()
onnxruntime::`anonymous-namespace'::GetClipConstantMinMax::<lambda_706aa48b16f5813a9c1cc576607ea9a2>::operator ()
onnxruntime::`anonymous-namespace'::GetCurrentTimeString
onnxruntime::`anonymous-namespace'::GetInputNodeMerges
onnxruntime::`anonymous-namespace'::GetOutputNodeMerges
onnxruntime::`anonymous-namespace'::GetRatioOrDefault
onnxruntime::`anonymous-namespace'::GetScalarConstantInitializer
onnxruntime::`anonymous-namespace'::GetScaleFromNode
onnxruntime::`anonymous-namespace'::MoveInputOutputImpl
onnxruntime::`anonymous-namespace'::ParsePathRoot
onnxruntime::`anonymous-namespace'::PartitionOrtFormatModel
onnxruntime::`anonymous-namespace'::TraverseFormalParametersWithTypeProto
onnxruntime::`anonymous-namespace'::VerifyEachNodeIsAssignedToAnEp
onnxruntime::`anonymous-namespace'::WindowsEnv::DeleteFolder
onnxruntime::`anonymous-namespace'::WindowsEnv::FormatLibraryFileName
onnxruntime::`anonymous-namespace'::WindowsEnv::GetCanonicalPath
onnxruntime::`anonymous-namespace'::WindowsEnv::GetNumCpuCores
onnxruntime::`anonymous-namespace'::WindowsEnv::ReadFileIntoBuffer
onnxruntime::`anonymous-namespace'::WindowsThread::WindowsThread
onnxruntime::AccumulateAllNestedSubgraphsInfo
onnxruntime::AllocateSparseTensor
onnxruntime::AllocatorManager::InsertAllocator
onnxruntime::AllocPlanPerValue::ProgramCounter::AddEnd
onnxruntime::AllocPlanPerValue::ProgramCounter::AddStart
onnxruntime::ApiGraph::CopyValueInfo
onnxruntime::ApiGraph::GetValueInfo
onnxruntime::ApiGraph::ReshapeInitializer
onnxruntime::ApiGraph::TransposeInitializer
onnxruntime::ApiTensor::Data
onnxruntime::ApiTensor::NumElements
onnxruntime::ApiValueInfo::PermuteDims
onnxruntime::AttentionFusion::ApplyImpl
onnxruntime::AttentionFusion::FuseSubGraph
onnxruntime::AttentionFusionHelper::CheckDistilBertReshapeShape
onnxruntime::AttentionFusionHelper::CheckNodesInPathK
onnxruntime::AttentionFusionHelper::CheckNodesInPathQ
onnxruntime::AttentionFusionHelper::CheckNodesInPathV
onnxruntime::AttentionFusionHelper::CheckSliceParameters
onnxruntime::AttentionFusionHelper::FuseGptAttention
onnxruntime::AttentionFusionHelper::MatchGemmSubgraph
onnxruntime::AttentionFusionHelper::MatchInputMaskSubgraph
onnxruntime::AttentionFusionHelper::MatchPastSubgraph
onnxruntime::AttentionFusionHelper::MatchUnidirMaskSubgraph
onnxruntime::AttentionFusionHelper::ValidateGemmInitializer
onnxruntime::AttentionFusionHelper::ValidateUnidirMask
onnxruntime::BatchNorm<double>::BatchNorm
onnxruntime::BatchNorm<double>::Compute
onnxruntime::BatchNorm<float>::BatchNorm
onnxruntime::BatchNorm<float>::Compute
onnxruntime::BFCArena::AllocateRawInternal
onnxruntime::BFCArena::AllocationRegion::AllocationRegion
onnxruntime::BFCArena::AllocationRegion::IndexFor
onnxruntime::BFCArena::BFCArena
onnxruntime::BFCArena::ChunkFromHandle
onnxruntime::BFCArena::DeallocateRawInternal
onnxruntime::BFCArena::Extend
onnxruntime::BFCArena::Extend::<lambda_7230621d141a3a3097634b97b00dbfd7>::operator ()
onnxruntime::BFCArena::FindChunkPtr
onnxruntime::BFCArena::FreeAndMaybeCoalesce
onnxruntime::BFCArena::InsertFreeChunkIntoBin
onnxruntime::BFCArena::Merge
onnxruntime::BFCArena::RegionManager::RegionFor
onnxruntime::BFCArena::RegionManager::RemoveAllocationRegion
onnxruntime::BFCArena::RemoveFreeChunkFromBin
onnxruntime::BFCArena::RemoveFreeChunkIterFromBin
onnxruntime::BFCArena::Reserve
onnxruntime::BFCArena::Shrink
onnxruntime::BFCArena::SplitChunk
onnxruntime::BiasDropoutFusion::ApplyImpl
onnxruntime::BiasGeluFusion::ApplyImpl
onnxruntime::BiasSoftmaxFusion::ApplyImpl
onnxruntime::BitShift<unsigned __int64>::BitShift
onnxruntime::BitShift<unsigned __int64>::Compute::<lambda_91eefeb04a6b3331ce7d69cfb07d2f86>::operator ()
onnxruntime::BitShift<unsigned char>::BitShift
onnxruntime::BitShift<unsigned char>::Compute::<lambda_8ec84423321713518948f166ef49c9bf>::operator ()
onnxruntime::BitShift<unsigned int>::BitShift
onnxruntime::BitShift<unsigned int>::Compute::<lambda_5511229e878cc46c7126f212269f5ffa>::operator ()
onnxruntime::Broadcaster::Broadcaster
onnxruntime::BroadcastIterator::Append
onnxruntime::BroadcastIterator::Init
onnxruntime::BroadcastLooper
onnxruntime::CheckInput
onnxruntime::Clip::ComputeImpl<__int64>::operator ()
onnxruntime::Clip::ComputeImpl<double>::operator ()
onnxruntime::Clip::ComputeImpl<float>::operator ()
onnxruntime::Clip::ComputeImpl<signed char>::operator ()
onnxruntime::Clip::ComputeImpl<unsigned __int64>::operator ()
onnxruntime::Clip::ComputeImpl<unsigned char>::operator ()
onnxruntime::clip_internal::Clip_6Base<float>::Clip_6Base
onnxruntime::common::Status::Status
onnxruntime::CommonSubexpressionElimination::ApplyImpl
onnxruntime::Compress::Compute
onnxruntime::ComputePadAndOutputShape
onnxruntime::ConcatBase::ComputeImpl
onnxruntime::ConcatBase::ConcatBase
onnxruntime::ConcatBase::PrepareForCompute
onnxruntime::ConcatFromSequence::Compute
onnxruntime::concurrency::CreateThreadPoolHelper
onnxruntime::concurrency::ThreadPool::ParallelFor
onnxruntime::concurrency::ThreadPool::ParallelSection::ParallelSection
onnxruntime::concurrency::ThreadPoolProfiler::MainThreadStat::LogEnd
onnxruntime::concurrency::ThreadPoolProfiler::MainThreadStat::LogEndAndStart
onnxruntime::concurrency::ThreadPoolProfiler::MainThreadStat::Reset
onnxruntime::concurrency::ThreadPoolProfiler::Stop
onnxruntime::concurrency::ThreadPoolTempl<class onnxruntime::Env>::RunInParallel
onnxruntime::concurrency::ThreadPoolTempl<class onnxruntime::Env>::RunInParallelSection
onnxruntime::ConfigOptions::AddConfigEntry
onnxruntime::ConstantFolding::ApplyImpl
onnxruntime::ConstantOfShapeBase<struct onnxruntime::TypeList<__int64,struct onnxruntime::MLFloat16,float,double,signed char,short,int,unsigned char,unsigned short,unsigned int,unsigned __int64,bool> >::ConstantOfShapeBase
onnxruntime::ConstantOfShapeBase<struct onnxruntime::TypeList<__int64,struct onnxruntime::MLFloat16,float,double,signed char,short,int,unsigned char,unsigned short,unsigned int,unsigned __int64,bool> >::PrepareCompute
onnxruntime::ConstantOfShapeBase<struct onnxruntime::TypeList<__int64,struct onnxruntime::MLFloat16,float,double,signed char,short,int,unsigned char,unsigned short,unsigned int,unsigned __int64,bool> >::SetValueFromTensorProto
onnxruntime::ConstPointerContainer<class std::vector<class onnxruntime::NodeArg *,class std::allocator<class onnxruntime::NodeArg *> > >::at
onnxruntime::contrib::`anonymous-namespace'::QLinearImpl
onnxruntime::contrib::`anonymous-namespace'::SparseToDenseCoo<__int64>::operator ()
onnxruntime::contrib::`anonymous-namespace'::SparseToDenseCoo<double>::operator ()
onnxruntime::contrib::`anonymous-namespace'::SparseToDenseCoo<float>::operator ()
onnxruntime::contrib::`anonymous-namespace'::SparseToDenseCoo<int>::operator ()
onnxruntime::contrib::`anonymous-namespace'::SparseToDenseCoo<unsigned __int64>::operator ()
onnxruntime::contrib::`anonymous-namespace'::SparseToDenseCoo<unsigned int>::operator ()
onnxruntime::contrib::Affine<float>::Affine
onnxruntime::contrib::Attention<float>::Compute
onnxruntime::contrib::AttentionBase::AttentionBase
onnxruntime::contrib::AttentionBase::GetPresent
onnxruntime::contrib::AttentionCPUBase::ApplyAttention
onnxruntime::contrib::BahdanauAttention<float>::BahdanauAttention
onnxruntime::contrib::BahdanauAttention<float>::PrepareMemory
onnxruntime::contrib::BiasGelu<float,0>::Compute
onnxruntime::contrib::BiasGelu<float,1>::Compute
onnxruntime::contrib::BifurcationDetector::BifurcationDetector
onnxruntime::contrib::BifurcationDetector::Compute
onnxruntime::contrib::CDist<double>::CDist
onnxruntime::contrib::CDist<float>::CDist
onnxruntime::contrib::Crop<float>::Compute
onnxruntime::contrib::CropAndResize<float>::CropAndResize
onnxruntime::contrib::DeepCpuAttnLstmOp::Compute
onnxruntime::contrib::DeepCpuAttnLstmOp::ComputeImpl
onnxruntime::contrib::DeepCpuAttnLstmOp::DeepCpuAttnLstmOp
onnxruntime::contrib::DeepCpuAttnLstmOp::ValidateInputs
onnxruntime::contrib::DynamicQuantizeLSTM::PrePack
onnxruntime::contrib::DynamicQuantizeMatMul::Compute
onnxruntime::contrib::EmbedLayerNorm<float>::Compute
onnxruntime::contrib::EmbedLayerNormBase::EmbedLayerNormBase
onnxruntime::contrib::ExpandDims::Compute
onnxruntime::contrib::FusedConvFloat::FusedConvFloat
onnxruntime::contrib::FusedGemm<float>::FusedGemm
onnxruntime::contrib::GridSample<float>::Compute
onnxruntime::contrib::GridSample<float>::GridSample
onnxruntime::contrib::ImageScaler<float>::ImageScaler
onnxruntime::contrib::LayerNorm<double,0>::Compute
onnxruntime::contrib::LayerNorm<double,0>::LayerNorm
onnxruntime::contrib::LayerNorm<double,1>::Compute
onnxruntime::contrib::LayerNorm<double,1>::LayerNorm
onnxruntime::contrib::LayerNorm<float,0>::Compute
onnxruntime::contrib::LayerNorm<float,0>::LayerNorm
onnxruntime::contrib::LayerNorm<float,1>::Compute
onnxruntime::contrib::LayerNorm<float,1>::LayerNorm
onnxruntime::contrib::MatMulInteger16<short,short,int>::Compute
onnxruntime::contrib::MatMulIntegerToFloat::Compute
onnxruntime::contrib::MatMulIntegerToFloatBase::ComputeCommon
onnxruntime::contrib::MaxpoolWithMask::Compute
onnxruntime::contrib::MurmurHash3::Compute
onnxruntime::contrib::NGramRepeatBlock::Compute
onnxruntime::contrib::NGramRepeatBlock::Compute::<lambda_611887ee8e00747c2e4fe06c46473fdf>::operator ()
onnxruntime::contrib::NGramRepeatBlock::NGramRepeatBlock
onnxruntime::contrib::NhwcMaxPool::Compute
onnxruntime::contrib::QAttention<float>::Compute
onnxruntime::contrib::QEmbedLayerNorm<float>::Compute
onnxruntime::contrib::QGemm::CheckInputs
onnxruntime::contrib::QGemm::Compute
onnxruntime::contrib::QLinearAveragePool::Compute
onnxruntime::contrib::QlinearBuildLookupTable
onnxruntime::contrib::QLinearConcat::Compute
onnxruntime::contrib::QLinearConcat::QLinearConcat
onnxruntime::contrib::QLinearGlobalAveragePool::Compute
onnxruntime::contrib::RegisterContribSchemas::<lambda_273270d63d2edeeb9cb50e9432a42c00>::operator ()
onnxruntime::contrib::RegisterCpuContribKernels
onnxruntime::contrib::RegisterQuantizationKernels
onnxruntime::contrib::Scale<float>::Scale
onnxruntime::contrib::SkipLayerNorm<double>::SkipLayerNorm
onnxruntime::contrib::SkipLayerNorm<float>::SkipLayerNorm
onnxruntime::contrib::SparseToDenseMatMul::Compute
onnxruntime::contrib::Tokenizer::Tokenizer
onnxruntime::contrib::WordConvEmbedding::Compute
onnxruntime::Conv<float>::Compute
onnxruntime::ConvActivationFusion::ApplyImpl
onnxruntime::ConvAddFusion::Apply
onnxruntime::ConvAttributes::ConvAttributes
onnxruntime::ConvAttributes::InferOutputShape
onnxruntime::ConvBNFusion::Apply
onnxruntime::ConvertMaskToInt32
onnxruntime::ConvInteger::Compute
onnxruntime::ConvMulFusion::Apply
onnxruntime::ConvTranspose<float>::DoConvTranspose
onnxruntime::ConvTransposeAttributes::ComputePadsAndOutputShape
onnxruntime::ConvTransposeAttributes::ComputeTransposePadAndOutputShape
onnxruntime::ConvTransposeAttributes::PrepareForCompute
onnxruntime::core_impl::<lambda_380759b69dafbe1155f8c031dc4b985c>::operator ()
onnxruntime::core_impl::<lambda_4038c6c41ccc2af39d2bcb85c83daf02>::operator ()
onnxruntime::core_impl::<lambda_68f04e9ae619f08036d765527117f897>::operator ()
onnxruntime::core_impl::<lambda_9931dee8adac3330994d4bf96a89cbe1>::operator ()
onnxruntime::CPUDataTransfer::CopyTensor
onnxruntime::CPUExecutionProvider::GetKernelRegistry
onnxruntime::CreateAllocator
onnxruntime::CreateCopyAndAppendCpuTensor
onnxruntime::CreateCustomRegistry
onnxruntime::CreateReplacementNode
onnxruntime::CreateSchema
onnxruntime::CumSum<__int64>::Compute
onnxruntime::CumSum<double>::Compute
onnxruntime::CumSum<float>::Compute
onnxruntime::CumSum<int>::Compute
onnxruntime::CustomOpKernel::CustomOpKernel
onnxruntime::data_types_internal::DataTypeRegistry::RegisterDataType
onnxruntime::data_types_internal::IsCompatible
onnxruntime::data_types_internal::MapTypeHelper::Set
onnxruntime::data_types_internal::OptionalTypeHelper::Set
onnxruntime::data_types_internal::SequenceTypeHelper::Set
onnxruntime::DataTransferManager::CopySparseTensors
onnxruntime::DataTransferManager::CopyTensors
onnxruntime::DataTypeImpl::GetType<T>() == type_
onnxruntime::DeepCpuGruOp::Compute
onnxruntime::DeepCpuGruOp::ComputeImpl
onnxruntime::DeepCpuGruOp::DeepCpuGruOp
onnxruntime::DeepCpuLstmOp::Compute
onnxruntime::DeepCpuLstmOp::PrePack
onnxruntime::DepthToSpace::Compute
onnxruntime::DepthToSpace::DepthToSpace
onnxruntime::DequantizeLinear<int>::Compute
onnxruntime::Det<float>::Compute
onnxruntime::DispatchStridedCopy
onnxruntime::DoTransposeEltWise
onnxruntime::DoTransposeImpl
onnxruntime::Dropout<double,double>::Compute
onnxruntime::Dropout<double,float>::Compute
onnxruntime::Dropout<float,double>::Compute
onnxruntime::Dropout<float,float>::Compute
onnxruntime::DynamicQuantizeLinear<unsigned char>::Compute
onnxruntime::DynamicQuantizeMatMulFusion::ApplyImpl
onnxruntime::Einsum::DeviceCompute
onnxruntime::Einsum::Einsum
onnxruntime::EinsumComputePreprocessor::PostProcessBroadcastedDims
onnxruntime::EinsumComputePreprocessor::Run
onnxruntime::EinsumOp::DeviceHelpers::CpuDeviceHelpers::DataCopy
onnxruntime::EinsumOp::DeviceHelpers::CpuDeviceHelpers::Diagonal
onnxruntime::EinsumOp::DeviceHelpers::CpuDeviceHelpers::DiagonalInnermostDims
onnxruntime::EinsumOp::IsTransposeRequired
onnxruntime::EinsumOp::MatMul
onnxruntime::EinsumOp::Transpose
onnxruntime::EinsumTypedComputeProcessor<__int64>::FinalizeOutput
onnxruntime::EinsumTypedComputeProcessor<__int64>::PairwiseOperandProcess
onnxruntime::EinsumTypedComputeProcessor<double>::FinalizeOutput
onnxruntime::EinsumTypedComputeProcessor<double>::PairwiseOperandProcess
onnxruntime::EinsumTypedComputeProcessor<float>::FinalizeOutput
onnxruntime::EinsumTypedComputeProcessor<float>::PairwiseOperandProcess
onnxruntime::EinsumTypedComputeProcessor<int>::FinalizeOutput
onnxruntime::EinsumTypedComputeProcessor<int>::PairwiseOperandProcess
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<__int64> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<__int64> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<int> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<int> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<short> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<short> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<signed char> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<signed char> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned __int64> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned __int64> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned char> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned char> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned int> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned int> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned short> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Abs<unsigned short> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Ceil<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Ceil<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Celu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Celu<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Elu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Elu<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Exp<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Exp<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Exp<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Exp<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Floor<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Floor<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::HardSigmoid<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::HardSigmoid<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::LeakyRelu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::LeakyRelu<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Log<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Log<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Log<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Log<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<__int64> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<__int64> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<int> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<int> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<signed char> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Neg<signed char> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ParametricSoftplus<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ParametricSoftplus<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Reciprocal<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Reciprocal<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Reciprocal<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Reciprocal<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Relu<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Relu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Relu<int> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Relu<signed char> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ScaledTanh<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ScaledTanh<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Selu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Selu<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sigmoid<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sigmoid<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Softplus<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Softsign<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sqrt<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sqrt<double> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sqrt<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Sqrt<float> >::ElementWiseKernel
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Tanh<double> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::Tanh<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ThresholdedRelu<float> >::Compute
onnxruntime::ElementWiseKernel<struct onnxruntime::functors::ThresholdedRelu<float> >::ElementWiseKernel
onnxruntime::EmbedLayerNormFusion::ApplyImpl
onnxruntime::ExecutionFrame::{ctor}::<lambda_9340f42f5f8bcfd7c2f8707644905958>::operator ()
onnxruntime::ExecutionFrame::AllocateAsPerAllocationPlan
onnxruntime::ExecutionFrame::AllocateMLValueTensorPreAllocateBuffer
onnxruntime::ExecutionFrame::AllocateMLValueTensorSelfOwnBufferHelper
onnxruntime::ExecutionFrame::AllocateReusedOrtValueIfNotAllocatedHelper
onnxruntime::ExecutionFrame::ExecutionFrame
onnxruntime::ExecutionFrame::GetAllocationPlan
onnxruntime::ExecutionFrame::ReleaseMLValueImpl
onnxruntime::ExecutionFrame::TraceAllocate
onnxruntime::ExecutionFrame::TraceFree
onnxruntime::ExecutionFrame::VerifyOutputSizes
onnxruntime::ExecutionProviders::Add
onnxruntime::ExLibLoader::{dtor}::<lambda_5c396c9fa2f7b5412218295bb7bb0fcf>::operator ()
onnxruntime::ExLibLoader::~ExLibLoader
onnxruntime::ExLibLoader::LoadExternalLib
onnxruntime::Expand_8<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::Compute::<lambda_f16ca9954ddc2a61d6a46f1a5f825891>::operator ()
onnxruntime::ExpandBroadcastLooper
onnxruntime::FastGeluFusion::ApplyImpl
onnxruntime::fbs::utils::LoadAttributeOrtFormat
onnxruntime::fbs::utils::LoadInitializerOrtFormat
onnxruntime::fbs::utils::LoadMapTypeOrtFormat
onnxruntime::fbs::utils::LoadOpsetImportOrtFormat
onnxruntime::fbs::utils::LoadSequenceTypeOrtFormat
onnxruntime::fbs::utils::LoadSparseInitializerOrtFormat
onnxruntime::fbs::utils::LoadTensorDimensionOrtFormat
onnxruntime::fbs::utils::LoadTensorShapeOrtFormat
onnxruntime::fbs::utils::LoadTensorTypeAndShapeOrtFormat
onnxruntime::fbs::utils::LoadTypeInfoOrtFormat
onnxruntime::fbs::utils::LoadValueInfoOrtFormat
onnxruntime::fbs::utils::SaveAttributeOrtFormat
onnxruntime::fbs::utils::SaveInitializerOrtFormat
onnxruntime::fbs::utils::SaveMapTypeOrtFormat
onnxruntime::fbs::utils::SaveSequenceTypeOrtFormat
onnxruntime::fbs::utils::SaveSparseInitializerOrtFormat
onnxruntime::fbs::utils::SaveTensorTypeAndShapeOrtFormat
onnxruntime::fbs::utils::SaveTypeInfoOrtFormat
onnxruntime::fbs::utils::SaveValueInfoOrtFormat
onnxruntime::FeedsFetchesInfo::FeedsFetchesInfo
onnxruntime::FeedsFetchesInfo::MapNamesToMLValueIdxs
onnxruntime::FeedsFetchesManager::SetDeviceCopyChecks
onnxruntime::FinalizeSessionOptions
onnxruntime::Flatten::Compute
onnxruntime::Flatten::Flatten
onnxruntime::FreeDimensionOverrideTransformer::ApplyImpl
onnxruntime::FreeDimensionOverrideTransformer::FreeDimensionOverrideTransformer
onnxruntime::FuncManager::GetFuncs
onnxruntime::FunctionImpl::FunctionImpl
onnxruntime::FunctionKernel::FunctionKernel
onnxruntime::functors::ElementWiseRangedTransform<float>::Create
onnxruntime::functors::HardSigmoid<float>::Init
onnxruntime::functors::ParametricSoftplus<float>::Init
onnxruntime::functors::ScaledTanh<float>::Init
onnxruntime::functors::Selu<float>::Init
onnxruntime::FuseReluClip::Apply
onnxruntime::FuseSubGraph
onnxruntime::FuseSubGraphDistilBert
onnxruntime::FuseSubGraphQK
onnxruntime::FuseSubGraphQKDistilBert
onnxruntime::FuseSubGraphQKImpl
onnxruntime::Gather::Compute
onnxruntime::GatherBase::GatherBase
onnxruntime::GatherElements::GatherElements
onnxruntime::GatherND::Compute
onnxruntime::GeluApproximation::ApplyImpl
onnxruntime::GeluFusion::ApplyImpl
onnxruntime::GemmActivationFusion::ApplyImpl
onnxruntime::GemmBase::GemmBase
onnxruntime::GemmBroadcastBias
onnxruntime::GemmHelper::GemmHelper
onnxruntime::GemmSumFusion::Apply
onnxruntime::GemmSumFusion::SatisfyCondition
onnxruntime::GetCpuPreferredNodes
onnxruntime::GetCpuPreferredNodes::<lambda_f72360673da28b091dd40b649eb46e9c>::operator ()
onnxruntime::GetKernelCreateInfo
onnxruntime::GetScalarSplitInput
onnxruntime::GetSeqIdx
onnxruntime::GetSplitSizesInput
onnxruntime::GetSubGraphSessionStatesOrtFormat
onnxruntime::GetTransposePerms
onnxruntime::Graph::AddEdge
onnxruntime::Graph::AddInitializedTensor
onnxruntime::Graph::AllocateNode
onnxruntime::Graph::BuildConnections
onnxruntime::Graph::CleanUnusedInitializersAndNodeArgs
onnxruntime::Graph::CreateFusedSubGraphNode
onnxruntime::Graph::FinalizeFuseSubGraph
onnxruntime::Graph::ForThisAndAllSubgraphs
onnxruntime::Graph::Graph
onnxruntime::Graph::InferAndVerifySubgraphTypes
onnxruntime::Graph::InferAndVerifyTypeMatch
onnxruntime::Graph::InitFunctionBodyForNode
onnxruntime::Graph::InitializeStateFromModelFileGraphProto
onnxruntime::Graph::InitInputsInitializersOutputs
onnxruntime::Graph::InlineFunction
onnxruntime::Graph::KahnsTopologicalSort
onnxruntime::Graph::LoadFromOrtFormat
onnxruntime::Graph::LoadFromOrtFormat::<lambda_002dfff45d93392e8dba7424e351b660>::operator ()
onnxruntime::Graph::NodeAtIndexImpl
onnxruntime::Graph::PerformTypeAndShapeInferencing
onnxruntime::Graph::RemoveEdge
onnxruntime::Graph::RemoveInitializedTensor
onnxruntime::Graph::RemoveNode
onnxruntime::Graph::Resolve
onnxruntime::Graph::SaveToOrtFormat
onnxruntime::Graph::SetInputs
onnxruntime::Graph::SetOuterScopeNodeArgs
onnxruntime::Graph::ToGraphProto
onnxruntime::Graph::ToGraphProtoInternal
onnxruntime::Graph::UpdateShapeInference
onnxruntime::Graph::VerifyNodeAndOpMatch
onnxruntime::graph_utils::AddInitializer
onnxruntime::graph_utils::AddNodeInput
onnxruntime::graph_utils::CanUpdateImplicitInputNameInSubgraphs
onnxruntime::graph_utils::FindPath
onnxruntime::graph_utils::GetIndexFromName
onnxruntime::graph_utils::GetNodeInputName
onnxruntime::graph_utils::GetNodeOutputName
onnxruntime::graph_utils::RemoveNode
onnxruntime::graph_utils::RemoveNodeWithSingleNodeInSingleUsedOutput
onnxruntime::graph_utils::ReplaceNodeInput
onnxruntime::graph_utils::UpdateImplicitInputNameInSubgraph
onnxruntime::GraphPartitioner::Partition
onnxruntime::GraphPartitioner::PartitionOnnxFormatModel
onnxruntime::GraphPartitioner::PartitionOrtFormatModel
onnxruntime::GraphTransformer::Apply
onnxruntime::GraphTransformer::Recurse
onnxruntime::GraphTransformerManager::ApplyTransformers
onnxruntime::GraphViewer::GetNodesInTopologicalOrder
onnxruntime::GraphViewer::GetRootNodes
onnxruntime::GraphViewer::GraphViewer
onnxruntime::HandleNegativeAxis
onnxruntime::Hardmax<float>::Compute
onnxruntime::IAllocator::CalcMemSizeForArrayWithAlignment::<lambda_6c2fbce111bd53fabf7be0ec544a2aae>::operator ()
onnxruntime::IDataTransfer::CopySparseTensors
onnxruntime::IDataTransfer::CopyTensors
onnxruntime::IdentityOp<0>::Compute
onnxruntime::IdentityOp<1>::Compute
onnxruntime::IExecutionFrame::GetMLValue
onnxruntime::IExecutionFrame::GetOrCreateNodeOutputMLValue
onnxruntime::IExecutionFrame::IExecutionFrame
onnxruntime::IExecutionFrame::Init
onnxruntime::IExecutionProvider::GenerateMetaDefId
onnxruntime::IExecutionProvider::InsertAllocator
onnxruntime::IExecutionProvider::TryInsertAllocator
onnxruntime::If::Compute
onnxruntime::If::Info::Info
onnxruntime::If::Init
onnxruntime::If::SetupSubgraphExecutionInfo
onnxruntime::IfImpl::Execute
onnxruntime::IfImpl::Initialize
onnxruntime::IncrementIndexAndComputeOffsetSetup
onnxruntime::inference_session_utils::JsonConfigParser::ParseOrtConfigJsonInModelProto
onnxruntime::inference_session_utils::JsonConfigParser::ParseOrtConfigJsonInModelProto::<lambda_8bac32b7d0f8f8e4174abd4e312abcfd>::operator ()
onnxruntime::inference_session_utils::JsonConfigParser::ParseSessionOptionsFromModelProto
onnxruntime::InferenceSession::{dtor}::<lambda_80a20a44de674d401d97ead9ef557a73>::operator ()
onnxruntime::InferenceSession::~InferenceSession
onnxruntime::InferenceSession::AddCustomOpDomains
onnxruntime::InferenceSession::AddPredefinedTransformers
onnxruntime::InferenceSession::ConstructorCommon
onnxruntime::InferenceSession::ConstructorCommon::<lambda_3216ea50ba086ad9b600d540bb145c88>::operator ()
onnxruntime::InferenceSession::CreateLoggerForRun
onnxruntime::InferenceSession::EndProfiling
onnxruntime::InferenceSession::GetModelInputs
onnxruntime::InferenceSession::GetModelMetadata
onnxruntime::InferenceSession::GetModelOutputs
onnxruntime::InferenceSession::GetOverridableInitializers
onnxruntime::InferenceSession::GetSessionState
onnxruntime::InferenceSession::InferenceSession
onnxruntime::InferenceSession::Initialize
onnxruntime::InferenceSession::Initialize::<lambda_4bbd332ce2e58a1b6a340868690db847>::operator ()
onnxruntime::InferenceSession::Initialize::<lambda_ab5819bf676de610a6a0f676072ed7ea>::operator ()
onnxruntime::InferenceSession::InitLogger
onnxruntime::InferenceSession::Load
onnxruntime::InferenceSession::LoadOrtModel
onnxruntime::InferenceSession::LoadOrtModel::<lambda_31da21da588bb3698a015fc212bb17cf>::operator ()
onnxruntime::InferenceSession::NewIOBinding
onnxruntime::InferenceSession::RegisterExecutionProvider
onnxruntime::InferenceSession::RegisterGraphTransformer
onnxruntime::InferenceSession::Run
onnxruntime::InferenceSession::SaveToOrtFormat
onnxruntime::InferenceSession::ShrinkMemoryArenas
onnxruntime::InferenceSession::TransformGraph
onnxruntime::InferenceSession::ValidateInputs
onnxruntime::Initializer::Initializer
onnxruntime::Initializer::ReadExternalRawData
onnxruntime::Initializer::ToProto
onnxruntime::InitNestedModelLocalFunction
onnxruntime::InlineNodes
onnxruntime::InputBroadcaster::AdvanceBy
onnxruntime::InsertCastTransformer::ApplyImpl
onnxruntime::InstanceNorm<float>::Compute
onnxruntime::InstanceNorm<float>::InstanceNorm
onnxruntime::IOBinding::BindInput
onnxruntime::IOBinding::SynchronizeInputs
onnxruntime::IOBinding::SynchronizeOutputs
onnxruntime::IOTypeConstraintHelper
onnxruntime::IsInf::IsInf
onnxruntime::KernelDefBuilder::VariadicAlias
onnxruntime::KernelRegistry::Register
onnxruntime::KernelRegistry::TryCreateKernel
onnxruntime::KernelUseSharedPrePackedBuffers
onnxruntime::LayerNormFusion::ApplyImpl
onnxruntime::LoadOrtModelBytes
onnxruntime::logging::LoggingManager::CreateDefaultLogger
onnxruntime::logging::LoggingManager::DefaultLogger
onnxruntime::logging::LoggingManager::LoggingManager
onnxruntime::Loop::Compute
onnxruntime::Loop::Info::Info
onnxruntime::Loop::Init
onnxruntime::Loop::SetupSubgraphExecutionInfo
onnxruntime::LoopDir
onnxruntime::LoopImpl::ConcatenateLoopOutput
onnxruntime::LoopImpl::Execute
onnxruntime::LoopImpl::Execute::<lambda_db6f760d5bdf70bdc754d8ffbb13a528>::operator ()
onnxruntime::LoopImpl::Initialize
onnxruntime::LoopImpl::SaveOutputsAndUpdateFeeds
onnxruntime::LpNorm<double>::LpNorm
onnxruntime::LpNorm<float>::LpNorm
onnxruntime::LRN<float>::Compute
onnxruntime::LRN<float>::LRN
onnxruntime::LSTMBase::ComputeImpl
onnxruntime::LSTMBase::LSTMBase
onnxruntime::MatchInputToConcatSubgraph
onnxruntime::MatchPositionEmbeddingSubgraphsFromGather
onnxruntime::math::Gemm
onnxruntime::math::NextPosition
onnxruntime::MatMul<__int64>::Compute
onnxruntime::MatMul<double>::Compute
onnxruntime::MatMul<float>::Compute
onnxruntime::MatMul<int>::Compute
onnxruntime::MatMulAddFusion::ApplyImpl
onnxruntime::MatMulComputeHelper::Compute
onnxruntime::MatMulComputeHelper::Compute::<lambda_74826040142e475fff8a25f7fa284d9f>::operator ()
onnxruntime::MatMulInteger::Compute
onnxruntime::MatMulIntegerToFloatFusion::ApplyImpl
onnxruntime::MatMulScaleFusion::ApplyImpl
onnxruntime::MatmulTransposeFusion::ApplyImpl
onnxruntime::Max_6<float>::Compute
onnxruntime::MaxPoolV8::ComputeImpl
onnxruntime::MaxUnpool::Compute
onnxruntime::MaxUnpool::MaxUnpool
onnxruntime::Mean_6<float>::Compute
onnxruntime::MeanVarianceNormalization_0<float>::MeanVarianceNormalization_0
onnxruntime::MemcpyTransformer::ApplyImpl
onnxruntime::MemPatternPlanner::TraceAllocation
onnxruntime::MergeIntoTarget::Run
onnxruntime::MergeShapeInfo
onnxruntime::Min_6<float>::Compute
onnxruntime::ml::batched_update_scores_inplace
onnxruntime::ml::CastInputToFloat
onnxruntime::ml::CastMap::CastMap
onnxruntime::ml::CastMap::ComputeImpl
onnxruntime::ml::CategoryMapper::CategoryMapper
onnxruntime::ml::detail::TreeAggregator<double,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregator<float,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorAverage<double,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorAverage<float,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorClassifier<__int64,float>::_set_score_binary
onnxruntime::ml::detail::TreeAggregatorClassifier<__int64,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorClassifier<double,float>::_set_score_binary
onnxruntime::ml::detail::TreeAggregatorClassifier<double,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorClassifier<float,float>::_set_score_binary
onnxruntime::ml::detail::TreeAggregatorClassifier<float,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorClassifier<int,float>::_set_score_binary
onnxruntime::ml::detail::TreeAggregatorClassifier<int,float>::FinalizeScores
onnxruntime::ml::detail::TreeAggregatorMax<double,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorMax<float,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorMin<double,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorMin<float,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorSum<__int64,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorSum<__int64,float>::ProcessTreeNodePrediction
onnxruntime::ml::detail::TreeAggregatorSum<double,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorSum<double,float>::ProcessTreeNodePrediction
onnxruntime::ml::detail::TreeAggregatorSum<float,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorSum<float,float>::ProcessTreeNodePrediction
onnxruntime::ml::detail::TreeAggregatorSum<int,float>::MergePrediction
onnxruntime::ml::detail::TreeAggregatorSum<int,float>::ProcessTreeNodePrediction
onnxruntime::ml::detail::TreeEnsembleCommon<__int64,float>::TreeEnsembleCommon
onnxruntime::ml::detail::TreeEnsembleCommon<double,float>::compute
onnxruntime::ml::detail::TreeEnsembleCommon<double,float>::TreeEnsembleCommon
onnxruntime::ml::detail::TreeEnsembleCommon<float,float>::compute
onnxruntime::ml::detail::TreeEnsembleCommon<float,float>::TreeEnsembleCommon
onnxruntime::ml::detail::TreeEnsembleCommon<int,float>::TreeEnsembleCommon
onnxruntime::ml::detail::TreeEnsembleCommonClassifier<__int64,float>::compute
onnxruntime::ml::detail::TreeEnsembleCommonClassifier<double,float>::compute
onnxruntime::ml::detail::TreeEnsembleCommonClassifier<float,float>::compute
onnxruntime::ml::detail::TreeEnsembleCommonClassifier<int,float>::compute
onnxruntime::ml::DictVectorizerOp<__int64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::DictVectorizerOp
onnxruntime::ml::DictVectorizerOp<__int64,double>::DictVectorizerOp
onnxruntime::ml::DictVectorizerOp<__int64,float>::DictVectorizerOp
onnxruntime::ml::DictVectorizerOp<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64>::DictVectorizerOp
onnxruntime::ml::DictVectorizerOp<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,double>::DictVectorizerOp
onnxruntime::ml::DictVectorizerOp<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float>::DictVectorizerOp
onnxruntime::ml::FeatureVectorizer::Compute
onnxruntime::ml::FeatureVectorizer::FeatureVectorizer
onnxruntime::ml::ImputerOp::Compute
onnxruntime::ml::ImputerOp::ImputerOp
onnxruntime::ml::LabelEncoder::LabelEncoder
onnxruntime::ml::LabelEncoder_2<__int64,__int64>::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<__int64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<__int64,float>::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64>::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float>::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<float,__int64>::LabelEncoder_2
onnxruntime::ml::LabelEncoder_2<float,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::LabelEncoder_2
onnxruntime::ml::LinearClassifier::ComputeImpl
onnxruntime::ml::LinearClassifier::LinearClassifier
onnxruntime::ml::LinearRegressor::LinearRegressor
onnxruntime::ml::MakeCast
onnxruntime::ml::MakeNormalize
onnxruntime::ml::MakePack
onnxruntime::ml::Normalizer::Normalizer
onnxruntime::ml::OneHotEncoderOp<__int64>::OneHotEncoderOp
onnxruntime::ml::OneHotEncoderOp<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::OneHotEncoderOp
onnxruntime::ml::OneHotEncoderOp<double>::OneHotEncoderOp
onnxruntime::ml::OneHotEncoderOp<float>::OneHotEncoderOp
onnxruntime::ml::RegisterOnnxMLOperatorKernels
onnxruntime::ml::ScalerOp<__int64>::ScalerOp
onnxruntime::ml::ScalerOp<double>::ScalerOp
onnxruntime::ml::ScalerOp<float>::ScalerOp
onnxruntime::ml::ScalerOp<int>::ScalerOp
onnxruntime::ml::SVMClassifier::Compute
onnxruntime::ml::SVMClassifier::SVMClassifier
onnxruntime::ml::SVMCommon::SVMCommon
onnxruntime::ml::SVMRegressor<float>::Compute
onnxruntime::ml::SVMRegressor<float>::SVMRegressor
onnxruntime::ml::ZipMapOp::ZipMapOp
onnxruntime::Mod::Mod
onnxruntime::mod_internal::CallModImpl<double,void>::operator ()
onnxruntime::mod_internal::CallModImpl<float,void>::operator ()
onnxruntime::mod_internal::CallModImpl<struct onnxruntime::MLFloat16,void>::operator ()
onnxruntime::Model::Load
onnxruntime::Model::LoadFromOrtFormat
onnxruntime::Model::Model
onnxruntime::Model::Save
onnxruntime::Model::SaveToOrtFormat
onnxruntime::model_load_utils::IsAllowReleasedONNXOpsetsOnlySet
onnxruntime::model_load_utils::ValidateOpsetForDomain
onnxruntime::MoveInputOutput
onnxruntime::Multinomial::Multinomial
onnxruntime::MultinomialCompute
onnxruntime::ngram_details::PopulateGrams
onnxruntime::NhwcTransformer::ApplyImpl
onnxruntime::Node::ForEachWithIndex
onnxruntime::Node::LoadEdgesFromOrtFormat
onnxruntime::Node::LoadEdgesFromOrtFormat::<lambda_8629f0a4d7cae9a2f0cd36f438d51fee>::operator ()
onnxruntime::Node::LoadFromOrtFormat
onnxruntime::Node::LoadFromOrtFormat::<lambda_9d9512158c4d36214ecdd758e8025182>::operator ()
onnxruntime::Node::SaveToOrtFormat
onnxruntime::NodeArg::UpdateTypeAndShape
onnxruntime::NodeIndexInfo::GetMLValueIndex
onnxruntime::NodeIndexInfo::GetNodeOffset
onnxruntime::NodeIndexInfo::Init::<lambda_68f4cdf20c40d247dba9fe0156c932f5>::operator ()
onnxruntime::NodeIndexInfo::Init::<lambda_dc786c9bfe42f2772e18a10475fce9cf>::operator ()
onnxruntime::NodesToOptimize::GetNode
onnxruntime::NodesToOptimizeIndicesBuilder::Build
onnxruntime::NonMaxSuppression::Compute
onnxruntime::NonMaxSuppressionBase::GetThresholdsFromInputs
onnxruntime::NonMaxSuppressionBase::NonMaxSuppressionBase
onnxruntime::NonMaxSuppressionBase::PrepareCompute
onnxruntime::NonTensorTypeBase::FromDataContainer
onnxruntime::NonTensorTypeBase::IsMapCompatible
onnxruntime::NonTensorTypeBase::IsSequenceCompatible
onnxruntime::NonTensorTypeBase::ToDataContainer
onnxruntime::NonZero<__int64>::Compute
onnxruntime::NonZero<bool>::Compute
onnxruntime::NonZero<float>::Compute
onnxruntime::NonZero<int>::Compute
onnxruntime::NonZero<unsigned char>::Compute
onnxruntime::OneHotOp<__int64,__int64,__int64>::Compute
onnxruntime::OneHotOp<__int64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64>::Compute
onnxruntime::OneHotOp<__int64,float,__int64>::Compute
onnxruntime::OneHotOp<__int64,float,float>::Compute
onnxruntime::OneHotOp<__int64,float,int>::Compute
onnxruntime::OneHotOp<__int64,int,float>::Compute
onnxruntime::OneHotOp<float,__int64,__int64>::Compute
onnxruntime::OneHotOp<float,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64>::Compute
onnxruntime::OneHotOp<float,float,float>::Compute
onnxruntime::OneHotOp<int,float,float>::Compute
onnxruntime::OneHotOp<int,float,int>::Compute
onnxruntime::OnnxRuntimeOpSchemaRegistry::RegisterOpSchemaInternal
onnxruntime::OnnxRuntimeOpSchemaRegistry::RegisterOpSet
onnxruntime::OpKernel::ComputeAsync
onnxruntime::OpKernelContext::GetOrCreateOutputMLValue
onnxruntime::OpKernelContext::Input
onnxruntime::OpKernelContext::NumVariadicInputs
onnxruntime::OpKernelContext::OpKernelContext
onnxruntime::OpKernelContext::Output
onnxruntime::OpKernelContext::OutputMLValue
onnxruntime::OpKernelContext::RequiredInput
onnxruntime::OpKernelContext::RequiredOutput
onnxruntime::OpKernelContextInternal::OpKernelContextInternal
onnxruntime::OpKernelInfo::GetMemoryInfo
onnxruntime::OpNodeProtoHelper<class onnxruntime::ProtoHelperNodeContext>::GetAttrs
onnxruntime::OpNodeProtoHelper<struct onnx::InferenceContext>::GetAttrs
onnxruntime::optimizer_utils::GenerateRewriteRules
onnxruntime::optimizer_utils::GenerateRuleBasedGraphTransformer
onnxruntime::optimizer_utils::GenerateTransformers
onnxruntime::optimizer_utils::GenerateTransformersForRuntimeOptimizations
onnxruntime::OptimizerExecutionFrame::Info::{ctor}::<lambda_35452b6fa29b4a84e3a3d83c879b06cd>::operator ()
onnxruntime::OptimizerExecutionFrame::Info::Info
onnxruntime::Optional::Compute
onnxruntime::Optional::Optional
onnxruntime::OptionalGetElement::Compute
onnxruntime::OptionalTypeBase::GetDeleteFunc
onnxruntime::OptionalTypeBase::GetElementType
onnxruntime::OptionalTypeBase::IsCompatible
onnxruntime::OrtValueTensorSlicer<struct OrtValue const >::Create
onnxruntime::OrtValueTensorSlicer<struct OrtValue const >::Iterator::Iterator
onnxruntime::OrtValueTensorSlicer<struct OrtValue const >::Iterator::operator *
onnxruntime::OrtValueTensorSlicer<struct OrtValue>::Create
onnxruntime::OrtValueTensorSlicer<struct OrtValue>::Iterator::Iterator
onnxruntime::OrtValueTensorSlicer<struct OrtValue>::Iterator::operator *
onnxruntime::OuterScopeNodeArgLocationAccumulator
onnxruntime::OuterScopeNodeArgLocationAccumulator::<lambda_3e31c198cbd0b452ff5ea25fd603eaa6>::operator ()
onnxruntime::OuterScopeNodeArgLocationAccumulator::<lambda_ee881615ba5030f55f651fb9e91a2637>::operator ()
onnxruntime::OutputBroadcaster::OutputBroadcaster
onnxruntime::Pad::Compute
onnxruntime::PadBase::PadBase
onnxruntime::PadImpl
onnxruntime::PadInputWithDimValueOfZero
onnxruntime::PadValueFromFloat
onnxruntime::ParallelExecutor::Execute
onnxruntime::ParallelExecutor::RunNodeAsync
onnxruntime::PartitionOnnxFormatModelImpl
onnxruntime::PartitionOrtFormatModelImpl
onnxruntime::Path::Parse
onnxruntime::PlaceNode
onnxruntime::PlannerImpl::AllocPlan
onnxruntime::PlannerImpl::Buffer
onnxruntime::PlannerImpl::ComputeReusePlan
onnxruntime::PlannerImpl::ComputeUseCounts
onnxruntime::PlannerImpl::ComputeUseCounts::<lambda_4a61ff195999a8a104bf36ba131bf403>::operator ()
onnxruntime::PlannerImpl::CreatePlan
onnxruntime::PlannerImpl::GenerateDeallocationPlan
onnxruntime::PlannerImpl::GeneratePlanForWeightsHelper
onnxruntime::PlannerImpl::GetElementSize
onnxruntime::PlannerImpl::GetLocationForNodeInput
onnxruntime::PlannerImpl::Index
onnxruntime::PlannerImpl::ProcessDef
onnxruntime::PlannerImpl::Reuse
onnxruntime::PlannerImpl::UseCount
onnxruntime::PlannerImpl::VerifyMemoryTimeSchedule
onnxruntime::Pool<float,class onnxruntime::LpPool>::Compute
onnxruntime::PoolAttributes::ComputeSizePadDilations
onnxruntime::PoolAttributes::InferOutputSize
onnxruntime::PoolAttributes::PoolAttributes
onnxruntime::PoolAttributes::SetOutputSize
onnxruntime::PoolBase::Compute
onnxruntime::PoolProcessContext::init
onnxruntime::PrePackedWeights::GetHash
onnxruntime::PrepackedWeightsContainer::GetOrCreateAllocator
onnxruntime::PrepareForQDQ
onnxruntime::profiling::Profiler::EndProfiling
onnxruntime::profiling::Profiler::EndTimeAndRecordEvent
onnxruntime::profiling::Profiler::Initialize
onnxruntime::profiling::Profiler::Start
onnxruntime::profiling::Profiler::StartProfiling
onnxruntime::PropagateInputOrtValueToFirstOutput
onnxruntime::ProviderLibrary::Get
onnxruntime::ProviderLibrary::Unload
onnxruntime::ProviderSharedLibrary::Ensure
onnxruntime::ProviderSharedLibrary::Unload
onnxruntime::QDQPropagationTransformer::ApplyImpl
onnxruntime::QDQS8ToU8Transformer::ApplyImpl
onnxruntime::QLinearConv::Compute
onnxruntime::QLinearConv::ComputeOffset
onnxruntime::QLinearConv::ComputeOutputScale
onnxruntime::QLinearConv::UseSharedPrePackedBuffers
onnxruntime::QLinearMatMul::Compute
onnxruntime::RandomNormal::RandomNormal
onnxruntime::RandomNormalLike::Compute
onnxruntime::RandomNormalLike::RandomNormalLike
onnxruntime::RandomUniform::RandomUniform
onnxruntime::RandomUniformLike::Compute
onnxruntime::RandomUniformLike::RandomUniformLike
onnxruntime::ReduceKernelBase<0>::ReduceKernelBase
onnxruntime::ReduceKernelBase<1>::ReduceKernelBase
onnxruntime::RegisterCPUKernels
onnxruntime::RegisterOnnxOperatorKernels
onnxruntime::ReleaseNodeMLValues
onnxruntime::RemoveDuplicateCastTransformer::ApplyImpl
onnxruntime::ReorderCastAndTranspose
onnxruntime::ReplaceWithNew::Run
onnxruntime::ReplaceWithNew::RunForSave
onnxruntime::Reshape::Compute
onnxruntime::Reshape_1::Reshape_1
onnxruntime::ReshapeFusion::ApplyImpl
onnxruntime::ReshapeFusion::Fuse_Subgraph
onnxruntime::ReshapeHelper::ReshapeHelper
onnxruntime::ResultsNoTransposePrepareForReduce::ValidateNotEmpty
onnxruntime::ReverseSequenceOp::Compute
onnxruntime::ReverseSequenceOp::ReverseSequenceOp
onnxruntime::rnn::detail::ComputeGemm
onnxruntime::rnn::detail::deepcpu::ActivationFuncByName
onnxruntime::rnn::detail::deepcpu::GruOutputGateFuncByName
onnxruntime::rnn::detail::deepcpu::GruResetGateFuncByName
onnxruntime::rnn::detail::deepcpu::LstmMergeGatesFuncByName
onnxruntime::rnn::detail::MakeDirection
onnxruntime::rnn::detail::NormalizeActivationArgumentAndGetAlphaBetaCount
onnxruntime::rnn::detail::SafeRawConstPointer
onnxruntime::rnn::detail::SafeRawPointer
onnxruntime::RNN<float>::Compute
onnxruntime::RNN<float>::RNN
onnxruntime::RoiAlignBase::RoiAlignBase
onnxruntime::RoiPool<float>::Compute
onnxruntime::RoiPool<float>::RoiPool
onnxruntime::RuleBasedGraphTransformer::ApplyImpl
onnxruntime::RuleBasedGraphTransformer::ApplyRulesOnNode
onnxruntime::SaveModel
onnxruntime::scan::detail::AllocateOutput
onnxruntime::scan::detail::CreateFeedsFetchesManager
onnxruntime::scan::detail::Info::Info
onnxruntime::scan::detail::IterateSequence
onnxruntime::scan::detail::IterateSequence::<lambda_59d9acea5faf21b910fd8b35ea3c81b3>::operator ()
onnxruntime::scan::detail::LoopStateVariable::Next
onnxruntime::scan::detail::OutputIterator::AllocateFinalBuffer
onnxruntime::scan::detail::OutputIterator::AllocateFinalOutput
onnxruntime::scan::detail::OutputIterator::GetOutput
onnxruntime::scan::detail::OutputIterator::Initialize
onnxruntime::scan::detail::OutputIterator::operator *
onnxruntime::scan::detail::OutputIterator::operator ++
onnxruntime::scan::detail::ReadDirections
onnxruntime::Scan<8>::Compute
onnxruntime::Scan<8>::Init
onnxruntime::Scan<8>::SetupSubgraphExecutionInfo
onnxruntime::Scan<9>::Compute
onnxruntime::Scan<9>::Init
onnxruntime::Scan<9>::SetupSubgraphExecutionInfo
onnxruntime::Scan8Impl::AllocateOutputTensors
onnxruntime::Scan8Impl::CreateLoopStateVariables
onnxruntime::Scan8Impl::Execute
onnxruntime::Scan8Impl::Initialize
onnxruntime::Scan8Impl::ValidateInput
onnxruntime::ScanImpl::AllocateOutputTensors
onnxruntime::ScanImpl::CreateLoopStateVariables
onnxruntime::ScanImpl::Execute
onnxruntime::ScanImpl::Initialize
onnxruntime::ScanImpl::SetupInputs
onnxruntime::ScanImpl::TransposeOutput
onnxruntime::ScanImpl::ValidateInput
onnxruntime::Scatter<struct onnxruntime::TypeList<float,double,__int64,unsigned __int64,int,unsigned int,short,unsigned short,signed char,unsigned char,struct onnxruntime::MLFloat16,struct onnxruntime::BFloat16,bool,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > >::Scatter
onnxruntime::ScatterND::Compute
onnxruntime::ScatterNDBase::PrepareForCompute
onnxruntime::SelectorActionTransformer::ApplyImpl
onnxruntime::SelectorActionTransformer::MatchAndProcess
onnxruntime::SelectorActionTransformer::SelectorActionTransformer
onnxruntime::SelectorsAndActions::RegisterSelectorAndAction
onnxruntime::SequenceConstruct::Compute
onnxruntime::SequenceEmpty::Compute
onnxruntime::SequenceErase::Compute
onnxruntime::SequenceInsert::Compute
onnxruntime::SequenceTensorTypeBase::GetElementType
onnxruntime::SequenceTensorTypeBase::IsCompatible
onnxruntime::SequentialExecutor::Execute
onnxruntime::session_state_utils::DeserializeTensorProto
onnxruntime::session_state_utils::SaveInitializedTensors
onnxruntime::session_state_utils::SaveInitializedTensors::<lambda_3778dc86d07cb69d69c8323ef7fb1134>::operator ()
onnxruntime::session_state_utils::SaveInputOutputNamesToNodeMapping
onnxruntime::session_state_utils::SaveInputOutputNamesToNodeMapping::<lambda_217727e3bc662f496961b1bb8f9e821c>::operator ()
onnxruntime::session_state_utils::SaveInputOutputNamesToNodeMapping::<lambda_27534cc625d5ab04dc11bfb2c47d616f>::operator ()
onnxruntime::SessionState::AddOutputNameToNodeInfoMapping
onnxruntime::SessionState::AddSubgraphSessionState
onnxruntime::SessionState::CreateGraphInfo
onnxruntime::SessionState::CreateSubgraphSessionState
onnxruntime::SessionState::FinalizeSessionState
onnxruntime::SessionState::FinalizeSessionStateImpl
onnxruntime::SessionState::GetNodeIndexInfo
onnxruntime::SessionState::GetNodeKernelCreateInfo
onnxruntime::SessionState::LoadFromOrtFormat
onnxruntime::SessionState::LoadFromOrtFormat::<lambda_b0c552e77b6d02e6c53c6386edafcda3>::operator ()
onnxruntime::SessionState::PopulateKernelCreateInfo
onnxruntime::SessionState::PrepackConstantInitializedTensors::<lambda_8969ecba8249304e2d1178678ef22189>::operator ()
onnxruntime::SessionState::SaveToOrtFormat
onnxruntime::SessionState::SetupAllocators
onnxruntime::SessionState::UpdateToBeExecutedNodes
onnxruntime::SetEnableProfiling
onnxruntime::SetExecutionMode
onnxruntime::SetGraphOptimizationLevel
onnxruntime::SetInterOpNumThreads
onnxruntime::SetIntraOpNumThreads
onnxruntime::Shrink::Shrink
onnxruntime::SimplifiedLayerNormFusion::ApplyImpl
onnxruntime::SkipLayerNormFusion::ApplyImpl
onnxruntime::SliceBase::Compute
onnxruntime::SliceBase::FillVectorsFromInput
onnxruntime::SliceBase::PrepareForCompute
onnxruntime::SliceBase::SliceBase
onnxruntime::SliceImpl::<lambda_1ca539266a3f7de690711da52dd46463>::operator ()
onnxruntime::SliceImpl::<lambda_3062cbfc3de47173984232698e8a1305>::operator ()
onnxruntime::SliceImpl::<lambda_63257b9ef94d1e010b58918661d013c1>::operator ()
onnxruntime::SliceImpl::<lambda_d22eb87071c64b2326104c92538d0533>::operator ()
onnxruntime::SliceImpl::<lambda_e09d0c7995e76488be3df01db27d6997>::operator ()
onnxruntime::SliceIteratorBase::CopyInnermostAxisNonSolitaryInnerStep
onnxruntime::SliceIteratorBase::Init
onnxruntime::SliceSkips::SliceSkips
onnxruntime::Softmax<double>::ComputeImplOpset13
onnxruntime::Softmax<float>::ComputeImplOpset13
onnxruntime::SpaceDepthBase::SpaceDepthBase
onnxruntime::SpaceToDepth::Compute
onnxruntime::sparse_utils::DenseTensorToSparseCoo
onnxruntime::sparse_utils::DenseTensorToSparseCsr
onnxruntime::sparse_utils::SparseCooToDenseTensor
onnxruntime::sparse_utils::SparseCsrToDenseTensor
onnxruntime::SparseTensor::AllocateBuffer
onnxruntime::SparseTensor::AsBlockSparse
onnxruntime::SparseTensor::AsCoo
onnxruntime::SparseTensor::AsCsr
onnxruntime::SparseTensor::Copy
onnxruntime::SparseTensor::GetCooIndexDims
onnxruntime::SparseTensor::GetSparseTensorFromOrtValue
onnxruntime::SparseTensor::MakeBlockSparseData
onnxruntime::SparseTensor::MakeBlockSparseStrings
onnxruntime::SparseTensor::MakeCooData
onnxruntime::SparseTensor::MakeCooStrings
onnxruntime::SparseTensor::MakeCsrData
onnxruntime::SparseTensor::MakeCsrStrings
onnxruntime::SparseTensor::UseBlockSparseIndices
onnxruntime::SparseTensor::UseCooIndices
onnxruntime::SparseTensor::UseCsrIndices
onnxruntime::SparseTensor::ValidateBlockSparseShapes
onnxruntime::SparseTensor::ValidateCsrIndices
onnxruntime::SparseTensorTypeBase::GetElementType
onnxruntime::SparseTensorTypeBase::IsCompatible
onnxruntime::Split::Compute
onnxruntime::Split::ComputeImpl
onnxruntime::SplitBase::SplitBase
onnxruntime::SplitToSequence::ComputeImpl
onnxruntime::Squeeze::Compute
onnxruntime::SqueezeBase::ComputeOutputShape
onnxruntime::StridedCopy
onnxruntime::StridedCopy::<lambda_22dae262e3744994c648a6a28f79603c>::operator ()
onnxruntime::StridedCopy::<lambda_31433bd7aba335c4dcfc2d21c7b0556b>::operator ()
onnxruntime::StridedCopy::<lambda_34078c7540b6602389f40bae3a86d150>::operator ()
onnxruntime::StridedCopy::<lambda_3ec6b12ad4cd55e04d715e409cf94caa>::operator ()
onnxruntime::StridedCopy::<lambda_6b8e74af1b2b937274f6d872f7931a4b>::operator ()
onnxruntime::StridedCopy::<lambda_8687679c83f6820abda420b1312e974a>::operator ()
onnxruntime::StridedCopy::<lambda_a99d8d5e4f7ccd722937dd9eb05ffd73>::operator ()
onnxruntime::StridedCopy::<lambda_dada8d57643005b7e2297cabdd6baa97>::operator ()
onnxruntime::StridedCopy::<lambda_e8aeb42bf0aa89a79a408edee79cf184>::operator ()
onnxruntime::StridedCopy::<lambda_f43464c7a9a747b50c25ae66c092bf51>::operator ()
onnxruntime::string_normalizer::Locale::Locale
onnxruntime::StringNormalizer::StringNormalizer
onnxruntime::StringToAutoPadType
onnxruntime::Sum_6<double>::Compute
onnxruntime::Sum_6<float>::Compute
onnxruntime::SwapAdjacentNodes
onnxruntime::SyncProviders
onnxruntime::Tensor::Data
onnxruntime::Tensor::DataAsSpan
onnxruntime::Tensor::DataRaw
onnxruntime::Tensor::Init
onnxruntime::Tensor::MutableData
onnxruntime::Tensor::MutableDataAsSpan
onnxruntime::Tensor::MutableDataRaw
onnxruntime::Tensor::Reshape
onnxruntime::Tensor::SizeInBytes
onnxruntime::Tensor::Tensor
onnxruntime::TensorAllocator::TensorAllocator
onnxruntime::TensorSeq::Add
onnxruntime::TensorSeq::Get
onnxruntime::TensorSeq::SetType
onnxruntime::TensorShape::SizeFromDimension
onnxruntime::TensorShape::SizeToDimension
onnxruntime::TensorShape::Slice
onnxruntime::TensorTypeBase::GetElementType
onnxruntime::TensorTypeBase::IsCompatible
onnxruntime::TfIdfVectorizer::TfIdfVectorizer
onnxruntime::Tile::Compute
onnxruntime::ToMBString
onnxruntime::TopkOpset10ConstructorCommon
onnxruntime::TopkOpset11ConstructorCommon
onnxruntime::TopkOpset9ConstructorCommon
onnxruntime::ToWideString
onnxruntime::TransformerMemcpyImpl::ProcessDefs
onnxruntime::TransformerMemcpyImpl::ProcessInitializers
onnxruntime::TransformerMemcpyImpl::ProcessInitializers::<lambda_339aa899e5780231e41cee84a9ac8a33>::operator ()
onnxruntime::Transpose::Compute
onnxruntime::TransposeBase::TransposeBase
onnxruntime::TransposeOptimizer::ApplyImpl
onnxruntime::Trilu::Compute
onnxruntime::Trilu::Trilu
onnxruntime::TypedDoTransposeEltWise
onnxruntime::Unsqueeze::Compute
onnxruntime::UnsqueezeBase::PrepareCompute
onnxruntime::UnsqueezeBase::UnsqueezeBase
onnxruntime::UnsqueezeElimination::Apply
onnxruntime::UntypedExpand
onnxruntime::UpdateConsumerCount
onnxruntime::UpdateSubgraphsWithinFunctionBody
onnxruntime::Upsample<float>::BaseCompute
onnxruntime::Upsample<float>::Compute
onnxruntime::Upsample<int>::BaseCompute
onnxruntime::Upsample<int>::Compute
onnxruntime::Upsample<unsigned char>::BaseCompute
onnxruntime::Upsample<unsigned char>::Compute
onnxruntime::UpsampleBase::ParseScalesData
onnxruntime::UpsampleBase::ParseScalesDataFromOutputSize
onnxruntime::UpsampleBase::ScalesValidation
onnxruntime::UpsampleBase::StringToCoordinateTransformationMode
onnxruntime::UpsampleBase::StringToNearestMode
onnxruntime::UpsampleBase::StringToUpsampleMode
onnxruntime::UpsampleBase::UpsampleBase
onnxruntime::UpsampleNearest
onnxruntime::utils::BatchOrCopyMLValue
onnxruntime::utils::CalculateStaticCopyInfoForFeed
onnxruntime::utils::CalculateStaticCopyInfoForFeeds
onnxruntime::utils::ConstantNodeProtoToTensorProto
onnxruntime::utils::ContainerChecker::ContainerChecker
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<__int64,__int64,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,__int64> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<__int64,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<__int64,double,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,double> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<__int64,float,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,float> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,__int64,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,__int64> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > > > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,double,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,double> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,float> > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::vector<class std::map<__int64,float,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,float> > >,class std::allocator<class std::map<__int64,float,struct std::less<__int64>,class std::allocator<struct std::pair<__int64 const ,float> > > > > >::check
onnxruntime::utils::ContainerChecker::IsContainerOfType<class std::vector<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,float> > >,class std::allocator<class std::map<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> >,float,struct std::less<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >,class std::allocator<struct std::pair<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > const ,float> > > > > >::check
onnxruntime::utils::CopyInputsAcrossDevices
onnxruntime::utils::CopyOneInputAcrossDevices
onnxruntime::utils::CopyOutputsAcrossDevices
onnxruntime::utils::CopySparseData
onnxruntime::utils::DenseTensorToSparseTensorProto
onnxruntime::utils::detail::CopyLittleEndian
onnxruntime::utils::ExecuteGraph
onnxruntime::utils::ExecuteGraphImpl
onnxruntime::utils::FinalizeCopyInfoForFeeds
onnxruntime::utils::FinalizeCopyInfoForFetches
onnxruntime::utils::FindMemoryInfoForValue
onnxruntime::utils::GetElementTypeFromOptionalSeqTensor
onnxruntime::utils::GetElementTypeFromOptionalTensor
onnxruntime::utils::GetFileContent
onnxruntime::utils::GetMLDataType
onnxruntime::utils::GetShape
onnxruntime::utils::InitializeFeedFetchCopyInfo
onnxruntime::utils::mltype_dispatcher_internal::CallableDispatchableHelper::CheckCalledOnce
onnxruntime::utils::mltype_dispatcher_internal::UnsupportedTypeDefaultPolicy<class onnxruntime::common::Status>::operator ()
onnxruntime::utils::SparseTensorProtoToDenseTensorProto
onnxruntime::utils::TensorProtoToMLValue
onnxruntime::utils::TensorProtoToTensor
onnxruntime::utils::UnpackInitializerData
onnxruntime::utils::UnpackTensorWithExternalDataImpl
onnxruntime::ValidateCommonFastReduce
onnxruntime::ValidateFastReduceKR
onnxruntime::ValidateFastReduceKRK
onnxruntime::ValidateFastReduceRK
onnxruntime::ValidateKeepDims
onnxruntime::ValidateMustBeOverloaded
onnxruntime::ValidateNoTransposeReduce
onnxruntime::ViewerFunctionImpl::Body
onnxruntime::ViewerFunctionImpl::MutableBody
onnxruntime::WritableSliceIterator<__int64>::Init
onnxruntime::WritableSliceIterator<double>::Init
onnxruntime::WritableSliceIterator<float>::Init
onnxruntime::WritableSliceIterator<int>::Init
onnxruntime_profile_
onnxruntime_providers_cuda.dll
onnxruntime_providers_dnnl.dll
onnxruntime_providers_openvino.dll
onnxruntime_providers_rocm.dll
onnxruntime_providers_shared.dll
onnxruntime_providers_tensorrt.dll
Op registered for 
Op with name (
op_kernel_info.GetAttr("axis", &axis_).IsOK()
op_kernel_info.GetAttr<float>("bias", &bias_temp).IsOK()
op_kernel_info.GetAttr<float>("epsilon", &epsilon_).IsOK()
op_kernel_info.GetAttr<float>("lambd", &lambd_temp).IsOK()
op_kernel_info.GetAttr<int64_t>("axis", &axis_).IsOK()
op_kernel_info.GetAttr<int64_t>("axis", &axis_temp).IsOK()
op_kernel_info.GetAttr<int64_t>("k", &k_temp).IsOK()
op_kernel_info.GetAttr<int64_t>("largest", &largest_temp).IsOK()
op_kernel_info.GetAttr<int64_t>("p", &p_).IsOK()
op_kernel_info.GetAttr<int64_t>("sorted", &sorted_temp).IsOK()
op_type
o'p<5
opaque
Opaque type is not a non_tensor type!!!
opaque(
opaque_type
open file 
OpenSemaphoreW
OpenVINO_GPU
OpenVINOExecutionProvider
operation canceled
operation in progress
operation not permitted
operation not supported
operation would block
Operator '
OpKernel was null
opset id is null. Invalid ORT format model.
opset import domain is null. Invalid ORT format model.
opt_k_transpose perm attribute not matched
optimization.enable_gelu_approximation
optimization.save_runtime_optimizations
optimizer_utils::CheckOutputEdges(graph, up_node, 1)
Optional
optional
Optional is expected to have an output.
Optional is expected to have either an input or the type attribute set.
Optional op must have a TypeProto in the 'type' attribute if the attribute is present
Optional position subgraph nodes number of outputs unexpected.
Optional position subgraph nodes Where node is expected to be the parent of Reshape.
Optional scaling values used by some activation functions. The values are consumed in the order of activation functions, for example (f, g, h) in LSTM.
Optional scaling values used by some activation functions. The values are consumed in the order of activation functions, for example (f, g, h) in LSTM. Default values are the same as of corresponding ONNX operators.
Optional scaling values used by some activation functions. The values are consumed in the order of activation functions, for example (f, g, h) in LSTM. Default values are the same as of corresponding ONNX operators.For example with LeakyRelu, the default alpha is 0.01.
Optional Type mismatch. Expected: 
optional(
optional(seq(tensor(bool)))
optional(seq(tensor(complex128)))
optional(seq(tensor(complex64)))
optional(seq(tensor(double)))
optional(seq(tensor(float)))
optional(seq(tensor(float16)))
optional(seq(tensor(int16)))
optional(seq(tensor(int32)))
optional(seq(tensor(int64)))
optional(seq(tensor(int8)))
optional(seq(tensor(string)))
optional(seq(tensor(uint16)))
optional(seq(tensor(uint32)))
optional(seq(tensor(uint64)))
optional(seq(tensor(uint8)))
optional(tensor(bool))
optional(tensor(complex128))
optional(tensor(complex64))
optional(tensor(double))
optional(tensor(float))
optional(tensor(float16))
optional(tensor(int16))
optional(tensor(int32))
optional(tensor(int64))
optional(tensor(int8))
optional(tensor(string))
optional(tensor(uint16))
optional(tensor(uint32))
optional(tensor(uint64))
optional(tensor(uint8))
optional_type
OptionalGetElement
OptionalGetElement must have an input element.
OptionalHasElement
OptionalHasElement is expected to have 1 input.
OptionalHasElement is expected to have 1 output.
or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.
Order
OriginalFilename
Oriya
ORT config json from the model: 
ORT model verification failed.
ORT optimization- Force fallback to CPU execution for node: 
ort_config
ORT_LOAD_CONFIG_FROM_MODEL
ort_value.Fence() == nullptr
ort_value.IsAllocated()
ort_value.IsTensor()
ort_value_idx >= 0 && static_cast<size_t>(ort_value_idx) < alloc_plan.size()
ort_value_index >= 0 && static_cast<size_t>(ort_value_index) < all_values_size_
ort_value_index >= 0 && static_cast<size_t>(ort_value_index) < alloc_plan.size()
ort_value_name_idx_map.MaxIdx() > -1
OrtApis::CreateOpaqueValue
OrtApis::FillSparseTensorBlockSparse
OrtApis::FillSparseTensorCoo
OrtApis::FillSparseTensorCsr
OrtApis::GetOpaqueValue
OrtApis::GetTensorTypeAndShape
OrtApis::UseBlockSparseIndices
OrtApis::UseCooIndices
OrtApis::UseCsrIndices
OrtCreateMapMLValue
OrtCreateValueImplMapHelper
OrtCreateValueImplSeqHelper
OrtEnv::Release
OrtGetApiBase
OrtGetValueImplMapHelper
OrtGetWinMLAdapter
OrtMemoryInfo is null
OrtMemoryInfo:[
OrtProgrammingProjection
OrtSessionOptionsAppendExecutionProvider_CPU
OrtSessionOptionsAppendExecutionProvider_Cuda: Failed to load shared library
OrtSessionOptionsAppendExecutionProvider_DML
OrtSessionOptionsAppendExecutionProvider_Rocm: Failed to load shared library
OrtSessionOptionsAppendExecutionProviderEx_DML
OrtValue has not been allocated so can't be sliced.
OrtValue indexes should have been populated.
OrtValue is TensorSequence type but has no element Tensor DataType.
OrtValue shape verification failed. Current shape:
OrtValue should contain a Tensor or a Sparse Tensor
OrtValue::Get
OrtValue::GetMutable
Osage
Osmanya
other_error
other_sum_input != nullptr
out of index
Out of memory
out_of_range
Outer index count must be rows + 1 or zero. Got: 
Outer indices must be M + 1. Got: 
Outer scope node arg name '
outer_num == (rows + 1)
outer_scope_node_arg != nullptr
outer_scope_node_args_consumed.empty()
OutOfBoundsInputValue
Output
output
Output 
output != nullptr
output == output_end
Output buffer allocation failed
output buffer is too small. Use GetStringTensorDataLength.
Output case #1: Y, mean, var, saved_mean, saved_var (training mode)
Output case #1: Y, running_mean, running_var (training_mode=True)
Output case #2: Y (test mode)
Output case #2: Y (training_mode=False)
Output channels M is not divisible by group.
output count mismatch, expected 2 outputs to be present for TopK operator
Output data after scaling
Output data tensor.
Output edge count not expected for Add or MatMul in path v
Output edge count not expected for mask nodes
Output edge count not expected for nodes in gemm gather path
Output edge count not expected for nodes in gemm path
Output edge count not expected for nodes in past subgraph
Output edge count not expected for nodes in path 1 of position shape.
Output edge count not expected for nodes in path 1 of unidirectional mask
Output edge count not expected for nodes in path 2 of position shape.
Output edge count not expected for nodes in path v
Output edge count not expected for nodes in path1.
Output edge count not expected for Softmax
Output edge count not expected for squeeze_2/slices2/shape2 of unidirectional mask
Output edge count not expected for unsqueeze2 of unidirectional mask
Output edge count not expected for unsqueeze3 of unidirectional mask
output edges
output name cannot be empty
Output OrtValue has not been created for loop state variable output 
Output size mismatch.
Output subscript contains letters not seen in the inputs
Output subscript contains repeated letters
Output tensor
output tensor
Output tensor must have at least 2 dimensions
Output tensor of shape (M, N).
Output tensor of the same type and shape as the input tensor.
Output tensor of the same type as the input tensor. Shape of the output is * x M, where '*' is the shape of input indices, and 'M' is the embedding size.
Output type is determined by the specified 'values_*' attribute.
Output type must be int32 or int64
Output type not supported in this build: 
Output vector incorrectly sized: output_names.size(): 
Output vector pointer is NULL
Output was expected to have tensor type. Got 
output, dropout_mask = Dropout(data + bias, ratio) + residual, Intended to specialize the dropout pattern commonly found in transformer models.
output.SizeInBytes() == input.SizeInBytes()
Output:
output_dims.size() == dims.size()
output_dims[i] == 0
output_height
output_mlvalue
output_names_to_nodeinfo.empty()
output_node.OutputDefs().size() == 1
output_padding
output_ptr
output_sequence
output_shape
output_shape is smaller than minimum required. output_shape:
'output_shape' must be rank 1 tensor.
'output_shape' must have same number of elements as the shape of input tensor X.
output_size
output_width
OutputBiasGradientTensor
OutputCellSingleTensor
OutputCoordinatesTensor
OutputCount
OutputCountTensor
OutputDebugStringW
OutputFirstMomentTensor
OutputGradientTensor
OutputIndexTensor
OutputIndicesTensor
OutputMeanTensor
OutputPadding
OutputParametersTensor
OutputPixelOffset
OutputPixelOffsets
OutputROIGradientTensor
outputs
Outputs from Scan are not optional and should never be null.
outputs...
OutputScaleGradientTensor
OutputScaleTensor
OutputSecondMomentTensor
OutputSequenceTensor
OutputSingleTensor
OutputStateTensor
OutputTensor
OutputTensors
OutputValueTensor
OutputVarianceTensor
OutputZeroPointTensor
owner dead
OYV4Z
p p t y 
p value of the Lp norm used to pool over the input data, default is 2.0.
p value of the Lp norm used to pool over the input data.
p,;L$
p.first->second->id_ == 0
p.second
P;KPt
P;O0t
p;S u
p_ == 1 || p_ == 2
p_fetches->size(): 
p_int < base_int + memory_size_
p_int >= base_int
p_kernel_def
p_ml_value
p_mlvalue
p_op_kernel
p_provider
p_type != nullptr
p|+px
P|+Px
p|+px
P|+Px
p|+px
P|+Px
P|+Px3
p|+pxXf
p7M}6p7M
P9QTt
Pad should be smaller than kernel.
pad_value
Padding for the beginning and ending along each axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the beginning and end part of the corresponding axis. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`. This attribute cannot be used simultaneously with auto_pad attribute.
Padding for the beginning and ending along each spatial axis, it can take any value greater than or equal to 0. The value represent the number of pixels added to the beginning and end part of the corresponding axis. `pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`. This attribute cannot be used simultaneously with auto_pad attribute. If not present, the padding defaults to 0 along start and end of each spatial axis.
Padding for the beginning and ending along each spatial axis, it can take any value greater than or equal to 0.The value represent the number of pixels added to the beginning and end part of the corresponding axis.`pads` format should be as follow [x1_begin, x2_begin...x1_end, x2_end,...], where xi_begin the number ofpixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`.This attribute cannot be used simultaneously with auto_pad attribute. If not present, the padding defaultsto 0 along start and end of each spatial axis.
padding_idx
padding_mode
padding_mode "
padding_mode_str == "zeros" || padding_mode_str == "border" || padding_mode_str == "reflection"
PaddingMode
paddings
PaddingValue
PaddingValueDataType
Pads has incorrect number of values
'pads' has wrong number of values
'pads' input must be a 1D (shape: [2 * input_rank]) tensor of type int64
'pads' input must be a 1D (shape: [2 * n_input_dims]) tensor of type int64
'pads' input must be a 1D (shape: [input_rank]) or 2D tensor (shape: [1, input_rank]) of type int64
Pads tensor should be a 1D tensor of shape [2 * input_rank] or a 2D tensor of shape [1, 2 * input_rank]
Pads tensor should be an INT64 tensor
Pads tensor size should be equal to twice the input dimension count 
pads[dim] < kernel_shape[dim] && pads[dim + kernel_shape.size()] < kernel_shape[dim]
pads_[dim] < kernel_shape_[dim] && pads_[dim + kernel_shape_.size()] < kernel_shape_[dim]
pads_size == 2 * data_rank
pads_tensor.IsDataType<int64_t>()
pads_tensor_dims.size() == 1 || (pads_tensor_dims.size() == 2 && pads_tensor_dims[0] == 1)
Pahawh_Hmong
Palmyrene
Parallel execution mode does not support the CUDA Execution Provider. 
Parallel execution mode does not support the DML Execution Provider. 
Parallel mode
ParallelExecutor::Execute
parameter_size
ParametricSoftplus
ParametricSoftplus takes one input data (Tensor<T>) and produces one output data
parse
parse error
parse_error
ParseData type mismatch for tensor: 
parsing 
PartA_PrivTags
Pass 1 to enable broadcasting
Pass CheckNodesInPathK
Pass CheckNodesInPathQ
Pass CheckNodesInPathV
Pass MatchGemmSubgraph
Pass MatchInputMaskSubgraph
Pass MatchInputMaskSubgraphDistilBert
Pass MatchPastSubgraph
Pass MatchUnidirMaskSubgraph
Pass ValidateGemmInitializer
past_k_gather indices != 0
past_k_transpose perm attribute not matched
past_v_gather and past_k_gather does not have same past input
past_v_gather indices != 1
PathCchRemoveBackslash
PathCchRemoveFileSpec
pattern length 
pattern too large - compile failed
Pau_Cin_Hau
PeepholeTensor
Per-column quantization parameter of batched matrix should have same dimension as the matrix,and its size by K should be equal to the matrix's size.
Perform mean variance normalization.
Performs element-wise binary {name} on 8 bit data types (with Numpy-style broadcasting support).
perm.size() == gsl::narrow_cast<size_t>(shape_proto->dim_size())
perm: 
permission denied
Permutation entry 
Permutation length 
Ph $D
Ph \G
Ph ]G
Ph |F
Ph AG
Ph GG
Ph jG
Ph$`F
Ph$|E
Ph$HF
Ph(zE
Ph,,F
Ph,rG
Ph@~E
Ph@kF
Ph@oG
Ph@SE
Ph\(G
Ph\4G
Ph\bE
Ph\lF
Ph`#D
Ph`.F
Ph`|F
Ph|lE
Ph|pF
Ph|RG
Ph<.D
Ph<_H
Ph<9F
Ph<GE
Ph<mF
Ph0 G
Ph0[D
Ph0<F
Ph0zG
Ph4CG
Ph4gF
Ph4nE
Ph4wG
Ph8AF
Ph8tG
Ph8TH
Ph8yG
Phags_Pa
Phd%D
PhD%G
Phd0D
PhD1G
PhDpE
PhdYF
PhH G
PhH#G
Phh$G
Phh;G
Phh|G
Phh+G
PhhdG
PhheF
PhhgF
PhHgG
PhHiF
PhHnG
PhHqF
PhhUF
PhHVE
PhhvG
PhhVG
PhHVH
PhHzF
PhL*F
PhL*G
Phl`F
PhL}G
PhLjG
PhLQD
Phoenician
Php#Q
Php`G
PhP2G
PhpmG
PhpuD
PhT)F
PhT>E
PhT8F
PhtcH
PhtmF
Phttp://www.microsoft.com/pkiops/certs/Microsoft%20Time-Stamp%20PCA%202010(1).crt0
PhTvD
PhtVH
Phx,G
PhX:G
Phx_H
Phx0G
PhxAG
Phx-G
PhXGF
PhxgG
PhxoE
PhxTD
Pj2jP
PjoZRY
Please fetch output tensor with specified shape.
Please register the allocator as OrtDeviceAllocator even if the provided allocator has arena logic built-in. OrtArenaAllocator is reserved for internal arena logic based allocators only.
points_.empty()
pool_int64s
pool_strings
pool_strings must not be empty if specified
pooled_height_ > 0
pooled_shape
pooled_shape.size() == 2
pooled_width_ > 0
PooledSize
Popularity of each node, used for performance and may be omitted.
position
Position embedding data type shall be float or float16.
Position embedding scale must be a scalar or 1D tensor of size 1
Position embedding shape is not expected.
Position embedding shape not matched.
Position embedding zero point must be a scalar or 1D tensor of size 1
position_ >= 0 && position_ < sequence_length_
position_embedding
position_embedding is expected to have 2 dimensions, got 
position_embedding should have 2 dimensions, dimension size known, and same hidden size as word_embedding.
position_embedding_quant
position_embedding_scale
position_embedding_zero_point
position_embeddings
position_ids
positive
post_transform
Pow takes input data (Tensor<T>) and exponent Tensor, and
PPhluD
PPPPPPPPPPP
PPPPPR
PPPPPWS
pq27L
PQhd*I
PQPQW
PQQQQQh
PQQQQQh 
PQQQR
PQQQV
PQQSV
PQQSVW
PQQVW
PQQWQ
PQSPh
PQSPhX
PQSPVQh
PQSPWQh
PQSVW
PQVQQ
PQVVhB
PQWPSQh
PQWPSQh([D
pred_tokens
pred_tokens_len == (src_tokens_len + 1 - prev_suffix_match_idx_data)
predictions.size() == (size_t)n_targets_or_classes_
predictions.size() == 2
predictions.size() == predictions2.size()
PrefixShape = Slice (XShape, Zero1D, Axis1D)
PRelu
prepacked_buffers[0].get() == nullptr
present
present_k_transpose perm attribute not matched
present_k_unsqueeze axes value not expected
present_v_unsqueeze axes value not expected
prev_suffix_match_idx
Previous entry was not terminated.
PRhp#Q
PRkND
PRkNp
prob_a
prob_b
proba_.size() == probb_.size()
Processed_STD
ProcessInfo
Produces a slice of the input tensor along multiple axes. Similar to numpy:
produces one output data (Tensor<T>) where the function `f(x) = x^exponent`,
ProductName
ProductVersion
Profiler is disabled.
Profiler not started yet
program size 
projected_index.size() > 0
proto != nullptr
Protobuf parsing failed.
Protobuf serialization failed.
protocol error
protocol not supported
Provided allocator is null
provided axis. The resulting tensor has the same rank as the input if keepdims equal 1.
Provided OrtMemoryInfo is null
Provided type is not an optional sequence tensor
Provided type is not an optional tensor
provider
Provider 
Provider_SetHost
PRPVW
PRQVQ
PRWQf
Psalter_Pahlavi
PShp#Q
PSjZW
PSQVP
PSSPVQh
PSVRWQ
PSWh(
PSWh@
PSWh`
PSWh8
PuY,Rg>
PVhp#Q
PVRQjoY
PVWWRh
PWj%S
PWj&S
PWj(S
PWj{S
PWj|S
PWj}S
PWj~S
PWj=S
PWj>S
PWj3S
PWjTS
PWQh0[D
PWVRjoZRY
pytorch_half_pixel
PZR_G
q and v are not from same Split node
q root should be layer normalization
Q!m"3
Q;CPt
Q;Ctt
q\Q17
q_matmul and q_add shape not matched
q_reshape const not matched
q_transpose perm attribute not matched
q_weight
Q|+Qx
Q0p0z0
QAttention
QBC;]
qdA3)
qdq_s8_to_u8_quant
qdq_s8_to_u8_zp_conversion
QDQPropagationTransformer
QDQS8ToU8Transformer
QDQSelectorActionTransformer
QEmbedLayerNormalization
QGemm
QGemm : scale of input a must be a scalar or 1D tensor of size 1
QGemm : scale of input b must be a scalar or 1D tensor of size 1 or N
QGemm : scale of y must be null or a scalar or 1D tensor of size 1
QGemm : zero point and scale of input b should have same shape size
QGemm : zero point of input a must be a scalar or 1D tensor of size 1
QGemm : zero point of input b must be a scalar or 1D tensor of size 1 or N
QGemm : zero point of y must be null or a scalar or 1D tensor of size 1
Qh$|E
Qh([D
Qh@jH
Qh\jH
qh`DE
Qh`nH
Qh<[F
Qh<uH
Qh0[D
Qh0iH
Qh8tH
qh9%jhx
qH9wH
QhDnH
QhDsH
Qhh:H
QhLjH
QhLsH
Qhp)Q
QhTnH
QhTqH
QhxgH
Qj+h(RE
qk_div const not matched.
qkv_bias
qkv_hidden_sizes
qkv_hidden_sizes attribute should have 3 elements
qkv_hidden_sizes first element should be same as the second
qkv_hidden_sizes should have 3 elements
qkv_sizes doesn't match the wights dimension
qkv_weights
QLinear
QLinear Pooling unsupported pooling size!
QLinearAdd
QLinearAveragePool
QlinearBuildLookupTable : input X_scale must be a scalar or 1D tensor of size 1
QlinearBuildLookupTable : input X_zero_point must be a scalar or 1D tensor of size 1
QlinearBuildLookupTable : input Y_scale must be a scalar or 1D tensor of size 1
QlinearBuildLookupTable : input Y_zero_point must be a scalar or 1D tensor of size 1
QLinearConcat
QLinearConv
QLinearConv : filter scale shape invalid
QLinearConv : filter zero point shape invalid
QLinearConv : input scale must be a scalar or 1D tensor of size 1
QLinearConv : input zero point must be a scalar or 1D tensor of size 1
QLinearConv : result scale must be a scalar or 1D tensor of size 1
QLinearConv : result zero point must be a scalar or 1D tensor of size 1
QLinearConv : zero point of per-channel filter must be same
QLinearGlobalAveragePool
QLinearGlobalAveragePool parameter out of computation range!
QLinearLeakyRelu
QLinearMatMul
QLinearMatmul : input scale must be a scalar or 1D tensor of size 1
QLinearMatmul : input zero point must be a scalar or 1D tensor of size 1
QLinearMatmul : result scale must be a scalar or 1D tensor of size 1
QLinearMatmul : result zero point must be a scalar or 1D tensor of size 1
QLinearMatmul : weight scale must be a scalar, 1D tensor of size 1, or last to second dimension is 1
QLinearMatmul : weight zero point must be a scalar, 1D tensor of size 1, or last to second dimension is 1
QLinearMul
QLinearReduceMean
QLinearSigmoid
QQFVQh
QQFVQWQh
QQhluD
QQQQQh
QQQQSPQQ
QQQVh
QQRPj
QQRQP
QQRVQh
QQRVQh0[D
QQSPQh
QQSVW
QQSVW3
QQSVWd
QQV;U
QQVQQP
QQVW3
QQWQh
QQWQhT
QRh,QH
QRhp3I
QRjpZjoY
QRPh(
QRPhH
QRQPQQ
QSQQVQQP
QSSWQh
QSVPP
QSVPW
QSVW3
QSVWj
QSVWQ
Quantized GEMM only support alpha equal to 1.0f and beta equal to 0.0f or 1.0f
QuantizeLinear
query
QueryPerformanceCounter
QueryPerformanceFrequency
QVh$!H
QVh([D
QVh,/H
QVh`DH
QVh0)H
QVh46H
QVh8%H
QVhh~J
QVhHDH
QVQh([D
QVQh0
QVQh0[D
QVQh4
QVVPP
QVVQh
QVVWQh
QVVWQh<uH
QWQh([D
QWQh0
QWQh0[D
QWQh4
QWQh8
QWQhD
QWWhT
QWWQhX
QWWQVQh0[D
R!s4Z
R-$y2
r,;G(t'
r,;G0t'
R_scale
R_zero_point
r|+rx
r~FG{P
R->Shape()[1] == 5
r0\1f1{1
r3;O4
R8m8w8
raB3G
RaiseException
RaiseFailFastException
RandomNormal
RandomNormalLike
RandomUniform
RandomUniformLike
Range
Rank = Size (XShape)
rank >= 2 && dim_1 != dim_2 && input_dims[dim_1] == input_dims[dim_2]
rank must be greater than axis
Rank of input 
Rank of input and output tensor should be same.
Rank of input to Normalized must be less than 2. Got 
Rank of the input must match number of subscript labels corresponding to the input
Ranks inferred (
Ranks of input data are different, cannot concatenate them. expected rank: 
Ranks of pair-wise operands must be equal. 
RAQQRh
RAQQRhX
RAQQRVQh0[D
RAQQRWQh0[D
ratio
ratio input should have a single value.
ratio must be in the range [0, 1)
Ratio of Dropout must be a scalar.
ratio_tensor->Shape().Size() == 1
raw_data
rD;rDt&
RE2: invalid startpos, endpos pair. [
RE2: unexpected op: 
read only file system
ReadExternalRawData() failed: 
ReadFile
ReadFile 
Reading the provided model for the ORT config
Real memory steps 
Received invalid value for allow_spinning. Valid values are 0 or 1
Received invalid value of arena_extend_strategy 
Received negative size from stat call
Received null OrtThreadingOptions
Received nullptr for custom registry
Received nullptr for exec provider
Received nullptr for graph transformer
Received nullptr for name.
Received nullptr for OrtValue.
Received OrtValue is not a tensor. Only tensors are supported.
Reciprocal
RecurrenceTensor
Recurrent
Redmond1
reduced
reduced_scale
reduced_zero_point
ReducedShape = Concat <axis = 0> (PrefixShape, SuffixShape)
ReduceL1
ReduceL2
ReduceLogSum
ReduceLogSumExp
ReduceMax
ReduceMean
ReduceMin
ReduceProd
ReduceSum
ReduceSumInteger
ReduceSumSquare
reduction
Reduction on all axes, output size should be 1.
ReductionFunction
reflect
reflection
Regexp not destroyed.
RegisterCustomOps
RegisterCustomOpsLibrary: Entry point RegisterCustomOps not found in library
RegisterCustomOpsLibrary: Failed to load library
Rejang
Release_State_
ReleaseMutex
ReleaseSemaphore
ReleaseSRWLockExclusive
ReleaseSRWLockShared
ReluQuantRewrite
RemoveDirectory() failed - path: 
RemoveDirectoryW
RemoveDuplicateCastTransformer
Removing initializer '
Removing NodeArg '
'repeat' input tensor must be 1 dimensional
'repeat' input tensor must have the same length as the 'input' tensor
repeats
Repeats
'Repeats' input has incorrect number of values. The number of values in 'repeats' must be equal to the number of input dimensions.
'Repeats' input must be 1D tensor of type int64
RepeatsCount
RepetitionWalker::ShortVisit called
replaced_value_float
replaced_value_int64
representative.output_index != kInvalidOutputIndex
Requested attribute: 
Requested size is too large to fit into size_t.
requested_shape[i] >= -1
Required attribute '
Required attribute axis is missing
Required input at index 
Required min_gram_length must be positive: 
Required output at index 
reserved_chunks_.find(ptr) == reserved_chunks_.end()
Reserving memory in BFCArena for 
ResetEvent
Reshape
reshape initializer value is not expected
reshaped
ReshapeFusion
residual
Resize
Resize operator
Resize: input shape needs to be at least a single dimension
Resize: input tensor's dimension does not match the scales.
Resize: input tensor's rank does not match the output tensor's rank.
Resize: input/output value is nullptr
Resize: input/output value's dimension mismatch
Resize: size of roi array should be 2 * N where N is the rank of input tensor X.
Resize: unexpected mode
Resolve subgraph failed:
resource deadlock would occur
resource unavailable try again
result
Result buffer is not large enough
result out of range
Result, has same shape and type as input
Result, has same type as input, with H and W dimensions reduced.
ReturnHr
reused != reused_for
reverse
ReverseSequence
Rh kH
Rh qH
Rh@tH
Rh@uH
Rh\fH
Rh\sH
Rh`5H
Rh<}H
Rh0iH
Rh4lH
Rh8_I
Rh8vH
Rhd>H
RHJz}
RhLgH
Rhp#Q
Rhp1I
RhtjH
RhtuH
Rhx`I
RhX0I
Rhy=<
RIGHT
right operand cannot broadcast on dim 
Right shape: 
right.NumDimensions() == 2
right.Shape().Size() == right_shape_override.Size()
RknpuExecutionProvider
RNN op: Invalid activation attribute - 
ROCMExecutionProvider
ROI pool output shape (height, width).
RoI pooled output, 4-D tensor of shape (num_rois, C, crop_height, crop_width). The r-th batch element Y[r-1] is a pooled feature map corresponding to the r-th RoI X[r-1].
roi_batch_id < batch_size
roi_batch_id >= 0
roi_input_idx_ > 0
RoiAlign
RoIs (Regions of Interest) to pool over; rois is 2-D input of shape (num_rois, 4) given as [[y1, x1, y2, x2], ...]. The RoIs' coordinates are normalized in the coordinate system of the input image. Each coordinate set has a 1:1 correspondence with the 'batch_indices' input.
rois input tensor has wrong dimension
RoIs tensor must have 2 dimensions
ROITensor
RoOriginateLanguageException
Round
round_prefer_ceil
round_prefer_floor
RoundingMode
rP/KEi4
RQVSQ
RRQPRh
RRQVQhh:H
RRRRP9Wdu7
RtlUnwind
run_options.run_log_severity_level >= 0 && run_options.run_log_severity_level <= static_cast<int>(logging::Severity::kFATAL)
Runic
Running with tag: 
running_mean
running_mean = input_mean * momentum + current_mean * (1 - momentum)
running_var
running_var = input_var * momentum + current_var * (1 - momentum)
RunStateOnByteUnlocked failed after Reset
RunStateOnByteUnlocked failed after ResetCache
RUNTIME_EXCEPTION
RuntimeError
RuntimePerf
runtimeVersion
rZy8T
's number of inputs is different from function body graph's number of input.
's number of outputs is different from function body graph's number of outputs.
S WVj
S-"OzjU
s(+CH
S;B(t@
S{6SL
S0Z0{0
s4+s0;
s4WQS
S4WVj
s5Q~)Ot
SafeIntExceptionHandler<class onnxruntime::OnnxRuntimeException>::SafeIntOnDivZero
SafeIntExceptionHandler<class onnxruntime::OnnxRuntimeException>::SafeIntOnOverflow
Samaritan
SAME_UPPER
Sample echo operator.
sample_size
SampleOp
Sampling ratio should be >=0, but it was 
sampling_ratio
sampling_ratio_ >= 0
Saurashtra
SaveAttributeOrtFormat: Unsupported attribute type: 
Saved inverse standard deviation used during training to speed up gradient computation.
Saved inverse standard variance used during training to speed up gradient computation.
Saved mean used during training to speed up gradient computation
saved_mean
saved_var
SaveMLValueNameIndexMapping
SaveValueInfoOrtFormat: value_info_proto for 
Saving initialized tensors.
Saving runtime optimizations is not enabled in this build.
sbetu
sC;:w
Scalar multiplier for input tensor C, the default value is 1.0.
Scalar multiplier for input tensor C.
Scalar multiplier for the product of input tensors A * B, the default value is 1.0.
Scalar multiplier for the product of input tensors A * B.
Scalar multiplier for the product of the input tensors.
Scale
scale
scale > 0
scale >= 1
Scale and bias the input image. Bias values are stored in
Scale and Zero-point must be a scalar
scale must be 1D tensor with size 
'Scale' must contain exactly 2 values - (height, width)
Scale size: (
Scale takes one input data (Tensor<float>) and produces one output data
Scale tensor.
Scale value should be greater than 0.
Scale value should be greater than or equal to 1.
scale.Shape().NumDimensions() == 1 && scale.Shape()[0] == broadcast_dim
scale_.size() == offset_.size()
scale_grad_by_freq
Scale2D = Flatten <axis = 0> (Scale)
ScaleBias
ScaleCount
Scaled = Mul (NormalizedT, Scale2D)
ScaledTanh
scaledtanh
Scaler
Scales
scales
scales size should be greater than 0.
scales.size() == 2 || (scales.size() == 4 && scales[0] == 1 && scales[1] == 1)
scales.size() == 2 || (scales.size() == 4 && scales[0] == 1 && scales[1] == 1) || (scales.size() == 4 && scales[0] == 1 && scales[3] == 1) || scales.size() == 3 || (scales.size() == 5 && scales[0] == 1 && scales[1] == 1)
scales_size > 0
ScaleSize
ScaleTensor
Scaling parameter.
Scaling value
Scan 'body' subgraph outputs should all be tensors but output 
Scan input 
Scan inputs have inconsistent batch size. Previous value was 
Scan inputs have inconsistent sequence lengths. Previous value was 
scan_input_axes
scan_input_directions
scan_output_axes
scan_output_directions
Scan<8> spec does not support transpose of output. This should never be called.
Scatter
ScatterElements
ScatterND
Schema error: 
schemaVersion
score_threshold
scores
scores must be a 3D tensor.
Scores output is incorrect size. Expected:
scores.size() == static_cast<size_t>(expected_num_scores)
scores_dims.size() == 2
scores_dims[0] == batch_size
scores_out
scores_output_data.length() >= scores_output_size
scores_tensor
SCSSSWQhh
SearchBitState inconsistency
SearchDFA inconsistency
SearchNFA inconsistency
SearchOnePass inconsistency
Second dimension for rois should be exactly 
Second input does not have rank 2
Second input of Gather in path 1 of position shape should be a constant with value 0.
Second input of Gather in path 2 of position shape should be a constant with value 1.
Second input of Gather should be a constant with value 1. 
Second input tensor has wrong dimension
Second set of probability coefficients. This array must be same size as prob_a.<br>If these are provided then output Z are probability estimates, otherwise they are raw scores.
Second, multiply by this.<br>Can be length of features in an [N,F] tensor or length 1, in which case it applies to all features, regardless of dimension count.<br>Must be same length as 'offset'
Seed for the hashing algorithm, unsigned 32-bit integer, default to 0.
Segment embedding scale must be a scalar or 1D tensor of size 1
Segment embedding zero point must be a scalar or 1D tensor of size 1
Segment id is not valid. 
segment_embedding
segment_embedding is expected to have 2 dimensions, got 
segment_embedding should have 2 dimensions, dimension size known, and same hidden size as word_embedding.
segment_embedding_scale
segment_embedding_zero_point
segment_ids
segment_ids input shall be 2 dimensions
select_last_index
selected_indices
selectors_and_actions_map_.find(name) == selectors_and_actions_map_.cend()
separators
separators must not be empty
seq(map(int64, float))
seq(map(string, float))
seq(tensor(bfloat16))
seq(tensor(bool))
seq(tensor(complex128))
seq(tensor(complex64))
seq(tensor(double))
seq(tensor(float))
seq(tensor(float16))
seq(tensor(int16))
seq(tensor(int32))
seq(tensor(int64))
seq(tensor(int8))
seq(tensor(string))
seq(tensor(uint16))
seq(tensor(uint32))
seq(tensor(uint64))
seq(tensor(uint8))
seq_lengths
Sequence
Sequence is missing type entry for its element
Sequence of (Tensor, Scale, ZeroPoint) tuples. The type is sequence of (T8, TF, T8).
sequence_lens
sequence_lens length of 
'sequence_lens' must have rank of 1
sequence_lens shape must be {batch_size}. Got:
sequence_type
SequenceAt
SequenceConstruct
SequenceConstruct is expected to have at least 1 input.
SequenceEmpty
SequenceErase
SequenceInsert
SequenceLength
SequenceLengthsTensor
Sequences must have tensors of the same data type. There was at least one tensor in the input that was different.
Sequential execution should be enabled when using DML execution provider.
Sequential mode
SequentialExecutor::Execute
Serialization error. Graph attribute was serialized without Graph instance
Serialization of fused function body is not currently supported, 
Serialized version info is null. Invalid ORT format model.
Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.
Session
session-
Session has already been initialized.
Session must be initialized to create session state.
Session not initialized.
Session successfully initialized.
Session was not initialized
session.disable_prepacking
session.disable_quant_qdq
session.inter_op.allow_spinning
session.intra_op.allow_spinning
session.load_model_format
session.save_model_format
session.set_denormal_as_zero
session.use_device_allocator_for_initializers
session.use_env_allocators
session.use_ort_model_bytes_directly
session_env.EnvCreatedWithGlobalThreadPools()
session_initialization
session_logger != nullptr
session_options
session_options_.session_log_severity_level >= 0 && session_options_.session_log_severity_level <= static_cast<int>(logging::Severity::kFATAL)
session_state
SessionCreation
SessionCreationStart
sessionId
SessionOptionsAppendExecutionProvider_OpenVINO: Failed to load shared library
SessionOptionsAppendExecutionProvider_Tensorrt: Failed to load shared library
SessionState for subgraphs is null. Invalid ORT format model.
SessionState is null. Invalid ORT format model.
SessionState should have saved the KernelCreateInfo prior to this running. NodeIndex:
SetEvent
SetFilePointerEx
SetFilePointerEx 
SetGraphAndCreateKernels must be called prior to GetExecutionInfo.
SetLastError
SetThreadAffinityMask
SetThreadDescription
setting data_type field (tensor name: 
Setting enable_profiling to 
Setting execution_mode to 
Setting graph_optimization_level to ORT_DISABLE_ALL
Setting graph_optimization_level to ORT_ENABLE_ALL
Setting graph_optimization_level to ORT_ENABLE_BASIC
Setting graph_optimization_level to ORT_ENABLE_EXTENDED
Setting inter_op_num_threads to 
Setting intra_op_num_threads to 
SetUnhandledExceptionFilter
SetupSubgraphExecutionInfo should only be called once for each subgraph.
sGJ]Wp
Sh\(E
shape
Shape
shape && sp_tensor.DenseShape() == *shape
shape && tensor.Shape() == *shape
shape as a contiguous subset of the first tensor's shape. The starting of the
Shape inference error(s): 
'shape' input must be 1D tensor of type INT64
Shape input must be a one-dimensional tensor.
shape is invalid
Shape mismatch attempting to re-use buffer. 
Shape must be 1 dimensional as it's tensor data of a shape
shape of layer norm bias tensor not expected
shape of left-hand-side argument. When broadcasting is specified, the second
shape.Size() must >=0
shape_.Size() == new_shape.Size()
shape_data_tensor.Shape().GetDims().size() == 1
shape_size == out.length()
Shape3D
shapeTensor->Shape().NumDimensions() == 1
Sharada
Shavian
Should be unreachable if CanRemoveNodeAndMergeEdges is in sync with the logic here.
should never happen
Should not have entry in kernel create info with nullptr for kernel_def
Shouldn't be possible to have NodeArgs that haven't been handled already.
ShpnH
Shrink
Siddham
Sigmoid
sigmoid
signal_ndim
SignWriting
SimplifiedLayerNormalization
SimplifiedLayerNormFusion
Simplify case not handled: 
SimplifyWalker::ShortVisit called
Single dimension value must be greater than 0
single_node_compute_func should have 1 elements
Sinhala
size != 0 && (input_shape.Size() % size) == 0
size >= 0
size is different
Size mismatch for kernel create info node indexes and hashes. Invalid ORT format model.
Size mismatch validating subgraph inputs. Got 
Size mismatch: feed_names has 
size overflow
size_ % 2 == 1
size_ == size
size_ > 0
size_t(impl_->max_gram_length_) <= impl_->ngram_counts_.size()
size_t(impl_->min_gram_length_) <= impl_->ngram_counts_.size()
sizeof(uint32_t) == output_element_bytes
sizes
Sizes
sizes != nullptr && sizes->Shape().Size() != 0
sizes == nullptr
sJF_d
SjpZjoY
sK;9w
skip is expected to have same shape as input
SkipLayerNormFusion
Sleep
SleepConditionVariableCS
SleepConditionVariableSRW
Slice
Slice does not have enough number of inputs
Slice ends is less than INT_MAX
Slice op must have either three, four or five inputs.
Slice parameter is not expected. Input index:
slice the input `data` tensor. If a negative value is passed for any of the
Sliced data tensor.
slices of `data` into an output tensor of rank q - 1 + r - indices[-1].
Slices uses `axes`, `starts` and `ends` inputs to specify the start and end
SLjt_
slope
SlopeTensor
snprintf() failed with return value: 
snprintf_result > 0
snprintf_result > 0 && gsl::narrow_cast<size_t>(snprintf_result) == buffer_span.size() - 1
So disabling it for this session since it uses the DML Execution Provider.
So making the execution mode sequential for this session since it uses the CUDA Execution Provider.
So making the execution mode sequential for this session since it uses the DML Execution Provider.
SOFTMAX
Softmax
Softmax attribute axis is expected to be 3
softmax_axis
SOFTMAX_ZERO
SoftmaxCPU inputs N, D and N * D must be < 
SoftmaxCrossEntropyLoss
softplus
Softplus
softsign
Softsign
Sogdian
Some nodes are not included in the topological sort, graph have a cycle.
Sora_Sompeng
sorted
source and destination buffer size mismatch
Source and target must both be tensors
source optional type missing element type.
source sequence type missing element type.
Soyombo
SpaceDepth ops require a 4-D input. Provided rank: 
SpaceToDepth
SpaceToDepth requires input height to be a multiple of block_size
SpaceToDepth requires input width to be a multiple of block_size
SPARSE
Sparse format must not be set. Already contains format: 
Sparse indices int32 data size does not match expected
Sparse indices int64 data size does not match expected
Sparse Indices raw data size does not match expected.
Sparse initializer must have a name. This model is invalid
Sparse Initializer tensor is missing. Invalid ORT format model.
Sparse tensor (
Sparse Tensor does not contain sparse data
Sparse tensor indices (
Sparse tensor initializers must have a non-empty name
sparse tensor type 
Sparse tensor values (
sparse_tensor
SPARSE_TENSOR
sparse_tensor(
sparse_tensor(double)
sparse_tensor(float)
sparse_tensor(int32)
sparse_tensor(int64)
sparse_tensor(uint32)
sparse_tensor(uint64)
sparse_tensor_names_ not in sync with name_to_initial_tensor_
sparse_tensor_names_.count(tensor_name) == 0
sparse_tensor_proto
sparse_tensor_type
SPARSE_TENSORS
sparse_tensors
sparse_value
SparseTensor Allocation failed for size: 
SparseToDenseMatMul
spatial
Spatial
spatial_scale
spatial_scale_ > 0
SpatialScale
SpatialScaleX
SpatialScaleY
specific_subgraph_kernel_create_info_map != subgraphs_kernel_create_info_maps_.end()
Specified axis to insert a dimension
Specified device is not supported.
Specified domain and type names combination does not refer to a registered opaque type
Specified provider is not supported.
Specifies a target value that is ignored and does not contribute to the input gradient. It's an optional value.
Specify batchs of sequence words to embedding
Specify bias of conv
Specify embedding vector of char
Specify if the RNN is forward, reverse, or bidirectional. Must be one of forward (default), reverse, or bidirectional.
Specify weights of conv
Split
split
Split operator does not support 
Split should be > 0
split_scalar > 0
split_size_sum (
split_tensor->Shape().NumDimensions() == 1
SplitToSequence
SplitToSequence operator does not support 
sqeuclidean
SQh([D
SQh`nH
SQh0[D
SQhDnH
SQQVQQP
Square = Mul (XU, XU)
SquareOfMean = Mul (Mean2D, Mean2D)
Squeeze
squeeze_mask
squeezed
Src and Dst must be of the same type
src and dst must have same shape and not be rank 0.
src and dst types must match
src.SizeInBytes() == dst.SizeInBytes()
src_tokens
src_tokens_len >= prev_suffix_match_idx_data
SRPSQ
SS8_,
SSPVQh
SSPWQh
SSPWQh0[D
SSQSPS
SSSPVQh
SSSPWQh0[D
SSSSP9_du7
SSSSWQhh
SSSVQhh
SSSWQh$
SSSWQh`nH
SSSWQhh
Stack not empty.
Stacktrace:
start
Start CheckNodesInPathK
Start CheckNodesInPathQ
Start CheckNodesInPathV
Start FuseGptAttention
start in Range operator should be scalar like tensor, yet got shape:
Start MatchGemmSubgraph
Start MatchInputMaskSubgraph
Start MatchInputMaskSubgraphDistilBert
Start MatchPastSubgraph
Start MatchUnidirMaskSubgraph
start or end indices, it represent number of elements before the end of that
Start ValidateGemmInitializer
start_offset % span_size == 0 && real_end % span_size == 0
start_offset >= 0 && real_end >= 0 && start_offset <= real_end && real_end <= len
Starting indices of corresponding axis in `axes`
StartPadding
startpos: 
starts
Starts and axes shape mismatch
Starts and ends shape mismatch
Starts and steps shape mismatch
Starts must be a 1-D array
starts.size()=
starts_.empty() || start > ends_.back()
starts_.size() == ends_.size()
starts_.size() == ends_.size() + 1
stash_type
state (NxD), and the sequence lengths (N), computes the GRU
state not recoverable
StateSaver failed to restore state.
static_cast<int>(activation_func_names.size()) == num_directions_ * 3
static_cast<size_t>(index) < nodes_.size() && ((node = nodes_[index]) != nullptr || !required)
static_cast<size_t>(num_subgraph_inputs) == subgraph_inputs.size()
static_cast<uint64_t>(num_kv_pairs) < std::numeric_limits<size_t>::max()
static_kv
statistics in inference mode (training_mode=False, default),
status.IsOK()
status.IsOK() && !impl_->ngram_counts_.empty()
status.IsOK() && !impl_->ngram_indexes_.empty()
status.IsOK() && !input_dimensions_.empty()
status.IsOK() && !pool_int64s.empty()
std::all_of(impl_->ngram_indexes_.cbegin(), impl_->ngram_indexes_.cend(), [](int64_t i) { return i >= 0; })
std::all_of(output_edges.cbegin(), output_edges.cend(), [&src_idx](const GraphEdge& edge) { return edge.src_arg_index == src_idx; })
std::all_of(split_sizes.cbegin(), split_sizes.cend(), [](int64_t value) { return value >= 0; })
std::all_of(split_sizes_.cbegin(), split_sizes_.cend(), [](int64_t value) { return value >= 0; })
std::count_if(subgraph_node.InputEdgesBegin(), subgraph_node.InputEdgesEnd(), [input_slot_index](const Node::EdgeEnd& entry) { return entry.GetDstArgIndex() == input_slot_index; }) == 0
StdDev = Sqrt (VarPlusEpsilon)
Steepness
'step' cannot be 0
'step' cannot be 0 for Slice
'step' value cannot be 0
steps
steps.size()=
stod argument out of range
stof argument out of range
stol argument out of range
stoll argument out of range
Stopword contains invalid utf8 chars
stopwords
storage_order
stoull argument out of range
strcspn
stream timeout
Stride along each axis.
Stride along each spatial axis.
Stride along each spatial axis. If not present, the stride defaults is 1 along each spatial axis.
Stride along each spatial axis. If not present, the stride defaults to 1 along each axis.
Stride along each spatial axis. If not present, the stride defaults to 1 along each spatial axis.
Strides
strides
strides.size() == kernel_shape.size()
strides_.size() == kernel_shape_.size()
string
STRING
string buffer allocation failed
STRING data (tensor name: 
string enum that cases output to be lowercased/uppercases/unchanged. Valid values are "LOWER", "UPPER", "NONE". Default is "NONE"
string literal
string tensor can not have raw data
string tensor can not use pre-allocated buffer
string tensor is not supported for copying between allocators
string too long
String value expected, but not found.
string_data
string_vocabulary
StringFileInfo
StringNormalizer
strings
STRINGS
Strings can only reside in CPU memory
Strings to tokenize
strncmp
strnlen
subgraph
Subgraph
Subgraph in 'body' produces 
Subgraph input missing type.
Subgraph must have the shape set for all outputs but 
Subgraph SessionState entry for 
Subgraph SessionState for 
Subgraph SessionState was not found for '
Subgraph SessionState was not found for 'body' attribute.
subgraphs_kernel_create_info_maps.find(local_subgraph_kernel_create_info_map_key) == subgraphs_kernel_create_info_maps.end()
success
SUCCESS
suffix matching is assumed. 1-dim expansion doesn't work yet.
suffix_match_idx
SuffixShape = ConstantOfShape (NumReducedAxes)
Sum of split values not equal to 'input' dim size on 'axis'. 'axis' dim size=
sum(sqrd(x_i - x_avg)) / N
sum_node.GetOutputEdgesCount() == 0
sum_output_edge.src_arg_index == 0
Sundanese
Support 2-D matrices only
Support padding modes for outside grid values: `zeros`(default), `border`, `reflection`. zeros: use 0 for out-of-bound grid locations, border: use border values for out-of-bound grid locations, reflection: use values at locations reflected by the border for out-of-bound grid locations.
Support vector coefficients.
support_vectors
Supported modes: `constant`(default), `reflect`, `edge`
SVMClassifier
SVMRegressor
SVW;B
SVW9A
SVWj$
SVWj0
SVWUj
SwapAdjacentNodes
Syloti_Nagri
syntax error 
Syriac
system
system error number 
SystemError
Sz9Jw
t hH7B
t j%X
t j}h(zE
t j=h
t jh[Sj
t jKh$HF
t!;3r
t";7r
t"VVV
T#I2{
t#j.X
t#j+X
t#jxh
t$ ;s
T$ ;U
T$ ;V
T$ +\$
T$ +\$$
T$ 9\$ |
T$ 9T$(|;
T$ u8
T$$;\$H|
T$$;V
t$$;w
T$$+\$
t$$G9^
t$$jpY
t$$PQ
t$$RQW
t$$WSQWR
T$(;S
t$(+|$8
t$(jpY
t$(Vj
t$,;t$\
T$,;U
t$,+|$<Q
t$,+|$8
T$,A;
t$,PV
t$,RP
t$,SQW
t$,Vj
t$;;r
t$;7r
t$@;\$
t$@;t$\
t$@;t$`
T$@;U
t$@;w
T$@9|$\
T$@R3
T$@RQ
t$<+|$8Q
T$<9s 
T$<RP
t$<WP
T$0;Q
T$0;t$
T$0;U
t$0+|$<P
t$0+|$8
T$09U
T$4;\$P|
T$4;U
t$4;U,|
t$4+|$<P
T$4VW
T$8;A
T$8;t$
t$8;w
T$89s 
T$8RP
t$8VQP
t$d;u
t$DPQ
T$DRP
t$DSQP
t$DVS
t$DVW
t$hPW
t$j#Y
t$jtX
t$l;u
t$LQP
t$LQQ
t$LRP
t$lSP
t$P;u
t$PPRPjp
t$PQP
t$PQWR
t$PQWRP
t$PRQW
t$pVQ
t$pVR
T$T;U
T$t9D$luL
t$tQP
t$tWR
T$X;\$<sG
T$x;U
t$XjoZRY
t$xPW
t$XQP
t$XRP
T$XY3
t%;7r
t%;A0t 
t*;7r
t,h<[F
t,j"h
t,j)hx}G
t,j.h
t,j3hD
t,j9h
t,jfh
t,jHh
t,jJh
t,jMh|,D
t,jVh0
t,j-Y
t.h`IP
t.j h(
t.j h\
t.j!h$
t.j*h
t.j?h
t.j}h
t.j7h@
t.j'h
t.j-h
t.j'hx
t.jJh
t.jJh@
t.jJhXGF
t.jNh(
t.jNh\
t.jQhp
t/j#h
t/j,h
t/j,hH
t/j`h
t/j|h
t/j~hdYG
t/j6hXGF
t/j9h
t/j9hXGF
t/jdh
t/jEh
t/jEh\
t/jFh
t/jFhL]H
t/jGh$HF
t/jlh
t/jQh
t/jsh
t/jwh(zE
t/jYh
t/jZh\jE
t:j?h|
t@33{@
t@j Y
t[hDQE
t^=333
t_;7s4
t_=UUU
t_proto_p->dims()[0] == 1
t_proto_p->dims_size() == 1
t`;>s7
t}h RE
t+j]h
t+j+h
t+jAhl
t+juh
t==fff
t=8B t8
t=j%XP
t>=333
t>jbhp
t0j~h
t0jjh<
t1jPh@
t2;;r
t2;3r
t2;7r
t2hD'I
t2jih
t2jrh
t3jUh
T3r3y3
t4;;r
t4;7r
t4j$h
t4j%h$
t4jYh
t4jYh\
t5;;r
t5jIh
t5jJh(
t5jnh
t5VPWf
t6;;r
t69H,}
t6hdXG
t6j(X
t6jHh
t6jIh
t6jJh
t6jKh
t6jQh
t6jRh
t6jTh
t6jXh
t6VW+
t7jThD
t7jvh|
t9jfh@
ta;GLt\j
Tagalog
Tagbanwa
Tai_Le
Tai_Tham
Tai_Viet
tAj8Y
Takri
Tamil
Tangut
target
target optional type missing element type.
Target rank must be 1 less than the input rank.
target sequence type missing element type.
Target shape may not have multiple -1 dimensions
Target shape may not have multiple -1 dimensions.
target_class_ids.size() == target_class_nodeids.size()
target_class_ids.size() == target_class_treeids.size()
target_ids
target_node != NodesToOptimizeIndices::kEmptyNodeIndex
target_nodeids
target_treeids
target_type
target_weights
targets
tbh\-D
tBVW+
tchh~J
tCj*h
tCj2h
tDj_h
tEjdhXGF
Telugu
TempSpace allocator not found
tensor
TENSOR
Tensor after padding.
tensor can either be of element size 1 (including a scalar tensor and any
tensor can't contain negative dims
Tensor does not have external data to read from.
Tensor element type mismatch. 
tensor failed memory size calculation
Tensor initializers must have a non-empty name
Tensor is expected to contain one of the primitive data types. Got: 
Tensor must always contain primitive types. Found: 
tensor of bool, which should be a scalar.
Tensor of data to extract slices from.
tensor of int64, which should be a scalar.
Tensor of integers indicating the number of padding elements to add or remove (if negative) at the beginning and end of each axis. For 2D input tensor, it is the number of pixels. `pads` should be a 1D tensor of shape [2 * input_rank] or a 2D tensor of shape [1, 2 * input_rank]. `pads` format (1D example) should be as follow [x1_begin, x2_begin,...,x1_end, x2_end,...], where xi_begin is the number of pixels added at the beginning of axis `i` and xi_end, the number of pixels added at the end of axis `i`.
Tensor of rank q >= 1.
Tensor of rank q-1+r-indices[-1].
Tensor of rank r >= 1.
Tensor proto with external data for value attribute is not supported.
Tensor sequence must contain only primitive types
Tensor shape cannot contain any negative value
Tensor shape is too large
Tensor size (
Tensor size mismatch
tensor size overflow
tensor type 
Tensor type mismatch.
Tensor type mismatch. 
Tensor types should have been handled already
tensor with rank equal to or smaller than the first tensor), or having its
Tensor with shape information must be 1 dimensional.
tensor(
tensor(bfloat16)
tensor(bool)
tensor(complex128)
tensor(complex64)
tensor(complext128)
tensor(complext64)
tensor(double)
tensor(float)
tensor(float16)
tensor(int16)
tensor(int32)
tensor(int64)
tensor(int8)
tensor(string)
tensor(string) expected as input
tensor(uint16)
tensor(uint32)
tensor(uint64)
tensor(uint8)
tensor_a_zero_point == nullptr || IsScalarOr1ElementVector(tensor_a_zero_point)
tensor_b_zero_point == nullptr || IsScalarOr1ElementVector(tensor_b_zero_point)
tensor_c_zero_point == nullptr || IsScalarOr1ElementVector(tensor_c_zero_point)
tensor_type
tensor_x_scale->IsDataType<float>()
tensor_x_zero_point == nullptr || IsScalarOr1ElementVector(tensor_x_zero_point)
tensor_x_zero_point->GetElementType() == tensor_y_zero_point->GetElementType()
tensor_y_zero_point == nullptr || IsScalarOr1ElementVector(tensor_y_zero_point)
TensorProto ( tensor name: 
TensorProto (tensor name: 
TensorProto external data size mismatch. 
TensorProto external data size mismatch. Computed size: 
TensorProto type 
TensorProtoToMLValue() must take a pre-allocated MemBuffer!
TensorProtoToTensor() tensor shape mismatch!
TensorRT execution provider is not enabled in this build.
TensorrtExecutionProvider
tensors
TENSORS
TensorSeq: tensor to be added has a different data type.
ter!u
TerminateProcess
text file busy
text size: 
tF;;r
tf_crop_and_resize
tf_half_pixel_for_nn
TFIDF
TfIdfVectorizer
tGj.h
tH;;r
tH;>r
Thaana
Thales TSS ESN:60BC-E383-26351%0#
than the operator set version 
The Alpha value in Celu formula which control the shape of the unit. The default value is 1.0.
The axis along which same quantization parameters are applied. It's optional.If it's not specified, it means per-tensor quantization and input 'x_scale' and 'x_zero_point' must be scalars.If it's specified, it means per 'axis' quantization and input 'x_scale' and 'x_zero_point' must be 1-D tensors.
The axis in which to compute the arg indices.
The axis in which to compute the arg indices. Accepted range is [-r, r-1] where r = rank(data).
The axis on which to apply normalization, -1 mean last axis.
The bias (or mask) as Tensor.
The bias input data that is a 1D tensor.
The bias input, a vector with the same shape as last dim of data OR same shape with data
The bias value added to output. Default is 0.
The broadcasted dimensions of the inputs are incompatible
The coefficient 'a' used in cubic interpolation. Two common choice are -0.5 (in some cases of TensorFlow) and -0.75 (in PyTorch). Check out Equation (4) in https://ieeexplore.ieee.org/document/1163711 for the details. This attribute is valid only if "mode" is "cubic".
The computation of ReduceMean and ReduceVar uses float to avoid overflow for float16 inputs.
The coordinate of each dimension is transformed individually. Let's describe a case using axis x as an example.
The data type for the elements of the output tensor. Default is TensorProto::FLOAT.
The data type for the elements of the output tensor. If not specified, default is TensorProto::FLOAT.
The data type for the elements of the output tensor. if not specified, we will use the data type of the input tensor.
The data type to which the elements of the input tensor are cast. Strictly must be one of the types from DataType enum in TensorProto
The dimension with value zero exceeds the dimension size of the input tensor.
The distance metric to use. If a string, the distance function can be "braycurtis", "canberra", "chebyshev", "cityblock", "correlation", "cosine", "dice", "euclidean", "hamming", "jaccard", "jensenshannon", "kulsinski", "mahalanobis", "matching", "minkowski", "rogerstanimoto", "russellrao", "seuclidean", "sokalmichener", "sokalsneath", "sqeuclidean", "wminkowski", "yule".
The embedding matrix of size N x M. 'N' is equal to the maximum possible index + 1, and 'M' is equal to the embedding size
The environment variable contained the value: 
The epsilon value to use to avoid division by zero, default is 1e-5f.
The epsilon value to use to avoid division by zero.
the expected transformation of a stochastic regularizer which randomly applies
The exponent.
The filled tensor
The first input of CDist kernel has wrong shape: 
The first input of Range should be a constant with value 0.
The first normalization dimension: normalization will be performed along dimensions axis : rank(inputs).
The fused convolution operator schema is the same as Conv besides it includes an attribute
The FusedGemm operator schema is the same as Gemm besides it includes attributes
The given version [%u] is not supported, only version 1 to %u is supported in this build.
The graph run each iteration. It has 2+N inputs: (iteration_num, condition, loop carried dependencies...). It has 1+N+K outputs: (condition, loop carried dependencies..., scan_outputs...). Each scan_output is created by concatenating the value of the specified output value at the end of each iteration of the loop. It is an error if the dimensions or data type of these scan_outputs change across loop iterations.
The graph run each iteration. It has N+M inputs: (loop state variables..., scan_input_elts...). It has N+K outputs: (loop state variables..., scan_output_elts...). Each scan_output is created by concatenating the value of the specified scan_output_elt value at the end of each iteration of the loop. It is an error if the dimensions of these values change across loop iterations.
The id of the tree that each node is in.
The id of the tree that this node is in.
the identity or zero map to a neuron's input. The GELU nonlinearity weights
The index of the class list that each weight is for.
The index of the target that each weight is for
The inner-most 2 dimensions must have the same size (mat_w:
The innermost dims should have the same dim value to parse the diagonal elements
The input data as Tensor.
The input is not evenly splittable
The input must be a map from strings or integers to either strings or a numeric type. The key and value types cannot be the same.
The input must be a tensor of a numeric type or string. The output will be of the same tensor type.
The input must be a tensor of a numeric type, and of of shape [N,C] or [C]. In the latter case, it will be treated as [1,C]
The input must be a tensor of a numeric type, either [C] or [N,C].
The input must be a tensor of a numeric type.
The input must be a tensor of a numeric type. The output will be of the same tensor type.
The input must be a tensor of strings or integers, either [N,C] or [C].
The input must be an integer map to either string or float.
The input tensor cannot be reshaped to the requested shape. Input shape:
The input type is a tensor of any shape.
The input type must be a tensor of a numeric type, either [C] or [N,C].
The input type must be a tensor of a numeric type, either [N,C] or [C]. The output type will be of the same tensor type and shape.
The input type must be a tensor of a numeric type.
The input type must be a tensor of integers or strings, of any shape.
The integers of the map. This sequence must be the same length as the 'cats_strings' sequence.
The kernel corresponding to the node 
The kernel type, one of 'LINEAR,' 'POLY,' 'RBF,' 'SIGMOID'.
The keys when using int keys.<br>One and only one of the 'classlabels_*' attributes must be defined.
The keys when using string keys.<br>One and only one of the 'classlabels_*' attributes must be defined.
The lambd value for the Shrink formulation. Default is 0.5.
The maximum NGram size for suffix matching.
The mean of the normal distribution.
The minimum NGram size for suffix matching.
The model contains a 16-bit input (
The model has input '
The Model Proto has already been checked for the ORT config json.
The Model Proto hasn't been checked for the ORT config json.
The most inner dimension in boxes must have 4 data.
The new GRU hidden state calculated by this op.
The NGram size.
The node id of each weight
The node is not placed on any Execution Provider. 
The node kind, that is, the comparison to make at the node. There is no comparison to make at a leaf node.<br>One of 'BRANCH_LEQ', 'BRANCH_LT', 'BRANCH_GTE', 'BRANCH_GT', 'BRANCH_EQ', 'BRANCH_NEQ', 'LEAF'
The normal input data.
The number of batch dimensions. The gather of indexing starts from dimension of data[batch_dims:]
The number of channels to sum over
The number of graph input cannot be smaller than the number of node input
The number of support vectors.
The only subscript labels allowed are lower-cased letters (a-z) and upper-cased letters (A-Z)
The only supported values for the environment variable 
The op type of a node cannot be empty
The order of the normalization, only 1 or 2 are supported.
The ORT format model version [
the ort_value must contain a constructed sparse tensor
the ort_value must contain a constructed tensor
the ort_value must contain a constructed tensor or sparse tensor
The output is a 1-D tensor of string, float, or integer.
The output is a tensor of strings or integers. Its shape will be the same as the input shape.
The output mask of dropout.
The output scalar. Its value is true if all input tensors are finite. Otherwise, the output value would be false.
The output type will be a tensor of strings or integers, and will have the same shape as the input.
The output type will be a tensor of strings or integers, depending on which of the the classlabels_* attributes is used.
The output type will be a tensor of strings or integers, depending on which of the the classlabels_* attributes is used. Its size will match the bactch size of the input.
The output will be a sequence of string or integer maps to float.
The output will be a tensor of strings or integers.
The output will be a tensor of the value type of the input map. It's shape will be [1,C], where C is the length of the input dictionary.
The output.
The outputs are updated as follows when training_mode=True:
The override dims are not compatible with given tensor's shape. 
The pads attribute cannot be used simultaneously with auto_pad attribute
The parent of shape nodes are expected to be input_ids.
The parent of two shape nodes are expected to be input_ids.
The pooling method. Two modes are supported: 'avg' and 'max'. Default is 'avg'.
The pooling method. Two modes are supported: 'bilinear' and 'nearest'. Default is 'bilinear'.
The preallocated buffer is too small. Requires 
The previous GRU hidden state.
The provided PrePackedWeightsContainer instance to be added to the session is null
The rank of input tensor must be >= axis
The rank of the input must match permutation size for Transpose
The ratio of random dropout
The ratio of random dropout, with value in [0, 1). If this input was not set, or if it was set to 0, the output would be a simple copy of the input. If it's non-zero, output will be a random dropout of input, which is typically the case during training.
The registered allocator for device-id 
The residual input, must have the same shape as data
the same ordering as the image pixel format.
The scale along height dimension. It takes value greater than or equal to 1.
The scale along width dimension. It takes value greater than or equal to 1.
The scale array along each dimension. It takes value greater than or equal to 1. The number of elements of 'scales' should be the same as the rank of input 'X'.
The scale to apply.
The scaled hyperbolic tangent values of the input tensor computed element-wise
The second input of CDist kernel has wrong shape: 
The sequence output for the hidden is optional if 0. Default 0.
The session already has a PrePackedWeightsContainer instance
The shape format of inputs X, initial_h and outputs Y, Y_h. If 0, the following shapes are expected: X.shape = [seq_length, batch_size, input_size], Y.shape = [seq_length, num_directions, batch_size, hidden_size], initial_h.shape = Y_h.shape = [num_directions, batch_size, hidden_size]. If 1, the following shapes are expected: X.shape = [batch_size, seq_length, input_size], Y.shape = [batch_size, seq_length, num_directions, hidden_size], initial_h.shape = Y_h.shape = [batch_size, num_directions, hidden_size].
The shape format of inputs X, initial_h, initial_c and outputs Y, Y_h, Y_c. If 0, the following shapes are expected: X.shape = [seq_length, batch_size, input_size], Y.shape = [seq_length, num_directions, batch_size, hidden_size], initial_h.shape = Y_h.shape = initial_c.shape = Y_c.shape = [num_directions, batch_size, hidden_size]. If 1, the following shapes are expected: X.shape = [batch_size, seq_length, input_size], Y.shape = [batch_size, seq_length, num_directions, hidden_size], initial_h.shape = Y_h.shape = initial_c.shape = Y_c.shape = [batch_size, num_directions, hidden_size].
The shape of filled tensor
The shape of the convolution kernel. If not present, should be inferred from input W.
The shape of the convolution kernel. If not present, should be inferred from input 'w'.
The shape of the output can be explicitly set which will cause pads values to be auto generated. If output_shape is specified pads values are ignored. See doc for details for equations to generate pads
The shape of the output tensor.
The size of each input in the input list
The size of the kernel along each axis.
The standard deviation of the normal distribution.
The starting indexes of 1-grams, 2-grams, and so on in pool. It is useful when determining the boundary between two consecutive collections of n-grams. For example, if ngram_counts is [0, 17, 36], the first index (zero-based) of 1-gram/2-gram/3-gram in pool are 0/17/36. This format is essentially identical to CSR (or CSC) sparse matrix format, and we choose to use this due to its popularity.
The storage order of the tensor. 0 is row major, and 1 is column major.
The string used to pad output tensors when the tokens extracted doesn't match the maximum number of tokens found. If start/end markers are needed, padding will appear outside the markers.
The strings of the map. This sequence must be the same length as the 'cats_int64s' sequence
The subgraph in 'body' requires 
the tensor elementwise.
the tensor to be tiled using Tile OP must be atleast 1 dimensional
The third input of Range should be a constant with value 1.
The timestep for this operation.
The total number of regression targets, 1 if not defined.
The total number of targets.
The type of tensor: 
The type of the output tensor is integer.
The underlying implementation is MurmurHash3_x86_32 generating low latency 32bits hash suitable for implementing lookup tables, Bloom filters, count min sketch or feature hashing.
The value for the elements of the output tensor in sparse format.
The value for the elements of the output tensor.
The value for the sole element for the scalar, float32, output tensor.
The value for the sole element for the scalar, int64, output tensor.
The value for the sole element for the scalar, UTF-8 string, output tensor.
The values for the elements for the 1D, float32, output tensor.
The values for the elements for the 1D, int64, output tensor.
The values for the elements for the 1D, UTF-8 string, output tensor.
The weight for each target
The weight for the class in class_id.
The weighting criteria. It can be one of "TF" (term frequency), "IDF" (inverse document frequency), and "TFIDF" (the combination of TF and IDF)
The WordConvEmbedding takes in a batch of sequence words and embed each word to a vector.
The zero-padding added to one side of the output. This is also called adjs/adjustment in some frameworks.
then optionally start the crop offset by the left/top border amounts.
then_branch
then_branch and else_branch produce different number of outputs. 
then_feeds_fetches_manager_ && else_feeds_fetches_manager_
There are five required inputs 'X', 'scale', 'B', 'input_mean' and
There are multiple cases for the number of outputs, which we list below:
there are multiple cases for the number of outputs, which we list below:
There are multiple cases for the number of outputs, which we list below:
There is no location for this node arg in the outer scope location map
There must be one (and only one) dynamic typed input to the custom op. Its type info at runtime will be used to infer the type info of this dynamic typed output which is required for the success of the model loading step. More than one dynamic typed inputs are currently not supported as differing types at runtime means the output type cannot be inferred without which model loading cannot proceed.
There was a problem acquiring temporary memory allocator in Einsum op
There's no data transfer registered for copying tensors from 
t-hHaF
this API does not support strings
This API supports Tensors or SparseTensors
This attribute describes how to transform the coordinate in the resized tensor to the coordinate in the original tensor. <br/>
This instance should not be empty
This is an invalid model. At top level graph without matching NodeArg that subgraph consumes. Name=
This is an invalid model. Error: Duplicate definition of name (
This is an invalid model. Error: the graph is not acyclic.
This is an invalid model. Error: two nodes with same node name (
This is an invalid model. Failed to find NodeArg in all parent graphs. Name=
This is an invalid model. Graph output (
This is an invalid model. In Node, 
This is an invalid model. Model input (
This is an invalid model. Node (
This is an invalid model. Tensor does not have type information.
This is an invalid model. The sum of input arg count is not equal to size of input defs in node (
This is an invalid model. Type Error: Type '
This may prevent some of the graph optimizations, like const folding. 
This method does not expect allocator to be set
This method should follow a call to constructor that supplies the allocator
This number of op outputs should be 1 when Training_mode = False, but it is not.
This number of op outputs should be 3 when Training_mode = True, but it is not.
This operator applies convolution to word from left to right with window equal to conv_window_size and stride to 1.Take word 'example' for example, with conv_window_size equal to 2, conv is applied to [ex],[xa], [am], [mp]...If not provide, use the first dimension of conv kernal shape.
This operator has **optional** inputs/outputs. See [the doc](IR.md) for more details about the representation of optional arguments. An empty string may be used in the place of an actual argument's name to indicate a missing argument. Trailing optional arguments (those not followed by an argument that is present) may also be simply omitted.
This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md).
This optimizer does not support external data for unidirectional mask right now
This session already contains a loaded model.
This session has already been initialized.
This session will use the allocator registered with the environment.
this tensor already has populated sparse_indices
This transformer is already registered 
This ZeroCopyOutputStream doesn't support aliasing. Reaching here usually means a ZeroCopyOutputStream implementation bug.
this->base_values_.size() == predictions.size()
thisProto->value_case() == TypeProto::ValueCase::kMapType
thisProto->value_case() == TypeProto::ValueCase::kOptionalType
thisProto->value_case() == TypeProto::ValueCase::kSequenceType
thisProto->value_case() == TypeProto::ValueCase::kSparseTensorType
thisProto->value_case() == TypeProto::ValueCase::kTensorType
tHj2h
thread_scheduling_stats
Three interpolation modes: bilinear (default), nearest and bicubic.
Three interpolation modes: nearest (default), linear and cubic. The "linear" mode includes linear interpolation for 1D tensor and N-linear interpolation for N-D tensor (for example, bilinear interpolation for 2D tensor). The "cubic" mode includes cubic interpolation for 1D tensor and N-cubic interpolation for N-D tensor (for example, bicubic interpolation for 2D tensor).
Three modes: `constant`(default) - pads with a given constant value, `reflect` - pads with the reflection of the vector mirrored on the first and last values of the vector along each axis, `edge` - pads with the edge values of array
Three modes: constant(default), reflect, edge
Threshold
threshold
Threshold value
thresholdedrelu
ThresholdedRelu
Thresholds to do the splitting on for each node.
t'htCI
-Ti)%
Tibetan
Tifinagh
tIj Y
Tile doesn't have an implementation yet for the type: 
Tile doesn't support string type yet
tiles
time_axis
time_axis < 2
time_axis and batch_axis must have different values but both are 
timed out
Tirhuta
t'j"h
t'j3hD
t'jKhL]H
t'jph
tKhpiE
TlP0X
tMj=h
tmp_cats_int64s.empty() || tmp_cats_strings.empty()
to flatten the input shape to (N x C * D1 * D2 * ... * Dn) before a BatchNormalization Op.
to flatten the input shape to (N x C*D1*D2 ..*Dn) before a BatchNormalization Op.
to.custom_join_thread_fn
TO_FLOAT
TO_INT64
TO_STRING
token_id < vocab_size
tokenexp
Tokenized strings
Tokenizer
tokens
too many files open
too many files open in system
too many links
too many symbolic link levels
TorchEmbedding
Total allocated bytes: 
Total fused Attention node count: 
Total fused reshape node count: 
Total Gelu Approximation (FastGelu) node count: 
totalRunDuration
totalRuns
TraceAllocation for ort_value_idx=
TraceFree for ort_value_idx=
trailing \
Training mode only supports spatial BN
training_mode
training_mode of Dropout must be a scalar.
TrainingStepTensor
transA
TransA
TransB
transB
transform_targets
Translation
Transpose
Transpose not implemented for empty tensors.
Transpose of element size not supported in this build. Size=
transpose_node.InputDefs().size() == 1
transposed
TransposeMatMul
TransposeOptimizer
Tree id for each node.
TreeEnsembleClassifier
TreeEnsembleRegressor
tried creating tensor with negative value in shape
tried Filling sparse tensor with negative value in block sparse indices shape
tried Filling sparse tensor with negative value in values shape
tried to allocate 0 bytes
Tried to allocate without valid type information, ort_value index=
Trilu
tRjtSV
true literal
truej
TryAcquireSRWLockExclusive
Trying to allocate memory for unused optional inputs/outputs
Trying to get a SparseTensor, but got: 
Trying to get a Tensor, but got: 
Trying to get a TensorSeq, but got: 
Trying to register schema with name 
Trying to use OptionalGetElement on an optional type OrtValue which contains no data
tSh mG
tT9sXvHV
tVh$|E
Two interpolation modes: nearest (default), and linear (including bilinear, trilinear, etc)
Two interpolation modes: nearest(default), bilinear
two paths share the same shape
type == dtype_
type case mismatch. existing=
type case unsupported for symbolic shape inference. inferred=
type case unsupported. existing=
Type Error: Data in initializer '
Type Error: Shape of initializer 
Type Error: Type (
Type Error: Type parameter (
type field and data field mismatch in attribute 
Type mismatch. Current=
type must be number, but is 
Type of Mean and InvStdDev tensors.
Type of reduction to apply to loss: none, sum, mean (default). 'none': the output is the loss for each sample. 'sum': the output will be summed. 'mean': the sum of the output will be divided by the sum of applied weights.
Type of reduction to apply to loss: none, sum, mean(default). 'none': no reduction will be applied, 'sum': the output will be summed. 'mean': the sum of the output will be divided by the number of elements in the output.
Type of reduction to apply: none (default), add, mul. 'none': no reduction applied. 'add':  reduction using the addition operation. 'mul': reduction using the multiplication operation.
Type of the element in the optional output
type used for stash mean/inv_std_var
Type:
type: 
type_error
type_id_counter == 1
type_proto
type_proto is not of type map!
type_proto is not of type sequence!
type_protos
TypeAndShapeInferenceFunction implementation incomplete: this line should never be reached.
TypeProto must have shape for this to run
u Vjd
u VWS
u WSV
u"hT"Q
u"RWQ
u#mj?
u#WSV
u$8\$
u$RWSV
u$VRQ
u$WPQ
u%;E$u 
u%hp@D
u)8G,t~
u,9{$u'
U,PQQQ
u.h CD
u:;T$
u';D$
u?j Y
u}h|IG
u<+}8
U<+u8
U0_0o0
U0S0Q
u1hLlG
u2hxrE
u6h$bD
u6hX%F
u6j Y
u7j8Y
ubh`rE
uf_^[
Ugaritic
u-h\-D
Uh`LE
uint16
uint32
uint64
uint64_data
uint8
UjAX;
uJh$VD
uLjih
Unable to convert strings tensor to a sparse tensor that is not on CPU
Unable to convert strings tensor to a sparse tensor that not on CPU
Unable to find a data transfer for copying from device type: 
Unable to find compiled kernel hash for node '
Unable to find node 
Unable to get an allocator
Unable to serialize model as it contains compiled nodes. Please disable any execution providers which generate compiled nodes.
Unable to shrink arena: 
Unable to write the provided PrePackedWeights instance into the container
Unactivated gate outputs from forget, update, and output gates, pre-activation.
UNDEFINED
Undefined tensor type!
unexpected 
unexpected )
Unexpected attribute type.
Unexpected CAST_TO value of 
Unexpected CBLAS_TRANSPOSE for TransA of 
Unexpected data type for Clip input of 
Unexpected data type for Clip 'min' input of 
Unexpected element size of 
unexpected error
unexpected failure
Unexpected input data type. Actual: (
Unexpected literal type.
Unexpected mode of 
Unexpected mode:
Unexpected NORMALIZE value of 
Unexpected op in Regexp::Equal: 
Unexpected opcode in short circuit: 
Unexpected opcode: 
Unexpected re_anchor value: 
Unexpected special state in RunStateOnByte
Unexpected type.
Unexpected value for 'add_second_class' of 
Unhandled 
unhandled 
unhandled opcode: 
Unhandled type: %d
UnhandledExceptionFilter
unidir mask is not constant
unidir mask shape not expected
unidirectional
unimplemented activation: 
Unique
unk__
unknown
Unknown aggregation function in TreeEnsemble.
Unknown AutoPadType String
Unknown Category and zeros = 0.
Unknown encoding 
unknown error
Unknown error during EndProfiling()
Unknown exception
Unknown exception in Load()
Unknown exception was caught by catch-all handler.
unknown kernel type
unknown round: 
Unknown tensor type of 
unknown token
unknown_dim == -1
UnknownEvent
Unloading DSO 
unnamed_thread_pool
unordered_map/set too long
UnpackTensor: the pre-allocate size does not match the size in proto
UnpackTensor: the pre-allocated size does not match the raw data size, expected 
Unrecognized attribute: 
Unrecognized data_type (tensor name: 
Unrecognized type value case (value_info name: 
Unsqueeze
unsqueeze_after_gather axes value not expected
UnsqueezeElimination
UnsqueezeElimination cannot remove node 
UnsqueezeElimination_
Unsuported type proto value case.
Unsupported attribute value type of 
Unsupported AutoPad Type.
Unsupported data type of 
unsupported data type: 
Unsupported data type: 
Unsupported device allocator in the context of pre-packed weights caching: 
Unsupported device id in the memory arena shrink list: 
Unsupported device specified in the memory arena shrink list: 
Unsupported 'dtype' in QLinear Pooling:
Unsupported 'dtype' value: 
Unsupported element size: 
Unsupported execution_mode value in ORT config: 
Unsupported graph_optimization_level value in ORT config: 
Unsupported indices_format passed
Unsupported input data type of 
Unsupported input element type of 
Unsupported input type
Unsupported input type in DepthToSpace op: 
Unsupported input type in SpaceToDepth op: 
Unsupported level
Unsupported model IR version: 
Unsupported non-raw-data data type!
Unsupported optimization level: 
Unsupported OrtValue type to copy between device.
Unsupported OrtValue type.
Unsupported output datatype with size: 
Unsupported output type of 
Unsupported pooling size : 
Unsupported pooling size.
Unsupported Source/Target type=
Unsupported sparse tensor data type of 
Unsupported tensor type of 
Unsupported type
Unsupported type:
Unsupported type: 
Unsupported value attribute datatype: 
Unsupported value for enable_profiling option: 
Unsupported value for inter_op_num_threads: 
Unsupported value for intra_op_num_threads: 
Unsupported version '
Unsupported X type: 
Unsupported Y type: 
unused
uOPPV
up_node should be parent of down_node and NodeArg slots of the edge between up_node and down_node should be (0, 0).
up_node should have only one Edge that points to down_node and its output is not graph output
updates
updates shape: 
updates tensor should have shape equal to indices.shape[:-1] + data.shape[indices.shape[-1]:]. 
UpdatesTensor
UpdateTypeShapeInference is not intended to be used with control flow nodes containing subgraphs
uPf9_
UPPER
upper
Upper boundary of the output values.
Upsample
Upsample operator
Upsample: input shape needs to be at least a single dimension.
Upsample: input tensor's dimension does not match the scales.
Upsample: input/output value is nullptr
Upsample: input/output value's dimension mismatch
Upsample: unexpected mode
UQPXY]Y[
uR9|$(uL
urh,rE
URPQQh
u'RWSV
US3,2
us9M$un
Use GetStringTensor*() API to retrieve strings
Use MakeBlockSparseStrings
Use MakeCooStrings
Use MakeCsrStrings
use_approximation
use_past
UseClipThreshold
usefp16
Using an input in multiple nodes on different devices is not supported currently. Input:
Using cached version of pre-packed weight for constant initializer: 
Using global/env threadpools since use_per_session_threads_ is false
Using transpose optimized pattern
Using user supplied initializer with name (
UTCReplace_AppSessionGuid
utils::HasDataType(t_proto)
utils::HasElemType(thisProto->optional_type())
utils::HasElemType(thisProto->sequence_type())
utils::HasElemType(thisProto->sparse_tensor_type())
utils::HasElemType(thisProto->tensor_type())
utils::HasKeyType(thisProto->map_type())
utils::HasName(sparse_tensor)
utils::IsPrimitiveDataType<T>(dtype_)
uVWjH
uXhXZH
uYhXZH
uZh|IG
v ;D$
v >= 0 && static_cast<uint64_t>(v) <= std::numeric_limits<size_t>::max()
v PjoZ
v$;|$
V$_^[
v)hL<G
V;8u"
v_final_and_scan_outputs
v_initial
v_reshape initializer value is not expected
V`SWj
V<;V@t
vAj0h|
valid
VALID
ValidateUnidirMask returns false for mask_slice
Validating no unexpected access using an invalid node_index. Got:
value
Value
value at X[t][n] >= seqLengths[n].
Value expected but not found.
Value of alpha
Value of alpha default to 0.2
Value of alpha.
Value of attribute 
Value of beta
Value of beta default to 0.5
Value of beta.
value of k must not be negative
Value tensor should be a 1D tensor of size 1 with the same type as that of the input tensor
value too large
Value type is not supported yet: 
Value used for extrapolation, when applicable. Default is 0.0f. 
Value(s) to change to
Value(s) to change to.
value_cache
value_float
value_floats
value_info
value_int
value_ints
value_proto != nullptr
value_string
value_strings
value_tensor->DataType() == data_type && value_tensor->Shape().Size() == 1
value_type
value_type != nullptr
ValueDataType
ValueDelta
Values
values
Values greater than this are mapped to 1, others to 0.
values in 'axes' are beyond the bounds of the computed output shape
values is 
values of data_type '
Values size 
values.size() == static_cast<size_t>(attr->floats_size())
values.size() == static_cast<size_t>(attr->ints_size())
values_count == index_size
values_floats
values_int64s
values_strings
ValueStart
ValuesTensor
Var = Sub (MeanOfSquare, SquareOfMean)
VarFileInfo
Variance
VarianceTensor
VarPlusEpsilon = Add (Var, Epsilon)
VCSSVh
Vd^[3
vector too long
vector<bool> too long
vectors_per_class
Vh ]E
Vh uD
Vh([D
Vh(2Q
Vh(8H
Vh(gF
Vh,pE
Vh,xE
Vh@$D
Vh@(H
Vh@*D
Vh@dG
Vh@fE
Vh\(E
Vh\pG
Vh\ZG
Vh`%G
Vh`.D
Vh`[E
Vh`~H
Vh`<F
Vh|5G
Vh|hE
Vh|sG
Vh0XH
Vh4>G
Vh46F
Vh8|G
Vh8TF
Vh8VE
Vhd(H
VhH]E
Vhh>I
VhHtE
VhHUH
VhHvE
VhHWD
VhL.F
VhL\G
VhL`G
Vhl~E
VhlEF
VhlGG
VhLRF
VhlVE
VhLwF
VhP?I
VhP`F
Vhp7G
VhPiG
Vht%G
VhT~F
VhTSD
VhTvG
VhTzG
Vhx9F
VhXdH
VhXmG
VhxQD
vI9~`
via some custom implementation such as CuDNN.
Violation of the requirment that all input tensors must have the same data type.
VirtualAlloc
VirtualFree
VirtualProtect
VirtualQuery
VitisAIExecutionProvider
VIWEF
VjdRP
VQh([D
VQh,4H
VQh`3I
VQh0[D
VQh8tH
VQhDnH
VQhh:H
VQhljH
VQhT3I
VQhTqH
VQhxgH
VQRQPQ
VS_VERSION_INFO
VSj(P
VVSQVh
VVSWj
VVVSQh$
VVVSQh`nH
VVVSQhh
VWhh~J
VWjp+
vYh@GD
W,|g,]&
w.&$p
w.k!N
w/uj6A
w_^[]
w_scale
W_scale
W_zero_point
w_zero_point
W_zero_point_data[i] == W_zero_point_value
W2%x#
W2A$s
W6!2L
W8+W4PQ
WaitForSingleObject
WaitForSingleObjectEx
WaitRevoke
WakeAllConditionVariable
WakeConditionVariable
Walk NULL
Wancho
Warang_Citi
WARNING
Warning: Checker does not support models with experimental ops: 
Warning: Shape inference does not support
Warning: Unsupported operator 
'was added but does not exist. 
Washington1
WASM and 32-bit builds support only COO format
wcsnlen
wds2eJ
We do not expect duplicate registration of types for: 
We do not support type [
We don't expect custom allocators for non-tensor types, so a shape is mandatory here.
weight
weight and zero_point pair is expected to have same type.
Weight point must be constant
Weight rank must be 1.
Weight zero point must be zero
weight_gather
weight_gather_sum
weight_gather_temp
weight_gather_temp_1
weight_scale
weight_zero_point
weights
Weights of the intercepts, if used.
Weights of the model(s).
weights.quant_para_
weights_to_be_filled_in.buffers_.size() > 0
WeightTensor
WGWWj
Wh :I
Wh(iI
Wh,sH
Wh|vD
When computing the output of the hidden gate, apply the linear transformation before multiplying by the output of the reset gate.
When coordinate_transformation_mode is "tf_crop_and_resize" and x_original is outside the range [0, length_original - 1], this value is used as the corresponding output value. Default is 0.0f.
When the session is not configured to use per session threadpools, the env must be created with the the CreateEnvWithGlobalThreadPools API.
When training_mode=False, extra outputs are invalid.
When training_mode=False:
Where
where const not matched.
where N is the population size (this formula does not use sample size N - 1).
where:
Whether A should be transposed
Whether A should be transposed on the last two dimensions before doing multiplication
Whether B should be transposed
Whether B should be transposed on the last two dimensions before doing multiplication
Whether C should be broadcasted
Whether every token can only attend to previous tokens. Default value is 0.
Whether include pad pixels when calculating values for the edges. Default is 0, doesn't count include pad.
Whether the operator should behave like fmod (default=0 meaning it will do integer mods); Set this to 1 to force fmod treatment
Whether to return the elements in sorted order.
Whether to return the top-K largest or smallest elements.
Whether to select the last index or the first index if the {name} appears in multiple indices, default is False (first index).
Whether to use ceil or floor (default) to compute the output shape.
Which axis to concat on
Which axis to concat on.  Default value is 1.
Which axis to concat on. A negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(inputs)..
Which axis to concat on. Accepted range in `[-r, r - 1]`, where `r` is the rank of input tensors. When `new_axis` is 1, accepted range is `[-r - 1, r]`. 
Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1]
Which axis to gather on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data).
Which axis to scatter on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1]
Which axis to scatter on. Negative value means counting dimensions from the back. Accepted range is [-r, r-1] where r = rank(data).
Which axis to split on
Which axis to split on. 
Which axis to split on. A negative value means counting dimensions from the back. Accepted range is [-rank, rank-1] where r = rank(input).
Which axis to split on. A negative value means counting dimensions from the back. Accepted range is [-rank, rank-1].
which does not equal the specified override of 
while parsing 
Whp:H
WhXxH
WideCharToMultiByte
width_scale
WilError_03
window
Windows.Foundation.Collections.IIterator`1<Windows.Foundation.Collections.IKeyValuePair`2<String, UInt32>>
Windows.Foundation.Collections.IKeyValuePair`2<String, UInt32>
Windows.Foundation.Collections.IMap`2<String, UInt32>
Windows::AI::MachineLearning::Adapter::SessionRegisterCustomRegistry
WindowSize
WinmlRuleTransformer
with a fixed dimension size 
with activation 
Wj(Y+
Wj8Y+
wLh4tD
Word embedding scale must be a scalar or 1D tensor of size 1
Word embedding shape not expected.
Word embedding zero point must be a scalar or 1D tensor of size 1
word_embedding
word_embedding and position_embedding shall have same dimension 1
word_embedding and segment_embedding shall have same dimension 1
word_embedding is expected to have 2 dimensions, got 
word_embedding should have 2 dimensions and dimension size is known.
word_embedding_quant
word_embedding_scale
word_embedding_zero_point
WordConvEmbedding
Works on NHWC layout or not? Default not.
WpCh*
WQh([D
WQh`nH
WQh0[D
WQh8tH
WQhDnH
WQhljH
WQhTnH
WQhxgH
WQPQQV
WQVQRQPQ
WQWWh
Writing profiler data to file 
Wrong input type encountered for zero point input def @
Wrong input type encountered for zero point of quantized input @
Wrong op_type name for running propagation: 
wrong protocol type
WSj?Z
WSSWh
wstr != wconv_error
WVQh([D
WVQh\
WVQh0[D
WVSQQ
wVSWP
WVWVP
WWQhX
WWWhT
WWWhX
WWWPSQh0[D
WWWQh
WWWQhX
WWWVQhh
wx;w|
X != nullptr
X and mask should have the same shape
X dims is empty.
X input is required!
X num_dims does not match W num_dims.
x.?79
x;'bC
x';N(}"
X[_^]
x^vmE
X_alpha
X_bias = Add (X, bias)
X_bias = Identity (X)
X_Exp
X_greater
X_Log
X_LogSM
X_LogSM_NCD
X_NCD
X_NDC
x_original = (x_resized + 0.5) / scale - 0.5, <br/>
x_original = (x_resized + 0.5) / scale, <br/>
x_original = length_resized > 1 ? (x_resized + 0.5) / scale - 0.5 : 0, <br/>
x_original = length_resized > 1 ? start_x * (length_original - 1) + x_resized * (end_x - start_x) * (length_original - 1) / (length_resized - 1) : 0.5 * (start_x + end_x) * (length_original - 1).
x_original = x_resized * (length_original - 1) / (length_resized - 1), <br/>
x_original = x_resized / scale, <br/>
x_ptr != nullptr
X_random
X_ReduceMax
X_ReduceSum
x_scale
X_scale
X_shape
X_squared
X_Sub
X_variance
X_zero_point
x_zero_point
x_zero_point must be null or 1D tensor with size 
x_zero_point must be null or a scalar or 1D tensor or size 1.
X|+Xx
X->Shape().GetDims().size() == output_dims.size()
X->Shape().NumDimensions() == 4
X0V0T
X2D = Flatten (X)
X425v6j8
X9F(s
XBRQqS
X-device copy of strings not supported
Xh &B
XhD$E
XhD)E
Xogy[K
xp+xl
xQ5cam
XShape = Shape (X)
xSu$W
XU = Cast (X2D)
xV;\$
Xv}dw.
xxUG6
Y = (X - current_mean) / sqrt(current_var + epsilon) * scale + B
Y = (X - input_mean) / sqrt(input_var + epsilon) * scale + B
Y = Reshape (Biased, XShape)
Y = softmax(scores + bias)) with simple broadcast on bias. Intended to specialize softmax(scores + additive_mask) commonly found in transformer models.
y";7r
y#LPA
Y(HmX
Y_^[]
Y_^Y]
Y__^[
y_scale
Y_scale
y_scale == nullptr || IsScalarOr1ElementVector(y_scale)
y_zero_point
Y_zero_point
y_zp == nullptr || IsScalarOr1ElementVector(y_zp)
Y95$<Q
Y95,<Q
Y954<Q
yet this opset 
Yezidi
yGF;u
Yh0MD
Yhp!B
YhP"B
YHRc\
Yj!h@
Yj)htEG
YPh|vD
YPQhd*I
YVWRP
YY_^[
YY_^[]
YY_^]
YY9~,t
YY90t
YY98u
YY98uS
YY9t$
YYj,hd@F
YYVQP
-z%Ji9
z/S!i>
Zanabazar_Square
zero_point == nullptr || std::all_of(zero_point, zero_point + x_zero_point->Shape().Size(), [](int32_t zp) { return zp == 0; })
zero_point_ptr == nullptr || (zero_point_ptr->Shape().NumDimensions() == 1 && zero_point_ptr->Shape()[0] == broadcast_dim)
zero_point_ptr == nullptr || IsScalarOr1ElementVector(zero_point_ptr)
Zero1D = Constant()
ZeroPointTensor
zeros
ZeY4u[l
ZH9^,
ZipMap
Zipmap does not support empty dim count
Zipmap only supports 1D or 2D input tensors
zK.mD!
zo"8=c
ZsQ\|(
ZvD:\a\_work\1\s\engine\lotus\onnxruntime\core\providers\dml\DmlExecutionProvider\src\GraphKernelHelper.cpp
